[
  {
    "id" : "119adb07-e764-461e-bbf6-8f6b30f9ef57",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-671230749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86fd3a71-e740-4d18-85c0-089896e5d2e1",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add a check here to ensure merge results were registered.\r\n`assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts.size)`",
        "createdAt" : "2021-05-28T12:07:29Z",
        "updatedAt" : "2021-05-28T12:07:29Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 341,
    "diffHunk" : "@@ -1,1 +3685,3689 @@      case (task, _) =>\n        (Success, makeMapStatus(\"hostA\", parts))\n    }.toSeq)\n\n    assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts)"
  },
  {
    "id" : "89f3ca07-e0cd-479b-a0c9-4e4455c2f646",
    "prId" : 28917,
    "prUrl" : "https://github.com/apache/spark/pull/28917#pullrequestreview-439653938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we simulate the usage of `withSQLConf` instead of integrating the confs with `test()`?",
        "createdAt" : "2020-06-29T01:42:31Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "68fa0fc3-c18e-4453-a33d-fa443ac02363",
        "parentId" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Most SQL-related configuration parameters can be changed dynamically, but most of Core's parameters are static.",
        "createdAt" : "2020-06-30T04:01:52Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec0d8d00b64662343dc6b3945dc5999343b699a7",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +289,293 @@  }\n\n  private def testWithSparkConf(testName: String, testTags: Tag*)\n      (pairs: (String, String)*)(testFun: => Any)(implicit pos: Position): Unit = {\n    super.test(testName, testTags: _*) {"
  },
  {
    "id" : "40883614-148e-4175-a48d-cdfa01dd0522",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-433763435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The test framework will call afterEach automatically. So you don't need to call it manually. ",
        "createdAt" : "2020-06-17T14:47:26Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f85ed1a6-e23a-4889-96bd-24db8d9b8c15",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "It is called by explicitly as the `init` which is called by the `beforeEach` was called with the `SHUFFLE_SERVICE_ENABLED` default config. So that one should be cleaned up and a new initialisation is needed see a few lines below.\r\n\r\nSome tests follow this pattern within this suite. ",
        "createdAt" : "2020-06-17T15:42:11Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8235625b-52bc-49e2-b911-4491c850fbee",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "Attila summed it up.",
        "createdAt" : "2020-06-17T16:33:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "1e4305dd-2baf-48a0-b8fb-7ef9ac26c453",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, make sense.",
        "createdAt" : "2020-06-19T02:28:20Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +563,567 @@  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()\n    val conf = new SparkConf()\n    conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")"
  },
  {
    "id" : "6716f514-9999-4098-a3aa-de92efcc251d",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-446080802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bedbb87b-2100-450a-8ed5-449686591948",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "unnecessary change?",
        "createdAt" : "2020-07-09T02:27:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4bba1bf1-6921-4378-9b3d-f104ecc361c7",
        "parentId" : "bedbb87b-2100-450a-8ed5-449686591948",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "@squito requested this change, to conform with what he said was the preferred scala style (map or flatMap with \"=>\" should use braces rather than parentheses).",
        "createdAt" : "2020-07-10T02:17:33Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +258,262 @@        _.asRDDId.map { id => (id.rddId -> id.splitIndex)\n        }.flatMap { key => cacheLocations.get(key)\n        }.getOrElse(Seq())\n      }.toIndexedSeq\n    }"
  },
  {
    "id" : "596efe16-bb00-4a41-a827-a8b0d6fe7875",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-453397596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Do we need to describe `when external shuffle service is used` assumption here because we have `conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")` at line 567?",
        "createdAt" : "2020-07-22T06:03:33Z",
        "updatedAt" : "2020-07-22T06:06:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a100b1ae-ecd6-461d-b494-dd3b659e30bc",
        "parentId" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "personally, I'm OK with this as is, I think its OK for some of the details to be down in the test itself and balance a super-duper long name.\r\n\r\n@dongjoon-hyun since you called this a nit I'm assuming you're OK with me merging this anyhow, but if not lemme know, can submit a quick followup.",
        "createdAt" : "2020-07-22T14:52:41Z",
        "updatedAt" : "2020-07-22T14:52:41Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +561,565 @@  }\n\n  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()"
  },
  {
    "id" : "c214a6ce-1d25-444d-bff7-daf7745da2b8",
    "prId" : 28641,
    "prUrl" : "https://github.com/apache/spark/pull/28641#pullrequestreview-424262214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Well, there's another concern that while we're reusing the function in more places, it gets more complicated to use after adding many new arguments.",
        "createdAt" : "2020-06-02T15:47:13Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "902d91f7-828b-476e-a7d6-967fb7c99fb6",
        "parentId" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@Ngone51 Thanks! Only two test cases passed `maptaskids`. I will delete this parameter.",
        "createdAt" : "2020-06-04T08:10:27Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "0db0d60e-613d-45f9-8cd8-75ce1de89686",
        "parentId" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : " Only two test cases passed `execIds`. I will delete this parameter too.",
        "createdAt" : "2020-06-04T09:25:59Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "681d9e5d14800a171b97e6d73a02ae6f14eba19d",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +886,890 @@   * @param hostNames - Host on which each task in the task set is executed\n   */\n  private def completeShuffleMapStageSuccessfully(\n      stageId: Int,\n      attemptIdx: Int,"
  },
  {
    "id" : "34bae23f-6374-4ade-abfc-091f85423700",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-377834649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4aabdfd1-45b6-4cb4-9b0e-4995c3bb7084",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we add a test for shuffle stages together with resource profiles? ",
        "createdAt" : "2020-03-17T03:50:08Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2649967d-1389-4df3-ba71-8eb4f1ef7d20",
        "parentId" : "4aabdfd1-45b6-4cb4-9b0e-4995c3bb7084",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sure, I'll add one",
        "createdAt" : "2020-03-19T15:31:03Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +3140,3144 @@    assert(error.contains(\"Multiple ResourceProfile's specified in the RDDs\"))\n  }\n\n  /**\n   * Checks the DAGScheduler's internal logic for traversing an RDD DAG by making sure that"
  },
  {
    "id" : "93675d5e-47cf-44c6-865d-a01d557e04ba",
    "prId" : 27050,
    "prUrl" : "https://github.com/apache/spark/pull/27050#pullrequestreview-346784825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f4df53-0924-4c5b-b94a-778264365954",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this isn't really testing that the stages got cleanup, right?  The activeJobs would have been empty without this fix because it gets cleaned up in cleanupStateForJobAndIndependentStages.. ie run this test without your fix, does the test still pass? \r\ncan we instead make sure SparkListenerStageCompleted was sent on the cancel",
        "createdAt" : "2020-01-22T17:53:22Z",
        "updatedAt" : "2020-02-17T14:36:35Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "95cfbd3fc7d64609c636e5d81a2d2b402726512a",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1973,1977 @@    runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success, 42))\n    assert(completedStage === List(0, 1, 1, 0))\n    assert(scheduler.activeJobs.isEmpty)\n  }\n"
  },
  {
    "id" : "7fbfb42b-3f3e-4400-ad14-cfefb28062b4",
    "prId" : 26206,
    "prUrl" : "https://github.com/apache/spark/pull/26206#pullrequestreview-306280473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c9821a4-99d9-4e8b-a4d5-f11d5a4d74aa",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we remove this commented code?",
        "createdAt" : "2019-10-23T18:38:14Z",
        "updatedAt" : "2019-10-24T01:30:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d467c05c-ea1b-4e3e-a427-388857f0aad6",
        "parentId" : "2c9821a4-99d9-4e8b-a4d5-f11d5a4d74aa",
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "done.",
        "createdAt" : "2019-10-24T01:52:15Z",
        "updatedAt" : "2019-10-24T01:52:15Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      }
    ],
    "commit" : "f23336dd808a196af8d36b7728e59af5dea9dced",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +581,585 @@      // make sure our test setup is correct\n      val initialMapStatus1 = mapOutputTracker.shuffleStatuses(firstShuffleId).mapStatuses\n      //  val initialMapStatus1 = mapOutputTracker.mapStatuses.get(0).get\n      assert(initialMapStatus1.count(_ != null) === 3)\n      assert(initialMapStatus1.map {"
  },
  {
    "id" : "7bf6f5ae-1e3f-47e4-8b7f-3976cdfff2c7",
    "prId" : 25751,
    "prUrl" : "https://github.com/apache/spark/pull/25751#pullrequestreview-287556963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f413993e-afd7-488d-8bfa-10ecee85ba06",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think we can stop here. We have enough test coverage for test rerun when the RDD is INDETERMINATE. We just need to prove that the sampled RDD with unordered input is INDETERMINATE",
        "createdAt" : "2019-09-12T15:35:56Z",
        "updatedAt" : "2019-09-12T17:57:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3ee8f2da-051e-4740-afe3-dfd1ce042e35",
        "parentId" : "f413993e-afd7-488d-8bfa-10ecee85ba06",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok. Let me modify the test later.",
        "createdAt" : "2019-09-12T16:31:35Z",
        "updatedAt" : "2019-09-12T17:57:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "71634f2c1339f5fcf50ed6f0469d27141e9c92d1",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2789,2793 @@\n    val sampledRdd = shuffleMapRdd2.sample(true, 0.3, 1000L)\n    assert(sampledRdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE)\n  }\n"
  },
  {
    "id" : "aa6d29ae-8716-4e9d-b8f0-298c3b73776f",
    "prId" : 25706,
    "prUrl" : "https://github.com/apache/spark/pull/25706#pullrequestreview-285127818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58b4fe7a-0ab8-4a1a-a089-a569c69eaf30",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is actual place of bug. The code is newly added (for \"newly\" I meant after the pattern of `sc.listenerBus.waitUntilEmpty(WAIT_TIMEOUT_MILLIS)` is applied) and doesn't follow the existing pattern. That is easily missed and here we can enforce it by disallowing access variables directly.",
        "createdAt" : "2019-09-06T22:06:22Z",
        "updatedAt" : "2019-09-06T22:36:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "a3537c96-471f-47a3-a3d5-a706bfd073dd",
        "parentId" : "58b4fe7a-0ab8-4a1a-a089-a569c69eaf30",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "There're some other places accessing failedStages without waiting. Search with `assert(failedStages === Seq(0))`",
        "createdAt" : "2019-09-06T22:37:54Z",
        "updatedAt" : "2019-09-06T22:37:54Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d999e9f722f382e1e7d26287317b5e7752524f7",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +1164,1168 @@      TaskKilled(\"test\"),\n      null))\n    assert(sparkListener.failedStages === Seq(0))\n    assert(mapOutputTracker.findMissingPartitions(shuffleId) === Some(Seq(0, 1)))\n"
  },
  {
    "id" : "0559b37c-27cf-44e8-8448-eeb550f6c5fc",
    "prId" : 25620,
    "prUrl" : "https://github.com/apache/spark/pull/25620#pullrequestreview-290116434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db79a835-7cc9-452f-820a-dac2f5fe708a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we do it consistently? The code is more readable with `mapTaskId = xxx` ",
        "createdAt" : "2019-09-18T16:52:57Z",
        "updatedAt" : "2019-09-18T18:26:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b4dcc166-4938-4374-aa55-d5f187b57f4b",
        "parentId" : "db79a835-7cc9-452f-820a-dac2f5fe708a",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, done in 28c9f9c.",
        "createdAt" : "2019-09-18T18:27:09Z",
        "updatedAt" : "2019-09-18T18:27:09Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c9f9c2da1215a836fef7925e95a40c7c6dd87e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +493,497 @@        MapStatus(\n          BlockManagerId(\"exec-hostA2\", \"hostA\", 12345), Array.fill[Long](1)(2), mapTaskId = 6)),\n      (Success, makeMapStatus(\"hostB\", 1, mapTaskId = 7))\n    ))\n    // map stage2 completes successfully, with one task on each executor"
  },
  {
    "id" : "67cc1570-1fb5-440c-b0bc-390e39441734",
    "prId" : 25508,
    "prUrl" : "https://github.com/apache/spark/pull/25508#pullrequestreview-277601647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62560e38-42f5-4263-b043-434200f9d952",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The behavior difference between 2.3 and 2.4 is related to #21758, which move the output status clean work forward: https://github.com/apache/spark/pull/21758/files#diff-6a9ff7fb74fd490a50462d45db2d5e11L1390.\r\nSo I fix the behavior by simulating the executor lost because here we want a scenario of missing some partitions while rerunning the shuffle map stage.",
        "createdAt" : "2019-08-21T07:25:55Z",
        "updatedAt" : "2019-08-22T02:18:15Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7b715090c646a9aae81059948d85c6d1977e526",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2523,2527 @@\n    // Simulate the scenario of executor lost\n    runEvent(ExecutorLost(\"exec-hostC\", ExecutorKilled))\n\n    // The first task of the final stage failed with fetch failure"
  },
  {
    "id" : "7d334235-69e2-43d2-b0c5-a47a9f7849b8",
    "prId" : 25498,
    "prUrl" : "https://github.com/apache/spark/pull/25498#pullrequestreview-276909491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d92b2ca-c022-4692-8b34-f07682c778ec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I feel it is good too to leave some comment here, as this change the test.",
        "createdAt" : "2019-08-19T20:26:03Z",
        "updatedAt" : "2019-08-20T02:59:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2b712ea7-0b98-4ee1-83e8-fee23841489a",
        "parentId" : "5d92b2ca-c022-4692-8b34-f07682c778ec",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, thanks for the advice, done in 485157a.",
        "createdAt" : "2019-08-20T03:00:42Z",
        "updatedAt" : "2019-08-20T03:00:43Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "485157a2cdc427b4612e4f84635d4bdb77e6b9f4",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +2745,2749 @@    // stage rerun.\n    // TODO: After we support re-generate shuffle file(SPARK-25341), this test will be extended.\n    assert(failure != null && failure.getMessage\n      .contains(\"Spark cannot rollback the ShuffleMapStage 1\"))\n  }"
  },
  {
    "id" : "410f1c3f-0472-41fa-8811-f151adea6573",
    "prId" : 25420,
    "prUrl" : "https://github.com/apache/spark/pull/25420#pullrequestreview-273722334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "977db35a-8a78-44f4-9086-5c3c0cb5d8dd",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ignore this for the behavior change, as the approach now, we need to abort the stage of the current mapStage.\r\nAs we will finally support stage rerun, I suggest to skip this behavior, we can directly support the cache scenario after SPARK-25341 merged.",
        "createdAt" : "2019-08-12T14:16:50Z",
        "updatedAt" : "2019-08-12T14:16:50Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3ea90ff45cc5e3e4ab18a64b397bf4a05c02e01",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2711,2715 @@  }\n\n  ignore(\"SPARK-23207: retry all the succeeding stages when the map stage is indeterminate\") {\n    val shuffleMapRdd1 = new MyRDD(sc, 2, Nil, indeterminate = true)\n"
  },
  {
    "id" : "7ae40dd0-9c88-4d38-be67-76f4114bfda1",
    "prId" : 24892,
    "prUrl" : "https://github.com/apache/spark/pull/24892#pullrequestreview-256690073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ed67a8d-b89a-4f74-aeb5-05185467bc91",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The code below is very similar to the next test, can we abstract out some common code?",
        "createdAt" : "2019-06-27T08:00:41Z",
        "updatedAt" : "2019-08-09T06:17:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ad7e9013-c8ea-4a09-ac36-ad79035ac549",
        "parentId" : "0ed67a8d-b89a-4f74-aeb5-05185467bc91",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, thanks, done in 3ec15dc.",
        "createdAt" : "2019-07-02T06:41:06Z",
        "updatedAt" : "2019-08-09T06:17:25Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8873f111f5e1dcd6137396eeda06d7595ac2a72e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2714,2718 @@    val shuffleMapRdd1 = new MyRDD(sc, 2, Nil, indeterminate = true)\n\n    val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))\n    val shuffleId1 = shuffleDep1.shuffleId\n    val shuffleMapRdd2 = new MyRDD(sc, 2, List(shuffleDep1), tracker = mapOutputTracker)"
  },
  {
    "id" : "5605619a-1b42-4733-85f0-5f8b776befb1",
    "prId" : 24892,
    "prUrl" : "https://github.com/apache/spark/pull/24892#pullrequestreview-257753189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa175ff7-34e0-4828-b844-3c0bf29c416e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Why stage 0 is not marked as failed?",
        "createdAt" : "2019-07-03T07:42:06Z",
        "updatedAt" : "2019-08-09T06:17:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fb8aaa1c-7adf-43bd-b774-59237d3a2314",
        "parentId" : "aa175ff7-34e0-4828-b844-3c0bf29c416e",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Because stage 0 succeeds, FetchFailed triggered the failed stage 1,2.\r\nIn this test, I use a combo of FetchFailed and removeOutputsOnHost to simulate executor lost and check all indeterminate stage will rerun. The scenario I want to test is, although the indeterminate stage 0 is not failed, just missing an output, but when its child stage reran, stage 0 will also whole stage reran.",
        "createdAt" : "2019-07-03T21:58:13Z",
        "updatedAt" : "2019-08-09T06:17:25Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8873f111f5e1dcd6137396eeda06d7595ac2a72e",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +2860,2864 @@    mapOutputTracker.removeOutputsOnHost(\"hostB\")\n\n    assert(scheduler.failedStages.toSeq.map(_.id) == Seq(1, 2))\n    scheduler.resubmitFailedStages()\n"
  },
  {
    "id" : "81ee08ad-10d0-4803-9176-ca6f18b1050d",
    "prId" : 24892,
    "prUrl" : "https://github.com/apache/spark/pull/24892#pullrequestreview-273686566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5be11836-d601-4726-9ea5-728d63f88aeb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about we make the `shuffleMapRdd3` deterministic? To prove that all parent stages need to rerun even if they are deterministic.",
        "createdAt" : "2019-08-12T13:20:12Z",
        "updatedAt" : "2019-08-12T13:20:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8873f111f5e1dcd6137396eeda06d7595ac2a72e",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +2824,2828 @@\n  test(\"SPARK-25341: continuous indeterminate stage roll back\") {\n    // shuffleMapRdd1/2/3 are all indeterminate.\n    val shuffleMapRdd1 = new MyRDD(sc, 2, Nil, indeterminate = true)\n    val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))"
  }
]