[
  {
    "id" : "05ca4362-fe15-46c0-97a6-2f313bad4aba",
    "prId" : 28731,
    "prUrl" : "https://github.com/apache/spark/pull/28731#pullrequestreview-435335833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It executes Hive's beeline. Does it respect Spark submit options?",
        "createdAt" : "2020-06-05T13:48:13Z",
        "updatedAt" : "2020-06-05T13:48:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8b8ff14c-c047-46c7-a757-5c75af052850",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "body" : "Hi Hyukjin, some discussions about the krb5.conf location before is here https://issues.apache.org/jira/browse/SPARK-12050, but in this case, this does not work. From the discussion, people saying we should use SPARK_SUBMIT_OPTS to pass the non-standard krb5.conf location by  `-Djava.security.krb5.conf=/etc/krb5.conf-custom`. ",
        "createdAt" : "2020-06-05T23:54:36Z",
        "updatedAt" : "2020-06-05T23:56:02Z",
        "lastEditedBy" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "tags" : [
        ]
      },
      {
        "id" : "17990a81-3551-4b43-a969-9cc743c93343",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Was the JIRA for Hive beeline? I'm  saying its Hive's, not Spark's ",
        "createdAt" : "2020-06-06T16:03:04Z",
        "updatedAt" : "2020-06-06T16:03:04Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2599e98b-4eb4-4da3-b091-75277a10c1be",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "body" : "From my understanding, It should not be hive beeline's issue. Its an environment variable. This might not be the best solution, but it does solve the issue here. I have test hive beeline also, it works because it is able to read HADOOP_OPS. But somehow for spark, in spark-class script, whatever specified in the SPARK_SUBMIT_OPS is not passed to execution. So here I have to pass it in for spark beeline.",
        "createdAt" : "2020-06-08T20:29:40Z",
        "updatedAt" : "2020-06-08T20:29:40Z",
        "lastEditedBy" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "tags" : [
        ]
      },
      {
        "id" : "63176280-318a-4260-8e9a-be4342bd038f",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's avoid a bandaid fix here. Can we make `HADOOP_OPS` work with the beeline in Spark?",
        "createdAt" : "2020-06-09T00:33:34Z",
        "updatedAt" : "2020-06-09T00:33:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "61a58df6-ea24-439d-862b-1e3dcb12f659",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "bbe6d981-d22f-4d19-a704-6a03a8024ae0",
        "body" : "I took a look of how hive's beeline works, it looks like the suggested fix for spark's beeline aligns well with that.\r\nAlso, spark's beeline actually calls \"spark-class\" (and some other code call it as well), after tracing them one by one, the suggested fix seems to be the safest way as well.",
        "createdAt" : "2020-06-21T23:53:39Z",
        "updatedAt" : "2020-06-21T23:53:39Z",
        "lastEditedBy" : "bbe6d981-d22f-4d19-a704-6a03a8024ae0",
        "tags" : [
        ]
      },
      {
        "id" : "96bdc706-a66d-42fe-b54c-a0a3282f7ef9",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@kdzhao, can you show some code pointers? I still think it's odd that `SPARK_SUBMIT_OPTS` should take an effect in Hive's beeline.",
        "createdAt" : "2020-06-22T02:20:40Z",
        "updatedAt" : "2020-06-22T02:20:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5fedeedc-6eac-4575-b313-4abbc3211736",
        "parentId" : "e876a905-1768-470c-aef5-bbad6f58a1f6",
        "authorId" : "bbe6d981-d22f-4d19-a704-6a03a8024ae0",
        "body" : "I think I misspoke for it. What I want to say is, in hive, looks like its beeline command is just call to hive with different parameters:\r\nhttps://github.com/apache/hive/blob/branch-1.2/bin/beeline\r\nhttps://github.com/apache/hive/blob/branch-1.2/bin/hive\r\nAgree that hive's beeline doesn't read spark parameter, and I would assume it reads its own (I saw \"HADOOP_CLIENT_OPTS\" etc in above script).\r\nNow back to spark, agree with you that fixing it in spark-class might cover more cases. On another side, so far only the beeline has this issue, so an easy fix on the beeline script also makes sense as a stopgap.",
        "createdAt" : "2020-06-22T23:14:05Z",
        "updatedAt" : "2020-06-23T05:28:53Z",
        "lastEditedBy" : "bbe6d981-d22f-4d19-a704-6a03a8024ae0",
        "tags" : [
        ]
      }
    ],
    "commit" : "99666e88ee025a3e6315c93041eececc0ff8733b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +32,36 @@\nCLASS=\"org.apache.hive.beeline.BeeLine\"\nexec \"${SPARK_HOME}/bin/spark-class\" $SPARK_SUBMIT_OPTS $CLASS \"$@\""
  }
]