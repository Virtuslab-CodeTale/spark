[
  {
    "id" : "3d7283b9-2bc6-4366-96f7-8beb4ed2cfa2",
    "prId" : 33591,
    "prUrl" : "https://github.com/apache/spark/pull/33591#pullrequestreview-719288031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7288f52-33d3-4fb4-8137-8adb9d02b834",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto.",
        "createdAt" : "2021-07-30T16:45:54Z",
        "updatedAt" : "2021-07-30T16:45:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d87e2380a333b0fe7e3aa314b61d811f1af02cf",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +196,200 @@    if: >-\n      (github.repository == 'apache/spark' && needs.configure-jobs.outputs.type == 'pyspark-coverage-scheduled')\n      || needs.configure-jobs.outputs.type == 'regular'\n    name: \"Build modules (${{ format('{0}, {1} job', needs.configure-jobs.outputs.branch, needs.configure-jobs.outputs.type) }}): ${{ matrix.modules }}\"\n    runs-on: ubuntu-20.04"
  },
  {
    "id" : "e25b6b08-6969-4ab9-ac17-2a69a046903c",
    "prId" : 33591,
    "prUrl" : "https://github.com/apache/spark/pull/33591#pullrequestreview-719534727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07377b41-7ec3-462c-9dc2-c30eeae28d22",
        "parentId" : null,
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "nit: I guess we could also introduce the coverage in https://github.com/apache/spark/blob/master/dev/requirements.txt",
        "createdAt" : "2021-07-31T01:36:36Z",
        "updatedAt" : "2021-07-31T01:39:16Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "fc15f0b6-677c-4dff-a6e7-3af82bd0acdc",
        "parentId" : "07377b41-7ec3-462c-9dc2-c30eeae28d22",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks. Let me address them tomorrow.",
        "createdAt" : "2021-07-31T01:54:28Z",
        "updatedAt" : "2021-07-31T01:54:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d87e2380a333b0fe7e3aa314b61d811f1af02cf",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +270,274 @@        python3.9 -m pip install 'mlflow>=1.0' sklearn\n        # TODO(SPARK-36361): Install coverage in Python 3.9 and PyPy 3 in the base image\n        python3.9 -m pip install coverage\n        pypy3 -m pip install coverage\n        export PATH=$PATH:$HOME/miniconda/bin"
  },
  {
    "id" : "09e5f2ef-ff16-44ea-a132-42f74194361c",
    "prId" : 33567,
    "prUrl" : "https://github.com/apache/spark/pull/33567#pullrequestreview-718782802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc8343ec-a6c5-40b0-a027-528ca173495a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'll proceed SPARK-36345.",
        "createdAt" : "2021-07-30T07:05:38Z",
        "updatedAt" : "2021-07-30T07:05:38Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a5a4e646a34f6da70268491679d318a4d05ce6d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +253,257 @@    - name: Run tests\n      run: |\n        # TODO(SPARK-36345): Install mlflow>=1.0 and sklearn in Python 3.9 of the base image\n        python3.9 -m pip install 'mlflow>=1.0' sklearn\n        export PATH=$PATH:$HOME/miniconda/bin"
  },
  {
    "id" : "e11cecff-0dd3-4628-ac37-492db94fe3bd",
    "prId" : 33532,
    "prUrl" : "https://github.com/apache/spark/pull/33532#pullrequestreview-715489081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d20e52bd-2e01-4b6f-a94f-cfcb80f297a0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we skip this in SparkR build too?",
        "createdAt" : "2021-07-27T04:58:38Z",
        "updatedAt" : "2021-07-27T04:58:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "29a7cee3-17b2-48fa-9ea2-450f1007ed01",
        "parentId" : "d20e52bd-2e01-4b6f-a94f-cfcb80f297a0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We might have to skip here too: https://github.com/apache/spark/blob/9f20c62ff0c94a790bfd1fd925fd62fcf1ecc05a/.github/workflows/build_and_test.yml#L721",
        "createdAt" : "2021-07-27T04:59:14Z",
        "updatedAt" : "2021-07-27T04:59:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "42bfa080-373d-413c-84a0-2f677117e30d",
        "parentId" : "d20e52bd-2e01-4b6f-a94f-cfcb80f297a0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for the idea.",
        "createdAt" : "2021-07-27T05:02:24Z",
        "updatedAt" : "2021-07-27T05:02:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d83429e2721f3c1b01131edf34bf839cc364eb67",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +207,211 @@      SPARK_LOCAL_IP: localhost\n      SKIP_UNIDOC: true\n      SKIP_MIMA: true\n      METASPACE_SIZE: 128m\n    steps:"
  },
  {
    "id" : "190f09a6-d1d3-4748-8d90-d6e963cd9d66",
    "prId" : 33197,
    "prUrl" : "https://github.com/apache/spark/pull/33197#pullrequestreview-698496966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e06c522-2d4e-4416-954c-8c0ad779016d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Well, is this correct?\r\n\r\nThe original one is the same with what we did in `branch-3.1`. Do you mean that `branch-3.1` is not working too?\r\n- https://github.com/apache/spark/blob/branch-3.1/.github/workflows/build_and_test.yml",
        "createdAt" : "2021-07-02T20:59:28Z",
        "updatedAt" : "2021-07-02T20:59:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0734b983-0404-4334-91fd-abbf0b88ef49",
        "parentId" : "8e06c522-2d4e-4416-954c-8c0ad779016d",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I think `branch-3.1` is fine. We changed to distribute the tests to each forked repository after `3.1`, IIRC.",
        "createdAt" : "2021-07-02T21:01:34Z",
        "updatedAt" : "2021-07-02T21:01:34Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e4151b75aaf3d145fff0f3d653d76898b9ade55",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +4,8 @@  push:\n    branches:\n      - '**'\n\njobs:"
  },
  {
    "id" : "2ba3f609-abdb-4356-9fec-eb6d13ef1a9c",
    "prId" : 32829,
    "prUrl" : "https://github.com/apache/spark/pull/32829#pullrequestreview-679136639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57715043-446f-42cc-801e-39424023e035",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is only for Python3.6, right? Otherwise, we need to pin `mypy` in `dev/requirements.txt`, too.",
        "createdAt" : "2021-06-09T01:34:34Z",
        "updatedAt" : "2021-06-09T01:34:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6750b3bf-cc20-40aa-b819-032e3dabc00d",
        "parentId" : "57715043-446f-42cc-801e-39424023e035",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Actually..  the errors are different from different python versions ... e.g.) the same version with Python 3.8 doesn't pass the mypy test .. we should do something here ..",
        "createdAt" : "2021-06-09T01:39:48Z",
        "updatedAt" : "2021-06-09T01:39:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "052c9c1a98505e03db2577640e59d88788e56006",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +369,373 @@        # TODO(SPARK-35684, SPARK-35683): Bump up the mypy version. This is blocked by\n        #   Index.difference logic issue.\n        python3.6 -m pip install flake8 pydata_sphinx_theme 'mypy==0.812' numpydoc 'jinja2<3.0.0' 'black==21.5b2'\n    - name: Install R linter dependencies and SparkR\n      run: |"
  },
  {
    "id" : "11018db5-9e19-4850-b390-eeedebd742e0",
    "prId" : 32755,
    "prUrl" : "https://github.com/apache/spark/pull/32755#pullrequestreview-674734710",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f3739bf-963e-421c-907f-247b9bf0a895",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Note that this is a change from 20201025 to 20210602 unlike the other two places.",
        "createdAt" : "2021-06-02T22:32:58Z",
        "updatedAt" : "2021-06-02T22:33:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "13997ebdb7becf54e5506876259b51afdc8aeaa5",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +245,249 @@    runs-on: ubuntu-20.04\n    container:\n      image: dongjoon/apache-spark-github-action-image:20210602\n    env:\n      HADOOP_PROFILE: hadoop3.2"
  },
  {
    "id" : "20e4d9d1-2aa6-46e3-94ed-bcbee0ce186a",
    "prId" : 32737,
    "prUrl" : "https://github.com/apache/spark/pull/32737#pullrequestreview-674634499",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh, interesting .. https://github.com/xinrong-databricks/spark/runs/2724401328?check_suite_focus=true\r\n\r\n```\r\n/usr/bin/python3.9: No module named pip\r\n```\r\n\r\nit complains that `python3.9` doesn't have `pip` .. but assuming the skip tests checks (https://github.com/apache/spark/runs/2707956326 for https://github.com/apache/spark/commit/c225196be0d18975a3d290b0b9d7283d764be322), seems all packages are installed properly for Python 3.9.",
        "createdAt" : "2021-06-02T04:08:46Z",
        "updatedAt" : "2021-06-02T04:09:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d958cab3-4396-48ff-a9b1-101bf46d5100",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "reading the update for docker image (https://github.com/dongjoon-hyun/ApacheSparkGitHubActionImage/commit/e6e1d1a62e1db1b3db878a16002a4704f2c65535) seems like it should have `pip` ... weird .. ",
        "createdAt" : "2021-06-02T04:15:36Z",
        "updatedAt" : "2021-06-02T04:15:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1a0e9b99-6409-4406-aedf-05756c38adf4",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "I see what you mean. Would you think we can afford to install pip again? It takes ~4s including listing the packages.",
        "createdAt" : "2021-06-02T16:43:34Z",
        "updatedAt" : "2021-06-02T16:44:19Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      },
      {
        "id" : "81ec2519-898b-496f-a7e0-53a2a8d5ee0b",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "CC @dongjoon-hyun Do you have any insights?",
        "createdAt" : "2021-06-02T16:45:23Z",
        "updatedAt" : "2021-06-02T16:45:23Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      },
      {
        "id" : "e56a5b3d-a436-4111-9325-a86b9c145b6e",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for pinging me, @xinrong-databricks . Let me check for you.",
        "createdAt" : "2021-06-02T16:50:23Z",
        "updatedAt" : "2021-06-02T16:50:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7c18160c-abf1-4abf-809e-1563a09679a8",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I confirmed the issue. We are having 3 python versions and installed in the following sequence.\r\n- Python 3.8\r\n- Python 3.9\r\n- PyPy 3.7\r\n\r\nThe last thing do and it seems to break something.\r\n```\r\ncurl -sS https://bootstrap.pypa.io/get-pip.py | pypy3\r\n```\r\n\r\nLet me update the image once more. And, I'll add `plotly` to Python 3.8, too. ",
        "createdAt" : "2021-06-02T17:01:33Z",
        "updatedAt" : "2021-06-02T17:01:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "743536fa-d8ee-49b3-bea3-08911f613723",
        "parentId" : "d083701c-090a-4139-bc82-22a9cc5fe0bd",
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "Thanks @dongjoon-hyun for help! I will put the PR on hold then.",
        "createdAt" : "2021-06-02T20:06:44Z",
        "updatedAt" : "2021-06-02T20:06:44Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      }
    ],
    "commit" : "d77b6be5a6845b7d9f529cf579021959723313c3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +220,224 @@    - name: List Python packages (Python 3.9)\n      run: |\n        python3.9 -m pip list\n    - name: Install Conda for pip packaging test\n      run: |"
  },
  {
    "id" : "f820a568-2509-427b-bf59-8764d97b4d31",
    "prId" : 32663,
    "prUrl" : "https://github.com/apache/spark/pull/32663#pullrequestreview-668342639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c17bbab9-eff6-4b72-8468-f6b30bde9145",
        "parentId" : null,
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "The run takes 5s.",
        "createdAt" : "2021-05-25T20:32:05Z",
        "updatedAt" : "2021-05-25T20:32:06Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      }
    ],
    "commit" : "3717bde1e9aba48ce7e0fa49408b6ad3205663ee",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +218,222 @@        python3.6 -m pip install numpy 'pyarrow<3.0.0' pandas scipy xmlrunner plotly>=4.8\n        python3.6 -m pip list\n    - name: List Python packages (Python 3.8)\n      run: |\n        curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py"
  },
  {
    "id" : "b759f388-27b2-49d4-9631-ea4fb4ad19b3",
    "prId" : 32657,
    "prUrl" : "https://github.com/apache/spark/pull/32657#pullrequestreview-668258312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b990a84-9147-4d5f-b127-c615a99ffbd5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do you mean to install both Python 3.8 or 3.9?",
        "createdAt" : "2021-05-25T05:11:12Z",
        "updatedAt" : "2021-05-25T05:11:12Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "432b2ea3-878d-4d56-95c0-adee4833a5c3",
        "parentId" : "9b990a84-9147-4d5f-b127-c615a99ffbd5",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh yeah because the image is shared across other branches. If we're going to maintain the image for `master` branch only, we could remove Python 3.8 too. I will leave it to you on how to manage them, @dongjoon-hyun! It's not urgent in any event :-).",
        "createdAt" : "2021-05-25T05:13:25Z",
        "updatedAt" : "2021-05-25T05:14:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d9b75fae-49e2-4320-8335-a404b9d21843",
        "parentId" : "9b990a84-9147-4d5f-b127-c615a99ffbd5",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "@dongjoon-hyun if you manage py 3.9 in the docker image, may I ask you to install pip or a way to list installed libraries as well? Then we can list the libraries easily and it helps debugging as @xinrong-databricks is trying at #32663. Anyway, it's up to you. :)",
        "createdAt" : "2021-05-25T19:20:15Z",
        "updatedAt" : "2021-05-25T19:20:15Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2c9c05226db2d8b99d5e1f920bdf488aa54167",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +218,222 @@        python3.6 -m pip install numpy 'pyarrow<3.0.0' pandas scipy xmlrunner plotly>=4.8\n        python3.6 -m pip list\n    # TODO(SPARK-35507) Move Python 3.9 installtation to the docker image\n    - name: Install Python 3.9\n      uses: actions/setup-python@v2"
  },
  {
    "id" : "bb1f8f29-3015-48ba-90de-71cda41e671d",
    "prId" : 32657,
    "prUrl" : "https://github.com/apache/spark/pull/32657#pullrequestreview-668500561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7842904-111c-4f4f-844d-42e636659166",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "QQ: is it necessary to specify the architecture here?",
        "createdAt" : "2021-05-25T14:17:14Z",
        "updatedAt" : "2021-05-25T14:17:15Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "c6385e94-18a9-4eeb-b6f8-83a45700ef98",
        "parentId" : "b7842904-111c-4f4f-844d-42e636659166",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems not .. but let me just leave it for consistency with other places above, and just to be explicit.",
        "createdAt" : "2021-05-26T00:18:43Z",
        "updatedAt" : "2021-05-26T00:18:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2c9c05226db2d8b99d5e1f920bdf488aa54167",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +223,227 @@      with:\n        python-version: 3.9\n        architecture: x64\n    - name: Install Python packages (Python 3.9)\n      run: |"
  },
  {
    "id" : "b96f69e1-5095-45cb-8047-ddf4f943667c",
    "prId" : 32657,
    "prUrl" : "https://github.com/apache/spark/pull/32657#pullrequestreview-671793125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98bda214-699e-4346-84ff-aa5d03bb4a67",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this intentional to add a new PyArrow version test coverage on Python 3.9 only?",
        "createdAt" : "2021-05-25T16:45:34Z",
        "updatedAt" : "2021-05-25T16:45:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ca7908fa-5ece-4e90-820a-ec2561bbebbc",
        "parentId" : "98bda214-699e-4346-84ff-aa5d03bb4a67",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh yeah. I should've commented here. Python 3.9 support was added from https://issues.apache.org/jira/browse/ARROW-10224, and I just tentatively tried PyArrow 4.0.0 but it worked. So I just set it to the highest working version for now.",
        "createdAt" : "2021-05-26T00:15:46Z",
        "updatedAt" : "2021-05-26T00:15:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7fad4bb3-a087-48b4-a7e1-21d1fee73b6f",
        "parentId" : "98bda214-699e-4346-84ff-aa5d03bb4a67",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @BryanCutler FYI",
        "createdAt" : "2021-05-26T00:25:30Z",
        "updatedAt" : "2021-05-26T00:25:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2a7ca7ae-479f-4d61-83f8-1e699d2a1d3f",
        "parentId" : "98bda214-699e-4346-84ff-aa5d03bb4a67",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "SGTM. The Arrow binary format remains the same so it's good to continue testing with the latest pyarrow.",
        "createdAt" : "2021-05-30T05:44:44Z",
        "updatedAt" : "2021-05-30T05:44:44Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2c9c05226db2d8b99d5e1f920bdf488aa54167",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +226,230 @@    - name: Install Python packages (Python 3.9)\n      run: |\n        python3.9 -m pip install numpy 'pyarrow<5.0.0' pandas scipy xmlrunner plotly>=4.8\n        python3.9 -m pip list\n    - name: Install Conda for pip packaging test"
  },
  {
    "id" : "3d50f3d6-c16b-4847-a719-0bf929554c2a",
    "prId" : 32649,
    "prUrl" : "https://github.com/apache/spark/pull/32649#pullrequestreview-672573570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abd60a37-219a-4f11-8911-1a991fcc629c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun FYI. Would you mind installing plotly at Python 3.8 in the Docker image we use when you find some time?",
        "createdAt" : "2021-05-24T08:19:52Z",
        "updatedAt" : "2021-05-24T08:19:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b0699b83-af64-4829-a262-5ca217606af2",
        "parentId" : "abd60a37-219a-4f11-8911-1a991fcc629c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, I missed this comment last week. I only added plotly to Python 3.9 for now. I will add it to Python 3.8 soon.",
        "createdAt" : "2021-05-31T23:23:21Z",
        "updatedAt" : "2021-05-31T23:23:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6d43ef0f-6c8c-4758-abab-3c086c3bf5af",
        "parentId" : "abd60a37-219a-4f11-8911-1a991fcc629c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh i should have clarified it.now python 3.9 has to have this since we don't run he pyspark tests with python 3.8 anymore in the master branch, and pandas on Spark is only in the master branch. ",
        "createdAt" : "2021-06-01T00:11:26Z",
        "updatedAt" : "2021-06-01T00:11:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "57ef92eb0bddbe11c619f717d7315bd2ecd1e45d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +216,220 @@    - name: Install Python packages (Python 3.6)\n      run: |\n        python3.6 -m pip install numpy 'pyarrow<3.0.0' pandas scipy xmlrunner plotly>=4.8\n        python3.6 -m pip list\n    - name: Install Conda for pip packaging test"
  },
  {
    "id" : "f4fdb4a8-1b74-47fe-bc41-249ec77d3d0c",
    "prId" : 32631,
    "prUrl" : "https://github.com/apache/spark/pull/32631#pullrequestreview-670942695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01967ce1-d593-4b12-a83b-771dbf2a137a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, we should add `echo \"::set-output name=APACHE_SPARK_REF::$apache_spark_ref\"` after this line because we're running tests with `run-tests.py`.",
        "createdAt" : "2021-05-28T05:28:13Z",
        "updatedAt" : "2021-05-28T05:28:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f00a64fd-f2c3-48fc-8562-5610fc0ab617",
        "parentId" : "01967ce1-d593-4b12-a83b-771dbf2a137a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I will revert this for now .. seems like it breaks other tests.",
        "createdAt" : "2021-05-28T05:28:27Z",
        "updatedAt" : "2021-05-28T05:28:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "84efd980-c427-42b5-b4ea-d063039e8b60",
        "parentId" : "01967ce1-d593-4b12-a83b-771dbf2a137a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@sarutak would you mind opening a Pr again for this?",
        "createdAt" : "2021-05-28T05:28:53Z",
        "updatedAt" : "2021-05-28T05:28:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "235c6b84-17ad-4b08-b77a-5f334b839a64",
        "parentId" : "01967ce1-d593-4b12-a83b-771dbf2a137a",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Ah, O.K. I'll do it. Thanks for letting me know.",
        "createdAt" : "2021-05-28T05:37:11Z",
        "updatedAt" : "2021-05-28T05:37:11Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "692d95d1458993cbb9cbd47014202e84cd6aa328",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +649,653 @@        git fetch https://github.com/$GITHUB_REPOSITORY.git ${GITHUB_REF#refs/heads/}\n        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' merge --no-commit --progress --squash FETCH_HEAD\n        git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' commit -m \"Merged commit\"\n    - name: Cache Scala, SBT and Maven\n      uses: actions/cache@v2"
  },
  {
    "id" : "d4f0ae24-c266-40a8-86f0-5d51cf4bb9c4",
    "prId" : 32243,
    "prUrl" : "https://github.com/apache/spark/pull/32243#pullrequestreview-639525671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c26c14f-22f8-4e95-ad6e-cb9166784b2a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this sufficient?",
        "createdAt" : "2021-04-20T04:38:18Z",
        "updatedAt" : "2021-04-30T13:13:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cce36d5e-41c5-4d98-9ac4-247d52b28245",
        "parentId" : "6c26c14f-22f8-4e95-ad6e-cb9166784b2a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yes, I think it is for now since this PR targets at schema definition changes.",
        "createdAt" : "2021-04-20T05:48:57Z",
        "updatedAt" : "2021-04-30T13:13:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f4d022f93e7da391fb0b58131b0191c8fd65edc",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +521,525 @@      with:\n        path: ./tpcds-sf-1\n        key: tpcds-${{ hashFiles('sql/core/src/test/scala/org/apache/spark/sql/TPCDSSchema.scala') }}\n    - name: Checkout tpcds-kit repository\n      if: steps.cache-tpcds-sf-1.outputs.cache-hit != 'true'"
  },
  {
    "id" : "8d427c90-7545-4375-b2f9-be5baa12c629",
    "prId" : 32069,
    "prUrl" : "https://github.com/apache/spark/pull/32069#pullrequestreview-629503054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bf78888-1957-4d52-963e-99a17c9b999d",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Maybe we can rebalance the pyspark tests after we finish porting unit tests.",
        "createdAt" : "2021-04-07T00:21:22Z",
        "updatedAt" : "2021-04-07T00:59:10Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "2651cfb83ccdbf871bcf1d5bc05df9b237e3b493",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +163,167 @@            pyspark-core, pyspark-streaming, pyspark-ml\n          - >-\n            pyspark-pandas\n    env:\n      MODULES_TO_TEST: ${{ matrix.modules }}"
  },
  {
    "id" : "b753929c-5278-49d1-8e59-854a0fc3b4a1",
    "prId" : 32047,
    "prUrl" : "https://github.com/apache/spark/pull/32047#pullrequestreview-627507040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3a77d9c-5299-466b-8686-3493a86a8221",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, interesting. Previously, I moved this from here to line 360 because the linter is executed inside `dongjoon/apache-spark-github-action-image:20201025`.",
        "createdAt" : "2021-04-04T05:57:30Z",
        "updatedAt" : "2021-04-04T05:57:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9d83d6cdd6400de6cc5bf4dbc5305f3f1493120",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +285,289 @@    env:\n      LC_ALL: C.UTF-8\n      LANG: C.UTF-8\n    container:\n      image: dongjoon/apache-spark-github-action-image:20201025"
  },
  {
    "id" : "898915be-7958-4bb9-b8ce-12fbfed19e44",
    "prId" : 32011,
    "prUrl" : "https://github.com/apache/spark/pull/32011#pullrequestreview-625079876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a07b5d6-8b82-45f4-8946-6c3b19ea97da",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Nit: is there any way to avoid duplicated specs for this step \"Cache Scala, SBT and Maven\"?",
        "createdAt" : "2021-03-31T09:16:07Z",
        "updatedAt" : "2021-03-31T09:16:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "04cf540f-9bdf-4330-ba93-e3c28f0bd459",
        "parentId" : "6a07b5d6-8b82-45f4-8946-6c3b19ea97da",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I tired to find a way so hard but couldn't find so far.",
        "createdAt" : "2021-03-31T09:17:15Z",
        "updatedAt" : "2021-03-31T09:17:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "33021389-fee8-4eb7-94f0-b6226a787f59",
        "parentId" : "6a07b5d6-8b82-45f4-8946-6c3b19ea97da",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "OK that's fine :)",
        "createdAt" : "2021-03-31T09:26:48Z",
        "updatedAt" : "2021-03-31T09:26:48Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "072cd698-46f6-4a81-a787-fd78e1a90228",
        "parentId" : "6a07b5d6-8b82-45f4-8946-6c3b19ea97da",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems there are many requests for that, but GA still does not support it: https://github.community/t/support-for-yaml-anchors/16128",
        "createdAt" : "2021-03-31T09:26:58Z",
        "updatedAt" : "2021-03-31T09:26:58Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "642d7c09f604c2912042e1b863a6750d86184170",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +368,372 @@    - name: Checkout Spark repository\n      uses: actions/checkout@v2\n    - name: Cache Scala, SBT and Maven\n      uses: actions/cache@v2\n      with:"
  },
  {
    "id" : "63b0fabb-ecb2-4840-9c20-77a74e39f2db",
    "prId" : 31886,
    "prUrl" : "https://github.com/apache/spark/pull/31886#pullrequestreview-623146518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a6ea973-ea5f-4068-b95d-6decb38f51dd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe we should do the caches here as we do above \"Cache Coursier local repository\" since it builds and runs the test with downloading new dependencies in any event.",
        "createdAt" : "2021-03-29T11:56:29Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "841005c36cda15352cc026c67401571e43e7610a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +434,438 @@    runs-on: ubuntu-20.04\n    steps:\n    - name: Checkout Spark repository\n      uses: actions/checkout@v2\n    - name: Cache TPC-DS generated data"
  },
  {
    "id" : "273d1320-2a11-4f30-a64d-51a42d1c2e38",
    "prId" : 31886,
    "prUrl" : "https://github.com/apache/spark/pull/31886#pullrequestreview-624180551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65606515-0b34-48ac-808e-0f89a0321792",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we follow https://github.com/apache/spark/pull/31303/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R458? I would be great if we can just stick to using the original repo with addressing https://github.com/apache/spark/pull/31303/files#r563236578. cc @wangyum FYI.",
        "createdAt" : "2021-03-29T13:46:11Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff9da870-a2e2-4760-9a60-998327a0e05c",
        "parentId" : "65606515-0b34-48ac-808e-0f89a0321792",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. To generate table data for `TPCDSQueryTestSuite`, I think we need more fixes  in the orignal repo because they have different TPC-DS schemas (char/varchar vs string):\r\nspark-sql-perf: https://github.com/databricks/spark-sql-perf/blob/master/src/main/scala/com/databricks/spark/sql/perf/tpcds/TPCDSTables.scala#L57-L542\r\nspark-master/branch-3.1: https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/TPCDSBase.scala#L51-L548\r\n\r\nI've filed a ticket for this: https://github.com/databricks/spark-sql-perf/issues/198 I'll make a PR for it when I have time later.",
        "createdAt" : "2021-03-30T01:04:45Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "bab0e6cc-7826-49f0-898f-73f3f33d78dc",
        "parentId" : "65606515-0b34-48ac-808e-0f89a0321792",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks @maropu. Just to clarify, do you need https://github.com/databricks/spark-sql-perf/pull/196 too?",
        "createdAt" : "2021-03-30T05:39:00Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "40d3b2cd-932e-4e9c-b363-87ed7c9f85f5",
        "parentId" : "65606515-0b34-48ac-808e-0f89a0321792",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, the @wangyum PR looks useful when generating data. ",
        "createdAt" : "2021-03-30T07:11:49Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "951e37e0-4e74-480a-abea-7ef5d954afea",
        "parentId" : "65606515-0b34-48ac-808e-0f89a0321792",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sure, I will ping him offline to review",
        "createdAt" : "2021-03-30T11:01:09Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ac3100eb-3aba-4efe-9ebb-515e1f26ad51",
        "parentId" : "65606515-0b34-48ac-808e-0f89a0321792",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks, @HyukjinKwon ~",
        "createdAt" : "2021-03-30T12:13:35Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "841005c36cda15352cc026c67401571e43e7610a",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +448,452 @@      uses: actions/checkout@v2\n      with:\n        repository: maropu/spark-tpcds-sf-1\n        ref: 6b660a53091bd6d23cbe58b0f09aae08e71cc667\n        path: ./tpcds-sf-1"
  },
  {
    "id" : "a2209e86-f267-4bdf-a874-d30ba81402b1",
    "prId" : 31303,
    "prUrl" : "https://github.com/apache/spark/pull/31303#pullrequestreview-574892668",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "635be6a4-42fd-4f5f-9993-bb854c49462f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do we enable CBO by default? I think we should run it with the default conf.",
        "createdAt" : "2021-01-24T03:17:49Z",
        "updatedAt" : "2021-01-24T03:17:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9da5102bc18b0a7c0bad124839bee034390d26e8",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +459,463 @@      - name: Run TPCDSQueryBenchmark\n        run: |\n          SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --data-location `pwd`/tpcds1g --cbo\"\n      - name: Upload benchmark result to report\n        if: always()"
  },
  {
    "id" : "cae9d107-1818-42d2-8f23-5500a7aadba4",
    "prId" : 31303,
    "prUrl" : "https://github.com/apache/spark/pull/31303#pullrequestreview-574895575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1524cb1b-8474-477e-9dfb-e3539d2c9898",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you describe what these repos do in the PR description? Also what does this fork do? Looks like only diff is https://github.com/wangyum/spark-sql-perf/commit/abf08ebc0d1006e3511fe6b3608935c627b90741. Can you just use the original repo, and keep the data generate Scala file somewhere in Spark?",
        "createdAt" : "2021-01-24T03:21:56Z",
        "updatedAt" : "2021-01-24T03:21:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "778cca4a-361a-4c0d-bd0e-7b61a2133fd9",
        "parentId" : "1524cb1b-8474-477e-9dfb-e3539d2c9898",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. Original repo need a class: https://github.com/databricks/spark-sql-perf/pull/196",
        "createdAt" : "2021-01-24T04:39:50Z",
        "updatedAt" : "2021-01-24T04:39:51Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9da5102bc18b0a7c0bad124839bee034390d26e8",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +447,451 @@        uses: actions/checkout@v2\n        with:\n          repository: wangyum/spark-sql-perf\n          path: ./spark-sql-perf\n      - name: Install Java 8"
  },
  {
    "id" : "7f0b2b1d-e9a5-4471-ac4f-1d36e856314f",
    "prId" : 31303,
    "prUrl" : "https://github.com/apache/spark/pull/31303#pullrequestreview-574892860",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c2a2920-4934-4abd-ab44-b98898febdf5",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we keep the cache of Coursier in SBT?",
        "createdAt" : "2021-01-24T03:23:01Z",
        "updatedAt" : "2021-01-24T03:23:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9da5102bc18b0a7c0bad124839bee034390d26e8",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +459,463 @@      - name: Run TPCDSQueryBenchmark\n        run: |\n          SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt \"sql/test:runMain org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark --data-location `pwd`/tpcds1g --cbo\"\n      - name: Upload benchmark result to report\n        if: always()"
  },
  {
    "id" : "3465b44f-dea1-4658-bfa5-3ce93dfc4481",
    "prId" : 31303,
    "prUrl" : "https://github.com/apache/spark/pull/31303#pullrequestreview-574892882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "258a3203-a45c-4fb4-9343-3307fe3faa89",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "no big deal but I would name it `tpcds-1g`",
        "createdAt" : "2021-01-24T03:23:47Z",
        "updatedAt" : "2021-01-24T03:23:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9da5102bc18b0a7c0bad124839bee034390d26e8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +432,436 @@        ./build/sbt -Pyarn -Pmesos -Pkubernetes -Phive -Phive-thriftserver -Phadoop-cloud -Pkinesis-asl -Phadoop-2.7 compile test:compile\n\n  tpcds1g:\n    name: Benchmark TPC-DS with 1GB scale factor\n    runs-on: ubuntu-20.04"
  },
  {
    "id" : "5dd19ecb-7f07-4d3a-bb4b-2332eb6accf0",
    "prId" : 31098,
    "prUrl" : "https://github.com/apache/spark/pull/31098#pullrequestreview-564637671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4df13130-c04a-4574-94c8-9fc56b9e11d3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This one seems to be not allowed as of now.\r\n![Screen Shot 2021-01-08 at 2 57 07 PM](https://user-images.githubusercontent.com/9700541/104073315-36cf6980-51c2-11eb-9070-9e811c5720fa.png)\r\n",
        "createdAt" : "2021-01-08T23:00:20Z",
        "updatedAt" : "2021-01-08T23:00:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9717fae44951a633e708f58a3eb67e3d28a33388",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +90,94 @@      with:\n        fetch-depth: 0\n    - uses: n1hility/cancel-previous-runs@v2\n      with:\n        token: ${{ secrets.GITHUB_TOKEN }}"
  },
  {
    "id" : "403d07c4-23e2-499a-97cd-aa217b441fa5",
    "prId" : 30959,
    "prUrl" : "https://github.com/apache/spark/pull/30959#pullrequestreview-559498687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b15e692-c6c6-4020-a649-b59c50dc3ddb",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, we might need to have a way to control the dependency in this image soon ... ",
        "createdAt" : "2020-12-29T11:51:40Z",
        "updatedAt" : "2020-12-29T11:51:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f97dee09fde2bd77bf3514ed855278f15de974",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +287,291 @@    runs-on: ubuntu-20.04\n    container:\n      image: dongjoon/apache-spark-github-action-image:20201025\n    steps:\n    - name: Checkout Spark repository"
  },
  {
    "id" : "26b495dc-03a7-4bc8-8ce5-985fb6b9f279",
    "prId" : 30391,
    "prUrl" : "https://github.com/apache/spark/pull/30391#pullrequestreview-533912658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun, I just move it back below. Seems like it affects when things are ported back (Hadoop 2 build is only in master branch). I think it will still obviously show the red X one of builds fails anyway .. so I think it might be fine to move below here together with Java 11 and Scala 2.13 builds.",
        "createdAt" : "2020-11-17T04:45:26Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "40ffc611-097b-4d57-bd6d-d08b22ede4fe",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@HyukjinKwon . This was added at the end but was moved to the first position due @mridulm 's direct request. (https://github.com/apache/spark/pull/30378#issuecomment-727261552).\r\n\r\nI'm okay with all positions.",
        "createdAt" : "2020-11-17T04:49:39Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a4dede7b-81d0-4f53-b1e9-107c39fd93ff",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@mridulm, would you mind moving it below here? It will show an X obviously if one of builds fails anyway. It seems like it can cause some conflicts easily when we backport something to other branches for GitHub Actions. Currently, we're running the builds in all active branches, and porting back things.",
        "createdAt" : "2020-11-17T04:51:29Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ac03ef8b-8e71-415d-9143-90f0bdda72e9",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "@HyukjinKwon Currently, we merge if either of the builds pass (jenkins or github).\r\nThe reason I was requesting @dongjoon-hyun to move it to top was if jenkins gives positive build (since it is not testing hadoop 2), then only way to identify hadoop 2.x failure is by github actions failing on hadoop 2 build.\r\n\r\nIf we can get jenkins to also build hadoop-2, then we dont need to move this to top.\r\nElse, it is common for reviewers to not see the last entry in github build (when there is a jenkins green).\r\n",
        "createdAt" : "2020-11-18T08:22:51Z",
        "updatedAt" : "2020-11-18T08:22:52Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "5fce6214-4f64-47ff-a32c-16834848ef72",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "@HyukjinKwon Is there a better solution currently ? I am not an expert on github or jenkins :-)",
        "createdAt" : "2020-11-18T08:23:48Z",
        "updatedAt" : "2020-11-18T08:23:48Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "cf836013-2d1d-4fa8-ae67-0d38e2fa926f",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@mridulm, oh but if there's any of them fails, it shows a clear X like\r\n\r\n![Screen Shot 2020-11-18 at 9 07 02 PM](https://user-images.githubusercontent.com/6477701/99528700-077e5680-29e2-11eb-9769-f27b414657da.png)\r\n\r\nSo it should be fine. I do believe reviewers check the X or test failures from Jenkins before merging it in :-). Actually, Java 11 and Scala 2.13 too. Jenkins PR builder does not run them as well.",
        "createdAt" : "2020-11-18T12:08:31Z",
        "updatedAt" : "2020-11-18T14:59:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "07260a9f-5c07-42da-9179-811a30a9e4c0",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "That sounds good to me. Thanks for the details @HyukjinKwon !\r\n@dongjoon-hyun given details @HyukjinKwon provided, I am fine with the changes he proposed. Thanks",
        "createdAt" : "2020-11-18T19:22:25Z",
        "updatedAt" : "2020-11-18T19:22:26Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "1c7fbb88-b164-4ad5-b8cd-48a5d91d9f2d",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks!",
        "createdAt" : "2020-11-18T21:01:23Z",
        "updatedAt" : "2020-11-18T21:01:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4ea621bc-f237-4ddd-a701-fc83e758a470",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Given the agreement, shall we merge this?",
        "createdAt" : "2020-11-18T21:03:19Z",
        "updatedAt" : "2020-11-18T21:03:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0320e68a-17e7-4ece-b56c-9012a82cabfc",
        "parentId" : "494618ca-f63f-4cdd-9334-ba3dd26bbd38",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Please go ahead @dongjoon-hyun , thanks !",
        "createdAt" : "2020-11-18T21:56:16Z",
        "updatedAt" : "2020-11-18T21:56:16Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "15ea6b5d6bd67510d88f31d662df7a082c96fd07",
    "line" : 271,
    "diffHunk" : "@@ -1,1 +416,420 @@        ./dev/change-scala-version.sh 2.13\n        ./build/sbt -Pyarn -Pmesos -Pkubernetes -Phive -Phive-thriftserver -Phadoop-cloud -Pkinesis-asl -Djava.version=11 -Pscala-2.13 compile test:compile\n\n  hadoop-2:\n    name: Hadoop 2 build with SBT"
  },
  {
    "id" : "56bda710-8d4a-4f1f-9ba7-16167d94da26",
    "prId" : 30391,
    "prUrl" : "https://github.com/apache/spark/pull/30391#pullrequestreview-532020001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d299fb4c-2323-42ef-b168-c3911eaa6f61",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "No problem.",
        "createdAt" : "2020-11-17T04:51:05Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "15ea6b5d6bd67510d88f31d662df7a082c96fd07",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +411,415 @@      uses: actions/setup-java@v1\n      with:\n        java-version: 8\n    - name: Build with SBT\n      run: |"
  },
  {
    "id" : "a0c81c26-db88-4ab7-9c82-1564e1963bce",
    "prId" : 30391,
    "prUrl" : "https://github.com/apache/spark/pull/30391#pullrequestreview-532265517",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "For `Java 11 build`, it seems `mvn install` runs but can we just do `mvn compile testCompile`? If we can do so, we can remove `.m2` repository here like other places. What do you think?",
        "createdAt" : "2020-11-17T05:45:13Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "5e135af7-c455-41c5-9e38-be6a7e7b1692",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That works to me.",
        "createdAt" : "2020-11-17T06:49:40Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e6f79f45-5f38-424b-95d7-2f550fde4279",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@sarutak and @HyukjinKwon . I'd like to keep this `install` test coverage. This is a contribution from @wangyum to check a valid error check from JDK8 age.\r\n\r\nPlease see e5c176a243b76b3953cc03b28e6c281658da93c8",
        "createdAt" : "2020-11-17T07:43:38Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f6e7d54a-d24f-41de-9e3b-2603a7a2b08c",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we have another new place to test `install`?",
        "createdAt" : "2020-11-17T07:44:33Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "35864f21-adb5-4c61-8f36-59fd7a6a6116",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, sure. Then it makes sense to keep `install`. +1 for keeping it.",
        "createdAt" : "2020-11-17T07:48:21Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "df0b1b78-c343-410c-b844-5dcb921aeb85",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you! You can add some comments there if you want~",
        "createdAt" : "2020-11-17T07:50:49Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2660d9c-087b-470c-8ad1-b8e97e24012f",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Done",
        "createdAt" : "2020-11-17T07:54:23Z",
        "updatedAt" : "2020-11-17T07:54:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ab12e30a-c6c3-484c-9528-dd20954a1d26",
        "parentId" : "4e21fafe-3bec-4e58-876c-6a0afc0a1e9a",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "O.K, make sense. Thanks @dongjoon-hyun .",
        "createdAt" : "2020-11-17T11:24:17Z",
        "updatedAt" : "2020-11-17T11:24:17Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "15ea6b5d6bd67510d88f31d662df7a082c96fd07",
    "line" : 249,
    "diffHunk" : "@@ -1,1 +393,397 @@        # It uses Maven's 'install' intentionally, see https://github.com/apache/spark/pull/26414.\n        ./build/mvn $MAVEN_CLI_OPTS -DskipTests -Pyarn -Pmesos -Pkubernetes -Phive -Phive-thriftserver -Phadoop-cloud -Djava.version=11 install\n        rm -rf ~/.m2/repository/org/apache/spark\n\n  scala-213:"
  },
  {
    "id" : "930526d4-6424-4066-8724-63cd4d92a089",
    "prId" : 30378,
    "prUrl" : "https://github.com/apache/spark/pull/30378#pullrequestreview-530865497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d58e08c-4a58-4171-b38d-fd28521d6df7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, we should get rid of Maven cache which is not used. Nice.",
        "createdAt" : "2020-11-16T01:23:07Z",
        "updatedAt" : "2020-11-16T05:28:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f26fc30f6c068b7381741505cb19369c720c49f3",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +20,24 @@    runs-on: ubuntu-20.04\n    steps:\n    - name: Checkout Spark repository\n      uses: actions/checkout@v2\n    - name: Cache Coursier local repository"
  },
  {
    "id" : "8ece2b64-9728-4c17-a27f-e671700c48f9",
    "prId" : 30259,
    "prUrl" : "https://github.com/apache/spark/pull/30259#pullrequestreview-523961365",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a395254-9b5e-41a0-86c1-11903c5a2e32",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you, @sarutak . BTW, I cannot find this directory in my Mac. Is it dependent on OS?\r\n```\r\n$ ls ~/.cache/coursier\r\nls: /Users/dongjoon/.cache/coursier: No such file or directory\r\n```",
        "createdAt" : "2020-11-05T05:54:48Z",
        "updatedAt" : "2020-11-05T05:54:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3f62a015-8704-4b58-a8a1-fd7b68bc4a43",
        "parentId" : "6a395254-9b5e-41a0-86c1-11903c5a2e32",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The directories are different in OSs (https://get-coursier.io/docs/cache)",
        "createdAt" : "2020-11-05T05:55:32Z",
        "updatedAt" : "2020-11-05T05:55:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a085d237-f29e-40ea-a9b3-d18755acd95f",
        "parentId" : "6a395254-9b5e-41a0-86c1-11903c5a2e32",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks! @HyukjinKwon .",
        "createdAt" : "2020-11-05T05:58:07Z",
        "updatedAt" : "2020-11-05T05:58:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9f446ba1-a103-4e7c-922d-3da419485c15",
        "parentId" : "6a395254-9b5e-41a0-86c1-11903c5a2e32",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "> BTW, I cannot find this directory in my Mac. Is it dependent on OS?\r\n\r\nYeah, seems so.\r\nOn macOS, it is `~/Library/Caches/Coursier`.",
        "createdAt" : "2020-11-05T06:40:44Z",
        "updatedAt" : "2020-11-05T06:40:44Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "018b507ef3012a8847b9270c0b9cbda7895fb1eb",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +115,119 @@      uses: actions/cache@v2\n      with:\n        path: ~/.cache/coursier\n        key: ${{ matrix.java }}-${{ matrix.hadoop }}-coursier-${{ hashFiles('**/pom.xml', '**/plugins.sbt') }}\n        restore-keys: |"
  }
]