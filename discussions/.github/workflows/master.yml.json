[
  {
    "id" : "4f4ab974-258b-45d1-9c44-dea8494ab3cb",
    "prId" : 29333,
    "prUrl" : "https://github.com/apache/spark/pull/29333#pullrequestreview-466681337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b9b9a4f-9b53-42f4-9574-9b88c3c544b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "By doing this, it treats newlines as spaces, which  It makes the title prettier. Without this, [actions/upload-artifact](https://github.com/actions/upload-artifact) fails for unknown reason.",
        "createdAt" : "2020-08-13T11:40:30Z",
        "updatedAt" : "2020-08-13T13:30:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c36ec7a93b5fe1715016f918e3005673a480214f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +27,31 @@        # Note that the modules below are from sparktestsupport/modules.py.\n        modules:\n          - >-\n            core, unsafe, kvstore, avro,\n            network-common, network-shuffle, repl, launcher,"
  },
  {
    "id" : "14c59377-57cf-460d-b80e-6beb13bb35cf",
    "prId" : 29333,
    "prUrl" : "https://github.com/apache/spark/pull/29333#pullrequestreview-467292922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77b4c1db-5871-46b8-a243-b3f28ad14847",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If previous `Run tests` is passed without failure, do we still need run this? I remember Github Actions has some conditions other than `always()` can be used?",
        "createdAt" : "2020-08-14T04:00:25Z",
        "updatedAt" : "2020-08-14T04:00:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7be0c2ad-e952-4254-8d2b-43dd70b1905f",
        "parentId" : "77b4c1db-5871-46b8-a243-b3f28ad14847",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, if the tests don't fail, it should upload JUnit XML files and then report the successful test cases. e.g.) 1000 tests passed 6 skipped 0 failures.\r\n\r\nGitHub Actions has things like `failure()` but I think we should run this always (to report successful cases and also failed cases).",
        "createdAt" : "2020-08-14T04:01:37Z",
        "updatedAt" : "2020-08-14T04:05:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c36ec7a93b5fe1715016f918e3005673a480214f",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +180,184 @@    - name: Upload test results to report\n      if: always()\n      uses: actions/upload-artifact@v2\n      with:\n        name: test-results-${{ matrix.modules }}-${{ matrix.comment }}-${{ matrix.java }}-${{ matrix.hadoop }}-${{ matrix.hive }}"
  },
  {
    "id" : "72ef999a-fed7-496b-8a4b-26ee0ed566b6",
    "prId" : 29312,
    "prUrl" : "https://github.com/apache/spark/pull/29312#pullrequestreview-458913260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f701376-3a79-4124-96c1-08979c95e569",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I mean, how about just adding an end-to-end Scala 2.13 test? it will fail beyond core now, but that would help track the progress?",
        "createdAt" : "2020-07-31T01:13:46Z",
        "updatedAt" : "2020-07-31T01:13:46Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4ac89937-27c8-49fc-ade7-9f92a9f63199",
        "parentId" : "0f701376-3a79-4124-96c1-08979c95e569",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I prefer to have a green light always in three ways.\r\n1. If CI fails always, nobody will care about that.\r\n2. It cannot protect `core` itself. Without looking at the log detail, there is no difference between `core` failure and `beyond core` failure\r\n3. GitHub Action update is cheap because we can do that in a few hours (PR Passing) while Jenkins update is heavy.",
        "createdAt" : "2020-07-31T03:44:47Z",
        "updatedAt" : "2020-07-31T03:44:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4444f215-88f2-4aa5-b082-40db35639920",
        "parentId" : "0f701376-3a79-4124-96c1-08979c95e569",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If that is Jenkins Scala 2.13 job, it makes sense. But, GitHub Action failure is different. It's on our GitHub repo commit log",
        "createdAt" : "2020-07-31T03:48:04Z",
        "updatedAt" : "2020-07-31T03:48:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "639f92b9-019c-4ed5-aa33-bd5ca8df6b38",
        "parentId" : "0f701376-3a79-4124-96c1-08979c95e569",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "My only concern is that it becomes difficult to port these to periodical GitHub Actions jobs or merge it to the main GitHub Actions job (`Build modules: ...` above) later.\r\n\r\nEventually I would like to:\r\n- put those different profiles build into the matrix in the main job `Build modules: ...` above so we can leverage the optimization in `run-tests.py` (running only relevant jobs, etc.). We could decide which profiles to run by default or optionally.\r\n  - provide a way to set other optional profiles, for example, via GitHub PR title or tags, which we currently does via Jenkins.\r\n- set periodical jobs for other profiles for Maven, Scala, JDK, Hive, Hadoop, etc. that we run in Jenkins, or possibly more. I think we need to resolve SPARK-32264 first to do this.\r\n\r\nCurrently `run-tests.py` runs the build for all components always if Maven is used. So, I was also thinking about fixing `run-tests.py` script to support for Maven to run tests for only relevant components just like we do for SBT, and then we only have one main job (`Build modules: ...`) that adds tests by matrix or environment control in this file rather than adding separate jobs for different profiles. \r\n\r\nI guess I am okay with this as a temporary status for the time being if we all agree with this plan. I would like to encourage us to stick to this plan in long run.\r\n\r\nMaybe we could alternatively just think about setting a Jenkins that that runs all tests too, which will ideally be converted into periodic Jenkins jobs.\r\n\r\n",
        "createdAt" : "2020-07-31T04:47:13Z",
        "updatedAt" : "2020-07-31T04:47:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "18f16da3c1f50c095bc818e23eb44a2a2f38969c",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +296,300 @@        mkdir -p ~/.m2\n        dev/change-scala-version.sh 2.13\n        build/mvn test -pl core --am -Pscala-2.13\n        rm -rf ~/.m2/repository/org/apache/spark"
  },
  {
    "id" : "ac428fcd-e19f-4fc1-9dbd-96b6863a6ecb",
    "prId" : 29116,
    "prUrl" : "https://github.com/apache/spark/pull/29116#pullrequestreview-448589133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "097361b6-6c78-4978-b6cc-7ea4bdcd52d2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yarn test does not need other packages.",
        "createdAt" : "2020-07-15T02:35:54Z",
        "updatedAt" : "2020-07-15T02:35:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b47107fe616ca5882c5e817ac91b4c50593911c",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +150,154 @@        pypy3 -m pip list\n    - name: Install Python packages (Python 3.8)\n      if: contains(matrix.modules, 'pyspark') || (contains(matrix.modules, 'sql') && !contains(matrix.modules, 'sql-'))\n      run: |\n        python3.8 -m pip install numpy pyarrow pandas scipy"
  },
  {
    "id" : "a34209cb-0a77-4a49-9dba-b777cba43280",
    "prId" : 29086,
    "prUrl" : "https://github.com/apache/spark/pull/29086#pullrequestreview-447357182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2901d95-2d29-4318-9bcc-e47ab3df1ad7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oops. I missed this.",
        "createdAt" : "2020-07-13T15:26:12Z",
        "updatedAt" : "2020-07-13T15:26:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d8555a44d8157791e8140236a6eba9c09b2d5d8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +30,34 @@          - |-\n            core, unsafe, kvstore, avro,\n            network-common, network-shuffle, repl, launcher,\n            examples, sketch, graphx\n          - |-"
  },
  {
    "id" : "e7a4002a-847f-4e12-9c4f-837ba5c7c354",
    "prId" : 29076,
    "prUrl" : "https://github.com/apache/spark/pull/29076#pullrequestreview-446908430",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c812afa-6c9e-4a24-87c0-24affa6f5361",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For Github Actions running for push event, where I can look at the action logs? Still in \"Check\" tab of PRs? When will it be triggered? Merging?",
        "createdAt" : "2020-07-12T17:21:37Z",
        "updatedAt" : "2020-07-12T17:23:32Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4104c40b-c8e8-4d07-83fc-d0536269be4d",
        "parentId" : "8c812afa-6c9e-4a24-87c0-24affa6f5361",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We can see at two places.\r\n1. At commit log. At every commit log, the green check mark is attached.\r\n    - https://github.com/apache/spark/commits/master\r\n2. At commit info,\r\n    - https://github.com/apache/spark/commit/10a65ee9b42fd544e4698267e0cd16711ed92104\r\n\r\nCurrently, it's disabled yesterday. So, recent commits do not have that.",
        "createdAt" : "2020-07-12T20:43:02Z",
        "updatedAt" : "2020-07-12T20:43:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac96756eb4dadd8c3b204f976dc949f1c2220ddc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +4,8 @@  push:\n    branches:\n    - master\n  pull_request:\n    branches:"
  },
  {
    "id" : "6e473d42-c8e8-4a1e-966b-dc3db77b1ebf",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445691633",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b3f6722-4ea7-4d16-9033-81967e48743e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, BTW, @dongjoon-hyun. I happened to remove the JDK 11 builds here. I filed a JIRA to recover it at SPARK-32248. I promise I will take a look right away after this PR is merged.",
        "createdAt" : "2020-07-09T14:48:53Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +4,8 @@  pull_request:\n    branches:\n    - master\n\njobs:"
  },
  {
    "id" : "feabf191-1c88-46fe-8a53-af4ceaa27cff",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445696405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c7d1d9f-835f-41c2-abaf-c72f02106c42",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, do we need `Python2` still?",
        "createdAt" : "2020-07-09T14:52:55Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b724e1b8-0a28-4112-8438-fb909fadee33",
        "parentId" : "5c7d1d9f-835f-41c2-abaf-c72f02106c42",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Because we didn't drop yet at https://github.com/apache/spark/pull/28957 ðŸ˜¢ ",
        "createdAt" : "2020-07-09T14:53:41Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 218,
    "diffHunk" : "@@ -1,1 +138,142 @@        python3 -m pip install numpy pyarrow pandas scipy\n        python3 -m pip list\n        python2 -m pip install numpy pyarrow pandas scipy\n        python2 -m pip list\n        pypy3 -m pip install numpy pandas"
  },
  {
    "id" : "cfc1bd29-e414-4a43-b3c4-f52a5b3336e9",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-446058433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1cdd2690-3345-4360-a000-ea38b3635c50",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "hive-1.2 and hadoop-2.7 are also removed only for now?",
        "createdAt" : "2020-07-09T23:52:03Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8e3933de-504c-4b27-9971-d074d8863149",
        "parentId" : "1cdd2690-3345-4360-a000-ea38b3635c50",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I filed a JIRA to restore it at SPARK-32255. I was thinking we can periodically run. The current PR mimics our default PR builder behaviour for now.",
        "createdAt" : "2020-07-10T00:38:52Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "79ce68ab-74ac-4ba1-99cc-e2edc76b1950",
        "parentId" : "1cdd2690-3345-4360-a000-ea38b3635c50",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We can change hive and hadoop profile on Jenkins test by changing the PR title. I think it is not possible with Github Actions?",
        "createdAt" : "2020-07-10T00:48:12Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6024de37-73af-4711-9b91-9944611a3b78",
        "parentId" : "1cdd2690-3345-4360-a000-ea38b3635c50",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I think this isn't possible at this moment. It would be great if we can figure out a way to do it.",
        "createdAt" : "2020-07-10T00:55:20Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +20,24 @@          - hadoop3.2\n        hive:\n          - hive2.3\n        # TODO(SPARK-32246): We don't test 'streaming-kinesis-asl' for now.\n        # Kinesis tests depends on external Amazon kinesis service."
  }
]