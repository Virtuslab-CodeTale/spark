[
  {
    "id" : "333e6326-3656-47a3-b5fb-2074abbf3c38",
    "prId" : 33211,
    "prUrl" : "https://github.com/apache/spark/pull/33211#pullrequestreview-703469554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1aea976c-9064-460e-9e5f-a075d9b0b759",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Would it make sense to instead change the superclass appId impl?",
        "createdAt" : "2021-07-09T17:07:58Z",
        "updatedAt" : "2021-07-09T17:08:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "03de390a-ed02-4922-9988-72e9fe6f30a3",
        "parentId" : "1aea976c-9064-460e-9e5f-a075d9b0b759",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "The superclass appId i.e. `SchedulerBackend` is only used as fallback id beside `KubernetesClusterSchedulerBackend` then it doesn't make much sense. I prefer the current one.",
        "createdAt" : "2021-07-10T04:15:44Z",
        "updatedAt" : "2021-07-10T04:15:44Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d0c3dcf12483d9cf0ba6cec9a8e9e1a379e6a38",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +90,94 @@   */\n  override def applicationId(): String = {\n    conf.getOption(\"spark.app.id\").map(_.toString).getOrElse(appId)\n  }\n"
  },
  {
    "id" : "e8f1f044-8ccc-47a8-9b4c-663bf327a2ef",
    "prId" : 32288,
    "prUrl" : "https://github.com/apache/spark/pull/32288#pullrequestreview-642783382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98c8f134-500b-4318-8837-368da1b52fe8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So we don't delete PVC before?",
        "createdAt" : "2021-04-22T21:36:38Z",
        "updatedAt" : "2021-04-22T21:36:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "47e309a5-7afa-43c2-b39c-fedced68f8dd",
        "parentId" : "98c8f134-500b-4318-8837-368da1b52fe8",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Previously, the lifecycle is tied with the executor pod.\r\nNow, the lifecycle is tied with the driver pod. So, it will be deleted when the driver pod die.\r\nThis code is to support early deletion at the app termination.\r\nThis is the same one for `spark.kubernetes.driver.service.deleteOnTermination`~\r\n",
        "createdAt" : "2021-04-23T00:02:19Z",
        "updatedAt" : "2021-04-23T00:02:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b350f2500e222244b41351fe6d68437f896bb1e1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +140,144 @@        .withLabel(SPARK_APP_ID_LABEL, applicationId())\n        .delete()\n    }\n\n    if (shouldDeleteExecutors) {"
  },
  {
    "id" : "46a60649-4ad9-408e-98e1-53b68f416a78",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91648ca8-5c42-4c44-8a94-2c0ab88894fb",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I didn't fully follow the need for distinction b/w the K8s case and the simple executor triggered case. \r\n\r\nI thought K8s only needs the SIGPWR based thing, and indeed ExecutorDecommissioning is only sent in response to a SIGPWR.\r\n\r\nSo I am missing why we override `ExecutorDecommissioning` here and the motivation for `K8SDecommission`.",
        "createdAt" : "2020-09-18T17:36:54Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +194,198 @@\n    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n      case ExecutorDecommissioning(executorId) =>\n        context.reply(\n          decommissionExecutor("
  },
  {
    "id" : "2e5d93e5-d8c0-4998-a7d1-6ad690b39f62",
    "prId" : 27735,
    "prUrl" : "https://github.com/apache/spark/pull/27735#pullrequestreview-523936374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "857be482-0c19-4435-8a5d-ce089642d74f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add an empty line after this?",
        "createdAt" : "2020-11-05T05:32:26Z",
        "updatedAt" : "2020-11-16T06:49:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba93111b8ccfc7958e4facf57280a7c980beed84",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +75,79 @@    val configMap = KubernetesClientUtils.buildConfigMap(configMapName, confFilesMap, labels)\n    kubernetesClient.configMaps().create(configMap)\n  }\n\n  /**"
  },
  {
    "id" : "2a299df2-45c7-4bf8-97d6-376110a9cfd1",
    "prId" : 25236,
    "prUrl" : "https://github.com/apache/spark/pull/25236#pullrequestreview-271541540",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20816dea-4312-44aa-b73b-afd3e8b3826a",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "this seems bad.  If we get the response wrong, then the ExecutorAllocationManager will mistakenly update its internal state to think the executors have been removed, when they haven't been:\r\n\r\nhttps://github.com/apache/spark/blob/b29829e2abdebdf6fa9dd0a4a4cf4c9d676ee82d/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L448-L455\r\n\r\nwhich means we're expecting that call to kubernetes to delete the pods to be foolproof.\r\n\r\nWhy is it so bad to wait here?  Is it because we are holding locks when making this call in CoarseGrainedSchedulerBackend?  could that be avoided?",
        "createdAt" : "2019-08-01T15:01:36Z",
        "updatedAt" : "2019-08-05T18:20:47Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "42933223-904b-4c57-aabd-c59fb3230ca8",
        "parentId" : "20816dea-4312-44aa-b73b-afd3e8b3826a",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I added a longer comment explaining this.\r\n\r\nThe gist is:\r\n- it's bad to wait because it blocks the EAM thread (in this case for a really long time)\r\n- it's ok to return \"true\" because these executors will all die eventually, whether because of the shutdown message or because of the explicit kill.\r\n\r\nThe return value, to the best of my understanding, is not meant to say \"yes all executors have been killed\", but rather \"an attempt has been made to remove all of these executors, and they'll die eventually\".\r\n\r\n(Otherwise there would be no need for the EAM to track which executors are pending removal, since it would know immediately from this return value.)",
        "createdAt" : "2019-08-05T18:23:28Z",
        "updatedAt" : "2019-08-05T18:23:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "70cee081-8630-442a-b3cc-e77739d341ff",
        "parentId" : "20816dea-4312-44aa-b73b-afd3e8b3826a",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "ok, thanks, I buy that explanation",
        "createdAt" : "2019-08-06T18:42:43Z",
        "updatedAt" : "2019-08-06T18:43:11Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "635326a88a19e21f7644dbc8a149aa2dd1cb917c",
    "line" : 140,
    "diffHunk" : "@@ -1,1 +171,175 @@    // The cleanup timer above is just an optimization to make sure that stuck executors don't\n    // stick around in the k8s server. Normally it should never delete any pods at all.\n    Future.successful(true)\n  }\n"
  }
]