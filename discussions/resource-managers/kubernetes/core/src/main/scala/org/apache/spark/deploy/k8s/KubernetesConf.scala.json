[
  {
    "id" : "fc6f6023-87fc-489e-a918-4c473c2aa84b",
    "prId" : 33211,
    "prUrl" : "https://github.com/apache/spark/pull/33211#pullrequestreview-703469520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f259b98-ecd4-4fbb-a7d6-819cda1d2376",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We should still allow the user to specify their own app ID.",
        "createdAt" : "2021-07-09T17:07:16Z",
        "updatedAt" : "2021-07-09T17:08:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "1491bcaa-246f-4c6b-82b7-5ce2bd3f0e5f",
        "parentId" : "5f259b98-ecd4-4fbb-a7d6-819cda1d2376",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yea, we pick the app id from conf first in `KubernetesClusterSchedulerBackend`.",
        "createdAt" : "2021-07-10T04:14:56Z",
        "updatedAt" : "2021-07-10T04:14:56Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d0c3dcf12483d9cf0ba6cec9a8e9e1a379e6a38",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +228,232 @@  def getKubernetesAppId(): String =\n    s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\"\n\n  def getResourceNamePrefix(appName: String): String = {\n    val id = KubernetesUtils.uniqueID()"
  },
  {
    "id" : "a11875c5-226c-47f1-b020-309e5549663a",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-340860100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56298ea1-d6d1-43a3-82db-1f4f43ecfb1a",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I'd avoid adding getters for simple configs.",
        "createdAt" : "2020-01-06T20:37:26Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "c8f81a6a-e912-4285-a9e6-67b93fca1f73",
        "parentId" : "56298ea1-d6d1-43a3-82db-1f4f43ecfb1a",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Is that a hard preference? I find this easier to write.",
        "createdAt" : "2020-01-09T22:28:33Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "761e7bf0-b2b8-464c-bdb4-14a20b21dcd0",
        "parentId" : "56298ea1-d6d1-43a3-82db-1f4f43ecfb1a",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I prefer to avoid these, but don't care that much.",
        "createdAt" : "2020-01-09T22:38:58Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +56,60 @@  }\n\n  def workerDecommissioning: Boolean =\n    sparkConf.get(org.apache.spark.internal.config.Worker.WORKER_DECOMMISSION_ENABLED)\n"
  },
  {
    "id" : "e04058a1-9a48-4df8-89bd-a571cec5541c",
    "prId" : 25920,
    "prUrl" : "https://github.com/apache/spark/pull/25920#pullrequestreview-293298988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "122617a4-d383-42c7-b9ac-bff8d0211c98",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This is a nit, but you're using a filter in order to apply a check to each element. A foreach would be more appropriate and avoid the need to \"filter\" when the predicate won't matter.",
        "createdAt" : "2019-09-25T13:15:30Z",
        "updatedAt" : "2019-10-03T23:43:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "ed9d40f7-64e6-4b01-a3a6-09203e314c73",
        "parentId" : "122617a4-d383-42c7-b9ac-bff8d0211c98",
        "authorId" : "d2f417d7-1e93-4165-826f-b1a57ed82b64",
        "body" : "My goal is to filter and remove irregular elements in executorEnv, and to retain qualified elements;\r\n\r\n I think 'filter' is more appropriate. \r\n\r\nIsn't it more complicated to use \"foreach\"?",
        "createdAt" : "2019-09-25T18:49:53Z",
        "updatedAt" : "2019-10-03T23:43:41Z",
        "lastEditedBy" : "d2f417d7-1e93-4165-826f-b1a57ed82b64",
        "tags" : [
        ]
      },
      {
        "id" : "5796ac77-79ec-4832-9945-af7cb82110d9",
        "parentId" : "122617a4-d383-42c7-b9ac-bff8d0211c98",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oops, I misread this. Disregard.",
        "createdAt" : "2019-09-25T19:04:09Z",
        "updatedAt" : "2019-10-03T23:43:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "799d87f5fb68d084c6f4e00363f2d9f89b47354f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +150,154 @@  }\n\n  override def environment: Map[String, String] = sparkConf.getExecutorEnv.filter(\n    p => checkExecutorEnvKey(p._1)).toMap\n"
  }
]