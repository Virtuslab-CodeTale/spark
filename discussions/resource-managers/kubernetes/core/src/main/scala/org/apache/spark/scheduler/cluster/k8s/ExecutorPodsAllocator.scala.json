[
  {
    "id" : "00657760-ceb3-418f-aa1e-f9eec29cb965",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-713520912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3e320b3-ab8a-4da5-a0e5-cc1bc7584e05",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "This is a rename but the old name was misleading as it counts the pending PODs which  unknown by the scheduler.",
        "createdAt" : "2021-07-23T08:54:41Z",
        "updatedAt" : "2021-07-23T08:54:41Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +244,248 @@      // This variable is used later to print some debug logs. It's updated when cleaning up\n      // excess pod requests, since currentPendingExecutorsForRpId is immutable.\n      var pendingCountForRpId = currentPendingExecutorsForRpId.size\n\n      val newlyCreatedExecutorsForRpId ="
  },
  {
    "id" : "b84a75ed-be35-4562-9447-25f6c297f259",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-713522986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c12e82f-7019-4d47-a208-024bca9abd2f",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "This again a rename to avoid \"known\" prefix as it not for scheduler known PODs but PODs for this resource profile.",
        "createdAt" : "2021-07-23T08:57:21Z",
        "updatedAt" : "2021-07-23T08:57:21Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +276,280 @@        currentPendingExecutorsForRpId.size + schedulerKnownPendingExecsForRpId.size +\n        newlyCreatedExecutorsForRpId.size + schedulerKnownNewlyCreatedExecsForRpId.size\n      val podCountForRpId = currentRunningCount + notRunningPodCountForRpId\n\n      if (podCountForRpId > targetNum) {"
  },
  {
    "id" : "0713907d-f2aa-4e1e-ad8c-d09fe65028b0",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-713524325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "714d03d1-d37e-46f5-b124-1b7a267d6771",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Last rename for the same reason as earlier: this are PODs unknow by the scheduler so safe to be removed here (no task can be scheduled on them).",
        "createdAt" : "2021-07-23T08:59:11Z",
        "updatedAt" : "2021-07-23T08:59:11Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +284,288 @@            currentTime - createTime > executorIdleTimeout\n          }.keys.take(excess).toList\n        val pendingToDelete = currentPendingExecutorsForRpId\n          .filter(x => isExecutorIdleTimedOut(x._2, currentTime))\n          .take(excess - newlyCreatedToDelete.size)"
  },
  {
    "id" : "70bb4d1f-32fb-4e78-9dad-1dec2849952c",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-713542468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60a94339-d888-49cb-bce5-1aefc664d640",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "The condition previously used \"outstanding\" which contains pending (unknown by the scheduler) PODs as well but the old condition to call `requestNewExecutors` was:\r\n\r\n```\r\n  if (newlyCreatedExecutorsForRpId.isEmpty\r\n        && knownPodCount < targetNum) {\r\n        requestNewExecutors(targetNum, knownPodCount, applicationId, rpId, k8sKnownPVCNames)\r\n      }\r\n```\r\n\r\nSo here really the emptiness of `newlyCreatedExecutorsForRpId` should be checked.",
        "createdAt" : "2021-07-23T09:22:58Z",
        "updatedAt" : "2021-07-23T09:22:58Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +320,324 @@            \"equal to the number of requested executors. Not scaling up further.\")\n        } else {\n          if (newlyCreatedExecutorsForRpId.nonEmpty) {\n            logDebug(s\"Still waiting for ${newlyCreatedExecutorsForRpId.size} executors for \" +\n              s\"ResourceProfile Id $rpId before requesting more.\")"
  },
  {
    "id" : "d9bb5ba9-b365-4880-b92f-3d65e1c7549d",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-713544424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "062e1df7-b763-4240-9b35-89d81d9b22c9",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Instead of `foreach` here is `flatMap` as we need to do the process in two steps for counting all the not running PODs for all the resource profiles before we decide how to split the remaining pending PODs slot between the resource profiles.",
        "createdAt" : "2021-07-23T09:25:29Z",
        "updatedAt" : "2021-07-23T09:25:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +228,232 @@      .toSeq\n      .sortBy(_._1)\n      .flatMap { case (rpId, targetNum) =>\n      val podsForRpId = rpIdToExecsAndPodState.getOrElse(rpId, mutable.HashMap.empty)\n"
  },
  {
    "id" : "bfd53eec-8ee0-4ed7-9660-e5ecfe300392",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-717317477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a9e8d12-e469-457d-9837-5f41a40c4a0c",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I get this is the old behaviour, but I would think that we might allocate up to max pending so requiring newlyCreated to be empty seems strange to me.",
        "createdAt" : "2021-07-23T23:15:20Z",
        "updatedAt" : "2021-07-23T23:15:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "4e6e28e4-a86a-4091-aee4-5372715524aa",
        "parentId" : "0a9e8d12-e469-457d-9837-5f41a40c4a0c",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "This is kept intentionally, see https://github.com/apache/spark/pull/33492#discussion_r675961263.\r\n\r\nThe new limit is enforced at https://github.com/apache/spark/blob/adc512d4e1837713713fefc6f64af3b0c6c8cdc8/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala#L336-L339\r\n",
        "createdAt" : "2021-07-24T07:26:29Z",
        "updatedAt" : "2021-07-24T07:26:30Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "9e1ab5d7-2887-4300-8002-26580ec12ee0",
        "parentId" : "0a9e8d12-e469-457d-9837-5f41a40c4a0c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Cool :)",
        "createdAt" : "2021-07-28T18:03:42Z",
        "updatedAt" : "2021-07-28T18:03:42Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +332,336 @@        None\n      }\n    }\n\n    val remainingSlotFromPendingPods = maxPendingPods - totalNotRunningPodCount"
  },
  {
    "id" : "aa74a084-814c-4130-89b2-055ed701c613",
    "prId" : 33492,
    "prUrl" : "https://github.com/apache/spark/pull/33492#pullrequestreview-717931383",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f9b8bd6-2fa7-4171-b624-1ab59c243ce7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I removed the previous my comment because I'm also not sure what is the best way to inform the users this situation. Do you think we have a good way to inform to the users when we hit this limitation, @attilapiros ?",
        "createdAt" : "2021-07-28T23:55:05Z",
        "updatedAt" : "2021-07-28T23:55:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0db78ae8-ecfe-4e68-8e19-a0629cc4bdbe",
        "parentId" : "3f9b8bd6-2fa7-4171-b624-1ab59c243ce7",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "We could change this to logInfo:\r\nhttps://github.com/apache/spark/blob/adc512d4e1837713713fefc6f64af3b0c6c8cdc8/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala#L323-L324\r\n\r\nBut for a higher batch allocation size this message could be annoying as every POD status change will generate such a log line while it reaches 0.",
        "createdAt" : "2021-07-29T10:28:44Z",
        "updatedAt" : "2021-07-29T10:28:45Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e78f6c08a8a066cc52123c35a5de852b92457c4",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +330,334 @@      } else {\n        // for this resource profile we do not request more PODs\n        None\n      }\n    }"
  },
  {
    "id" : "b376e080-db13-45ed-8d90-dc678a32dab1",
    "prId" : 32752,
    "prUrl" : "https://github.com/apache/spark/pull/32752#pullrequestreview-675881629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f912a9b-699c-45d1-975e-1591cdf92d38",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we are going to just `logWarning`, the following code pattern will be better like the other places.\r\n```scala\r\nUtils.tryLogNonFatalError {\r\n  kubernetesClient.pods()\r\n    .withName(kubernetesDriverPodName.get)\r\n    .waitUntilReady(driverPodReadinessTimeout, TimeUnit.MINUTES)\r\n}\r\n```",
        "createdAt" : "2021-06-03T16:52:25Z",
        "updatedAt" : "2021-06-03T16:52:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "83972e88-cde8-4972-afbb-f34b76c37b56",
        "parentId" : "0f912a9b-699c-45d1-975e-1591cdf92d38",
        "authorId" : "da8d4675-e56b-4240-b43c-bac7c4a2f788",
        "body" : "If we just do `logWarning` and continue to create executor pods after this timeout, it's very likely executor pods will still hit the `UnknownHostException` issue.\r\nConsidering this, throwing a SparkException may be more intuitive and fail-fast. Please let me know your thoughts on this. Thanks.\r\nLike here:\r\nhttps://github.com/apache/spark/blob/0342dcb6289bae67f6ab742b8fd03b2d653e52ea/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsAllocator.scala#L73-L79\r\n",
        "createdAt" : "2021-06-03T17:39:01Z",
        "updatedAt" : "2021-06-03T17:39:02Z",
        "lastEditedBy" : "da8d4675-e56b-4240-b43c-bac7c4a2f788",
        "tags" : [
        ]
      },
      {
        "id" : "42a79b3f-43ab-4fd7-870e-790bba227d53",
        "parentId" : "0f912a9b-699c-45d1-975e-1591cdf92d38",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "-1 for `SparkException` idea. That is a huge regression.\r\n\r\n`UnknownHostException` is not a job blocker because Spark is going to create the required executors in any way eventually. If you throws SparkException, you change will be a breaker of the existing Spark pipelines indeed.",
        "createdAt" : "2021-06-04T00:10:15Z",
        "updatedAt" : "2021-06-04T00:10:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "31b37a83-194c-44a2-9dff-16f2ddca3fc1",
        "parentId" : "0f912a9b-699c-45d1-975e-1591cdf92d38",
        "authorId" : "da8d4675-e56b-4240-b43c-bac7c4a2f788",
        "body" : "As the [JIRA issue](https://issues.apache.org/jira/browse/SPARK-32975) describes, driver will not create new executors when this error happens.\r\n> Once this error happens, driver doesn't restart executor.\r\n\r\nHowever, other executors may be running fine. It's just that the number of healthy executors will be less than expected.    So I agree not to use `SparkException` here.",
        "createdAt" : "2021-06-04T02:36:04Z",
        "updatedAt" : "2021-06-04T02:36:04Z",
        "lastEditedBy" : "da8d4675-e56b-4240-b43c-bac7c4a2f788",
        "tags" : [
        ]
      },
      {
        "id" : "e552fa54-cf97-4d04-882c-8536806a9d2f",
        "parentId" : "0f912a9b-699c-45d1-975e-1591cdf92d38",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I don't think the error report is correct because Apache Spark doesn't use `restart` executor at all. The driver will keep creating new executors because the executors with `UnknownHostException` will be dead on the failure. I'm currently describing the Apache Spark behavior, not 3rd party operator behavior.",
        "createdAt" : "2021-06-04T03:04:26Z",
        "updatedAt" : "2021-06-04T03:04:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dbae0a7280471df9903c9a4b2a998f915968bc9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +110,114 @@        .withName(kubernetesDriverPodName.get)\n        .waitUntilReady(driverPodReadinessTimeout, TimeUnit.SECONDS)\n    }\n    snapshotsStore.addSubscriber(podAllocationDelay) {\n      onNewSnapshots(applicationId, schedulerBackend, _)"
  },
  {
    "id" : "b15492a6-36f7-47ba-a3c3-da0c607cf301",
    "prId" : 32564,
    "prUrl" : "https://github.com/apache/spark/pull/32564#pullrequestreview-660627292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8d5672b-cacd-4153-8237-637f8365b535",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Does it mean the PVCs here include both in-use and not in-use (from removed executors)?",
        "createdAt" : "2021-05-17T06:32:51Z",
        "updatedAt" : "2021-05-17T06:32:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "93fc8338-f609-49fc-a8b8-e263e79643f2",
        "parentId" : "d8d5672b-cacd-4153-8237-637f8365b535",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Spark only gets `snapshots` here. So, the snapshot cannot be the same with K8s AS-IS status. Always, there is a delay and some deleted executors will be notified at the next snapshot by being removed from the snapshot.\r\n> both in-use and not in-use (from removed executors)?\r\n\r\nIn addition, we cannot reclaim the PVC in the `Terminating`-status pods. Those PVCs are still in the snapshot and connected to the pods.",
        "createdAt" : "2021-05-17T06:42:04Z",
        "updatedAt" : "2021-05-17T06:44:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cbee88a1-b47d-4c34-8d8f-07b351e4301f",
        "parentId" : "d8d5672b-cacd-4153-8237-637f8365b535",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I guess this one is answered correctly.",
        "createdAt" : "2021-05-17T07:18:39Z",
        "updatedAt" : "2021-05-17T07:18:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b87803e241d6ea380884db477be6b35fdc86f137",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +128,132 @@    // Although we are going to delete some executors due to timeout in this function,\n    // it takes undefined time before the actual deletion. Hence, we should collect all PVCs\n    // in use at the beginning. False positive is okay in this context in order to be safe.\n    val k8sKnownPVCNames = snapshots.flatMap(_.executorPods.values.map(_.pod)).flatMap { pod =>\n      pod.getSpec.getVolumes.asScala"
  },
  {
    "id" : "3b595237-d558-4944-b866-2d0214da9a86",
    "prId" : 32564,
    "prUrl" : "https://github.com/apache/spark/pull/32564#pullrequestreview-673763724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2a264d8-8262-4dfd-b241-5800ed149115",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Add return type",
        "createdAt" : "2021-06-02T04:03:19Z",
        "updatedAt" : "2021-06-02T04:03:19Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "b87803e241d6ea380884db477be6b35fdc86f137",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +391,395 @@      pod: Pod,\n      resources: Seq[HasMetadata],\n      reusablePVCs: mutable.Buffer[PersistentVolumeClaim]) = {\n    val replacedResources = mutable.ArrayBuffer[HasMetadata]()\n    resources.foreach {"
  },
  {
    "id" : "00ec86a6-2c02-4ff8-b947-14ee10e4a880",
    "prId" : 32564,
    "prUrl" : "https://github.com/apache/spark/pull/32564#pullrequestreview-674523754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fc0edd2-2251-4175-b4d1-d3159b0f121b",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Set instead ?",
        "createdAt" : "2021-06-02T04:04:29Z",
        "updatedAt" : "2021-06-02T04:04:30Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "a83e8ec5-b92b-4e78-b875-68d8779bfef4",
        "parentId" : "7fc0edd2-2251-4175-b4d1-d3159b0f121b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya, maybe. I didn't try to add another semantic like adding uniqueness here.",
        "createdAt" : "2021-06-02T05:44:37Z",
        "updatedAt" : "2021-06-02T05:44:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7465ac26-5c58-4894-9800-aae242ca349e",
        "parentId" : "7fc0edd2-2251-4175-b4d1-d3159b0f121b",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "The only use of `replacedResources` is to do a `filterNot` on `resources` in this method - right ?\r\nSo whether unique or not does not matter ?",
        "createdAt" : "2021-06-02T17:54:08Z",
        "updatedAt" : "2021-06-02T17:54:08Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "620fc447-1a10-4e17-8537-bcea3de25d27",
        "parentId" : "7fc0edd2-2251-4175-b4d1-d3159b0f121b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "You are right~",
        "createdAt" : "2021-06-02T18:01:55Z",
        "updatedAt" : "2021-06-02T18:01:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b87803e241d6ea380884db477be6b35fdc86f137",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +392,396 @@      resources: Seq[HasMetadata],\n      reusablePVCs: mutable.Buffer[PersistentVolumeClaim]) = {\n    val replacedResources = mutable.ArrayBuffer[HasMetadata]()\n    resources.foreach {\n      case pvc: PersistentVolumeClaim =>"
  },
  {
    "id" : "27d4d1be-17d0-4bbf-a9bf-f9f97d6fd2b6",
    "prId" : 32288,
    "prUrl" : "https://github.com/apache/spark/pull/32288#pullrequestreview-642783451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98bca466-7fd1-450a-9a3b-7b5fc4f0be0a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This overrides the added owner reference at L338 `addOwnerReference(createdExecutorPod, resources)`, right?\r\n\r\n",
        "createdAt" : "2021-04-22T21:40:33Z",
        "updatedAt" : "2021-04-22T21:40:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6ef734aa-3748-4325-bbc7-288fa73796fa",
        "parentId" : "98bca466-7fd1-450a-9a3b-7b5fc4f0be0a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, correct!",
        "createdAt" : "2021-04-23T00:02:29Z",
        "updatedAt" : "2021-04-23T00:02:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b350f2500e222244b41351fe6d68437f896bb1e1",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +342,346 @@            if (conf.get(KUBERNETES_DRIVER_OWN_PVC) && driverPod.nonEmpty) {\n              addOwnerReference(driverPod.get, Seq(resource))\n            }\n            val pvc = resource.asInstanceOf[PersistentVolumeClaim]\n            logInfo(s\"Trying to create PersistentVolumeClaim ${pvc.getMetadata.getName} with \" +"
  },
  {
    "id" : "f09f23ff-f9b1-43a1-9d97-86ea0975dfda",
    "prId" : 31790,
    "prUrl" : "https://github.com/apache/spark/pull/31790#pullrequestreview-693378050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da36ade4-ea09-4cef-a324-3b5d34a4e16a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we reuse the old variable `totalPendingCount`? It looks resemble enough for `sumPendingPods` in the next context.",
        "createdAt" : "2021-06-27T05:10:21Z",
        "updatedAt" : "2021-06-27T05:10:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "61cdaab5057da929d0eea59caa87cd272c235c17",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +223,227 @@      }\n    }\n    var sumPendingPods = 0\n    // The order we request executors for each ResourceProfile is not guaranteed.\n    val knownPodsPerTargetForRpId = totalExpectedExecutorsPerResourceProfileId"
  },
  {
    "id" : "eff2091d-ab20-480f-a000-89c4621fb523",
    "prId" : 31513,
    "prUrl" : "https://github.com/apache/spark/pull/31513#pullrequestreview-598354954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f5c6218-9263-42c3-9139-d54da68f5036",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "This fixes an extra bug I found during testing. Without this change the 5.) point in my new test would fail as those pending pods which requested to be deleted earlier by the executor PODs allocator counted as available pending PODs for this resource profile (into the `currentPendingExecutorsForRpId`) and upscale is not triggered at:\r\n\r\n```\r\nif (knownPodCount > targetNum) {\r\n}\r\n```\r\n\r\nAs `knownPodCount` contains the `currentPendingExecutorsForRpId.size`.",
        "createdAt" : "2021-02-25T09:25:23Z",
        "updatedAt" : "2021-02-25T09:34:13Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb578d296e77af20b4c3a006ac0349a84630fdad",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +181,185 @@    }\n\n    val notDeletedPods = lastSnapshot.executorPods.filterKeys(!_deletedExecutorIds.contains(_))\n    // Map the pods into per ResourceProfile id so we can check per ResourceProfile,\n    // add a fast path if not using other ResourceProfiles."
  },
  {
    "id" : "44ddf916-3cd9-4b81-9468-2d169aa94075",
    "prId" : 30204,
    "prUrl" : "https://github.com/apache/spark/pull/30204#pullrequestreview-524366418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22e84670-1e50-435a-904b-86a8e34730e4",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "How often is `onNewSnapshots` called ?\r\nTrying to understand the cost of this method given the changes.",
        "createdAt" : "2020-11-03T09:42:03Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "9c32c940-edd3-466a-a80d-9ab621b7529d",
        "parentId" : "22e84670-1e50-435a-904b-86a8e34730e4",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this is called by the ExecutorPodsSnapshotsStoreImpl potentially pretty often. Its on start, whenever you set the total executors (which would be dynamic allocation manager updating things), and there is an allocation timer that fires (spark.kubernetes.allocation.batch.delay=1s default) that updates the allocator on any k8s pod changes. We only request small number of pods at a time and iterate, so if you request 20 executors, its definitely going to go through here a few times.\r\n\r\nSince it can potentially be called a lot, this is why I made the normal case with no resource profiles a fast path and constructing the resource profile to pod state only when we are using them.  I didn't actually profile that to see how slow it could be but was thinking it still shouldn't be that bad.  The alternative is to store the resource profile to pod state but this required a lot more changes to the snapshot store and lifecycle manager.",
        "createdAt" : "2020-11-05T15:17:41Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f5321ffe8637ae32434ac2d5eaff476fc3e0d19",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +122,126 @@    // assume that the pod was either never created (the API server never properly\n    // handled the creation request), or the API server created the pod but we missed\n    // both the creation and deletion events. In either case, delete the missing pod\n    // if possible, and mark such a pod to be rescheduled below.\n    val currentTime = clock.getTimeMillis()"
  },
  {
    "id" : "73a64643-919d-4020-82d8-2c1114b955c5",
    "prId" : 30155,
    "prUrl" : "https://github.com/apache/spark/pull/30155#pullrequestreview-517256451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df248631-aa63-469e-9c5e-1adc29972e34",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is irrelevant to `[SPARK-33231][CORE] Make pod allocation executor timeouts configurable`. Could you make another PR about this logic change?",
        "createdAt" : "2020-10-26T23:32:57Z",
        "updatedAt" : "2020-10-26T23:57:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "52c09244-3de4-47da-920b-d5f082aba62b",
        "parentId" : "df248631-aa63-469e-9c5e-1adc29972e34",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@holdenk . Please split this part.",
        "createdAt" : "2020-10-26T23:33:30Z",
        "updatedAt" : "2020-10-26T23:57:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "da0f6bb5-fd08-4b9c-91a1-92350c7d7a3a",
        "parentId" : "df248631-aa63-469e-9c5e-1adc29972e34",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So it's not unrelated, but I can give it a second JIRA? Having more time available means that depending on currentRunningCount can be stale information.",
        "createdAt" : "2020-10-26T23:53:41Z",
        "updatedAt" : "2020-10-26T23:57:50Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "90ad4d140a80df379384f81ffbbdfec758eb1b39",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +217,221 @@\n    if (newlyCreatedExecutors.isEmpty\n        && knownPodCount < currentTotalExpectedExecutors) {\n      val numExecutorsToAllocate = math.min(\n        currentTotalExpectedExecutors - knownPodCount, podAllocationSize)"
  },
  {
    "id" : "17452ca6-963b-463b-b28e-d0075ab5f526",
    "prId" : 30155,
    "prUrl" : "https://github.com/apache/spark/pull/30155#pullrequestreview-518031595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ab03261-1f14-4edf-b079-3798aa94ec97",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test case for `SPARK-33262`?",
        "createdAt" : "2020-10-27T18:44:31Z",
        "updatedAt" : "2020-10-27T18:44:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "90ad4d140a80df379384f81ffbbdfec758eb1b39",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +219,223 @@        && knownPodCount < currentTotalExpectedExecutors) {\n      val numExecutorsToAllocate = math.min(\n        currentTotalExpectedExecutors - knownPodCount, podAllocationSize)\n      logInfo(s\"Going to request $numExecutorsToAllocate executors from Kubernetes.\")\n      for ( _ <- 0 until numExecutorsToAllocate) {"
  },
  {
    "id" : "a9b7b088-8d1c-4c2f-b85d-8760b85e3ef7",
    "prId" : 26433,
    "prUrl" : "https://github.com/apache/spark/pull/26433#pullrequestreview-317958443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6ee1f73-6402-45ea-96cd-c731fac321f4",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I don't know about this necessarily being the best way to track the number of failed executors. In dynamic allocation it seems like if we scaled up and then down a few times this would give us a incorrect reading. Or is that taken care of elsewhere?",
        "createdAt" : "2019-11-15T17:49:29Z",
        "updatedAt" : "2019-11-15T17:52:08Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "15234dd5-9e11-47d6-a15b-816144558c9f",
        "parentId" : "d6ee1f73-6402-45ea-96cd-c731fac321f4",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Holden is right. To track failures you'll have to look at `ExecutorPodsLifecycleManager` instead.",
        "createdAt" : "2019-11-15T20:54:15Z",
        "updatedAt" : "2019-11-15T20:54:21Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "c38b27dc-3c3c-4a7b-b158-ad09d17acdb0",
        "parentId" : "d6ee1f73-6402-45ea-96cd-c731fac321f4",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "Oh, I will reconsider the acquisition of numFailedExecutors.",
        "createdAt" : "2019-11-16T07:47:32Z",
        "updatedAt" : "2019-11-16T07:47:32Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ef2cd70684da1cbb355db3fb8d78f816c24764b",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +159,163 @@        s\"${newlyCreatedExecutors.size} unacknowledged.\")\n\n      val numFailedExecutors =\n        EXECUTOR_ID_COUNTER.get() - currentPendingExecutors.size - currentRunningCount\n      if (numFailedExecutors >= maxNumExecutorFailures) {"
  },
  {
    "id" : "b14f190c-88f9-49b8-b01c-bf6c6edc20ad",
    "prId" : 26433,
    "prUrl" : "https://github.com/apache/spark/pull/26433#pullrequestreview-317862661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ab0ea6b-ae91-4b6e-b93b-37bbf2eb4fc5",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "This isn't a good way to stop the application. It may be running as part of other code that can recover from such a failure.\r\n\r\nYou should do something like stop the SparkContext instead.",
        "createdAt" : "2019-11-15T20:53:39Z",
        "updatedAt" : "2019-11-15T20:54:21Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ef2cd70684da1cbb355db3fb8d78f816c24764b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +163,167 @@      if (numFailedExecutors >= maxNumExecutorFailures) {\n        logError(s\"Max number of executor failures ($maxNumExecutorFailures) reached\")\n        System.exit(EXIT_MAX_EXECUTOR_FAILURES)\n      }\n    }"
  }
]