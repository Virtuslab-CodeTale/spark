[
  {
    "id" : "e95ace24-e165-4ee2-a1a6-64a6e9c473ff",
    "prId" : 32730,
    "prUrl" : "https://github.com/apache/spark/pull/32730#pullrequestreview-675888391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78cab6bd-0ccb-41dd-80bc-3bd2ed1b5dcb",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I don't see `dataFiles` and `indexFiles` are treated differently below, do we need to partition them?",
        "createdAt" : "2021-06-03T07:17:08Z",
        "updatedAt" : "2021-06-03T07:22:36Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4c2cb16c-cd58-44e0-9b44-ba63314ef903",
        "parentId" : "78cab6bd-0ccb-41dd-80bc-3bd2ed1b5dcb",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for review, @viirya . Apache Spark assumes that `dataFile` arrives first.\r\n- https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala#L599",
        "createdAt" : "2021-06-04T03:26:21Z",
        "updatedAt" : "2021-06-04T03:26:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9bf099c653ccd485e3409dc20f9d067974deb53",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +89,93 @@    val level = StorageLevel.DISK_ONLY\n    val (indexFiles, dataFiles) = files.partition(_.getName.endsWith(\".index\"))\n    (dataFiles ++ indexFiles).foreach { f =>\n      try {\n        val id = BlockId(f.getName)"
  },
  {
    "id" : "89e4dd1e-fd33-4245-ae0f-3be2d5536266",
    "prId" : 32730,
    "prUrl" : "https://github.com/apache/spark/pull/32730#pullrequestreview-681243034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cec64ebf-9516-4a73-8a6a-a0917989414a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we need to check the directory names too?",
        "createdAt" : "2021-06-09T21:20:25Z",
        "updatedAt" : "2021-06-09T21:20:26Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3bda9b4f-56a8-430c-977a-be7d38464065",
        "parentId" : "cec64ebf-9516-4a73-8a6a-a0917989414a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it's a protected approach which excludes some random files in the intermediate level.",
        "createdAt" : "2021-06-10T16:43:59Z",
        "updatedAt" : "2021-06-10T16:44:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cecd2340-c6d9-49e1-a225-b9a6820e089b",
        "parentId" : "cec64ebf-9516-4a73-8a6a-a0917989414a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, will you add check directory name (executor-xxx, blockmgr-xxx, etc) later?",
        "createdAt" : "2021-06-10T17:25:41Z",
        "updatedAt" : "2021-06-10T17:25:42Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6acc6bd6-4f33-4057-bd77-9c9f4aadcc8d",
        "parentId" : "cec64ebf-9516-4a73-8a6a-a0917989414a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, we can check the pattern. I will do later~",
        "createdAt" : "2021-06-10T20:44:04Z",
        "updatedAt" : "2021-06-10T20:44:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9bf099c653ccd485e3409dc20f9d067974deb53",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +78,82 @@          .flatMap(_.listFiles).filter(_.isDirectory) // executor-xxx\n          .flatMap(_.listFiles).filter(_.isDirectory) // blockmgr-xxx\n          .flatMap(_.listFiles).filter(_.isDirectory) // 00\n          .flatMap(_.listFiles)\n        if (files != null) files.toSeq else Seq.empty"
  },
  {
    "id" : "5545fa9c-1171-4005-8237-26b4f68f6c95",
    "prId" : 32730,
    "prUrl" : "https://github.com/apache/spark/pull/32730#pullrequestreview-681030958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "564b4a62-e2c0-43d1-a646-69243f8ec73c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why we need to get two-level parent? We store shuffle files outside local dirs?",
        "createdAt" : "2021-06-09T21:22:02Z",
        "updatedAt" : "2021-06-09T21:22:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8b6e298e-67ee-48f6-96ba-87e633f18e88",
        "parentId" : "564b4a62-e2c0-43d1-a646-69243f8ec73c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The current directory is the local directory for this executor. Here, we are trying to find dead executor's local directory. \r\n> We store shuffle files outside local dirs?",
        "createdAt" : "2021-06-10T16:53:32Z",
        "updatedAt" : "2021-06-10T16:53:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9bf099c653ccd485e3409dc20f9d067974deb53",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +70,74 @@    val files = Utils.getConfiguredLocalDirs(conf)\n      .filter(_ != null)\n      .map(s => new File(new File(new File(s).getParent).getParent))\n      .flatMap { dir =>\n        val oldDirs = dir.listFiles().filter { f =>"
  },
  {
    "id" : "23148640-76fc-4e1b-9208-d62b04daa070",
    "prId" : 32730,
    "prUrl" : "https://github.com/apache/spark/pull/32730#pullrequestreview-681023755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3af9bae3-07f4-45f7-b52a-f209985282fd",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it good to ignore the exception? Looks like we will lose the block?",
        "createdAt" : "2021-06-09T21:25:31Z",
        "updatedAt" : "2021-06-09T21:25:32Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bc3a5f8f-c37e-4eb7-a2e9-34f46eab24b5",
        "parentId" : "3af9bae3-07f4-45f7-b52a-f209985282fd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "No, we will not lose any valid block files. `UnrecognizedBlockId` means the file name is not in Spark-generated pattern. We are excluding non-Spark files. This is also a preventive approach.\r\n```scala\r\n@DeveloperApi\r\nclass UnrecognizedBlockId(name: String)\r\n    extends SparkException(s\"Failed to parse $name into a block ID\")\r\n```",
        "createdAt" : "2021-06-10T16:45:57Z",
        "updatedAt" : "2021-06-10T16:50:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9bf099c653ccd485e3409dc20f9d067974deb53",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +95,99 @@        bm.TempFileBasedBlockStoreUpdater(id, level, classTag, f, decryptedSize).save()\n      } catch {\n        case _: UnrecognizedBlockId =>\n      }\n    }"
  }
]