[
  {
    "id" : "5f1aaba4-325f-47b6-8bf5-911382959543",
    "prId" : 31935,
    "prUrl" : "https://github.com/apache/spark/pull/31935#pullrequestreview-632220021",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77e1db58-f1b7-4429-9c16-fa78cd847095",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this `assert` required in tests, @attilapiros ?\r\nI'm wondering when does this happen because we set this explicitly on both Maven/SBT test environment setting.",
        "createdAt" : "2021-04-09T04:28:32Z",
        "updatedAt" : "2021-04-09T04:28:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ee2e486e-f738-4f51-a5e5-c82cf3360c01",
        "parentId" : "77e1db58-f1b7-4429-9c16-fa78cd847095",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I followed the existing pattern:\r\n```\r\n$ rg \"spark.test.home\"\r\nsql/hive/src/test/scala/org/apache/spark/sql/hive/SparkSubmitTestUtils.scala\r\n46:      sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\")))\r\n\r\nsql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/plans/SQLHelper.scala\r\n90:    if (!(sys.props.contains(\"spark.test.home\") || sys.env.contains(\"SPARK_HOME\"))) {\r\n91:      fail(\"spark.test.home or SPARK_HOME is not set.\")\r\n93:    sys.props.getOrElse(\"spark.test.home\", sys.env(\"SPARK_HOME\"))\r\n\r\nproject/SparkBuild.scala\r\n1058:    javaOptions in Test += \"-Dspark.test.home=\" + sparkHome,\r\n\r\npom.xml\r\n262:    <spark.test.home>${session.executionRootDirectory}</spark.test.home>\r\n2654:              <spark.test.home>${spark.test.home}</spark.test.home>\r\n2706:              <spark.test.home>${spark.test.home}</spark.test.home>\r\n\r\npython/pyspark/tests/test_context.py\r\n306:        conf = SparkConf().set(\"spark.test.home\", SPARK_HOME)\r\n\r\npython/pyspark/tests/test_taskcontext.py\r\n309:        conf = SparkConf().set(\"spark.test.home\", SPARK_HOME)\r\n\r\ncore/src/main/scala/org/apache/spark/api/r/RUtils.scala\r\n36:    val sparkHome = sys.env.get(\"SPARK_HOME\").orElse(sys.props.get(\"spark.test.home\"))\r\n\r\ncore/src/main/scala/org/apache/spark/deploy/worker/Worker.scala\r\n148:      assert(sys.props.contains(\"spark.test.home\"), \"spark.test.home is not set!\")\r\n149:      new File(sys.props(\"spark.test.home\"))\r\n\r\ncore/src/test/scala/org/apache/spark/deploy/worker/CommandUtilsSuite.scala\r\n32:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n\r\ncore/src/test/scala/org/apache/spark/deploy/worker/ExecutorRunnerTest.scala\r\n29:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n\r\ncore/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala\r\n728:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n748:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n1537:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n\r\ncore/src/test/scala/org/apache/spark/DriverSuite.scala\r\n34:    val sparkHome = sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n\r\ncore/src/test/scala/org/apache/spark/util/SparkUncaughtExceptionHandlerSuite.scala\r\n29:    sys.props.getOrElse(\"spark.test.home\", fail(\"spark.test.home is not set!\"))\r\n\r\ncore/src/test/scala/org/apache/spark/launcher/LauncherBackendSuite.scala\r\n50:      .setSparkHome(sys.props(\"spark.test.home\"))\r\n\r\nresource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala\r\n150:      // Try the spark test home\r\n151:      sys.props(\"spark.test.home\")\r\n\r\nresource-managers/yarn/src/test/scala/org/apache/spark/launcher/TestClasspathBuilder.scala\r\n28:  childEnv.put(CommandBuilderUtils.ENV_SPARK_HOME, sys.props(\"spark.test.home\"))\r\n\r\nresource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala\r\n207:      .setSparkHome(sys.props(\"spark.test.home\"))\r\n310:    val sparkHome = sys.props(\"spark.test.home\")\r\n\r\nresource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala\r\n158:    launcher.setSparkHome(sys.props(\"spark.test.home\"))\r\n\r\nlauncher/src/test/java/org/apache/spark/launcher/SparkSubmitCommandBuilderSuite.java\r\n290:      System.getProperty(\"spark.test.home\"));\r\n308:      launcher.childEnv.put(\"SPARK_CONF_DIR\", System.getProperty(\"spark.test.home\")\r\n415:    builder.childEnv.put(CommandBuilderUtils.ENV_SPARK_HOME, System.getProperty(\"spark.test.home\"));\r\n\r\nlauncher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java\r\n250:      path = System.getProperty(\"spark.test.home\");\r\n``` ",
        "createdAt" : "2021-04-09T09:26:42Z",
        "updatedAt" : "2021-04-09T09:26:42Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6454b007679c0ef9152ae98a7fdabbf07860526",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +101,105 @@\n  test(\"Run SparkRemoteFileTest using a remote data file\", k8sTestTag) {\n    assert(sys.props.contains(\"spark.test.home\"), \"spark.test.home is not set!\")\n    TestUtils.withHttpServer(sys.props(\"spark.test.home\")) { baseURL =>\n      sparkAppConf"
  },
  {
    "id" : "6bce0cd9-620c-401a-9291-5600839482d4",
    "prId" : 29844,
    "prUrl" : "https://github.com/apache/spark/pull/29844#pullrequestreview-495949274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "735a1a96-ebd5-4a6a-b394-2a5f618d97d8",
        "parentId" : null,
        "authorId" : "4c995a1a-1668-4460-9df8-b8244472ae24",
        "body" : "should discuss whether desired behavior is \"executor SA defaults to driver\" or \"defaults to none\"",
        "createdAt" : "2020-09-24T15:52:01Z",
        "updatedAt" : "2020-09-25T00:17:06Z",
        "lastEditedBy" : "4c995a1a-1668-4460-9df8-b8244472ae24",
        "tags" : [
        ]
      },
      {
        "id" : "a8939969-e514-49be-a535-a8d4e6dfb0fb",
        "parentId" : "735a1a96-ebd5-4a6a-b394-2a5f618d97d8",
        "authorId" : "4c995a1a-1668-4460-9df8-b8244472ae24",
        "body" : "ignore this, since it is being pulled back from 3.0",
        "createdAt" : "2020-09-24T21:03:21Z",
        "updatedAt" : "2020-09-25T00:17:06Z",
        "lastEditedBy" : "4c995a1a-1668-4460-9df8-b8244472ae24",
        "tags" : [
        ]
      }
    ],
    "commit" : "81205a58cf53042017123c7b8d54778347ef608c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +85,89 @@  }\n\n  test(\"All pods have the same service account by default\", k8sTestTag) {\n    runSparkPiAndVerifyCompletion(\n      executorPodChecker = (executorPod: Pod) => {"
  },
  {
    "id" : "866c0a49-c064-4828-97ea-ee03222e3906",
    "prId" : 28561,
    "prUrl" : "https://github.com/apache/spark/pull/28561#pullrequestreview-413228415",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d124c6ba-756b-4a3c-beb3-1b87b1e107c0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One downside of this is, we will happen to keep this file location permanently from now on. Once we change the location, then it will break all builds in other branches. But I assume it's fine given that this file hasn't been moved for the last 7 years.",
        "createdAt" : "2020-05-18T01:44:01Z",
        "updatedAt" : "2020-05-18T01:44:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "77c2e14669f64c19d6068dbec695287b08f54205",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +112,116 @@     s\"$CONTAINER_LOCAL_FILE_DOWNLOAD_PATH/pagerank_data.txt\"\n  val REMOTE_PAGE_RANK_DATA_FILE =\n    \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/pagerank_data.txt\"\n  val REMOTE_PAGE_RANK_FILE_NAME = \"pagerank_data.txt\"\n}"
  },
  {
    "id" : "3486e6ad-bad7-44be-a080-1a72eb2e7e83",
    "prId" : 26161,
    "prUrl" : "https://github.com/apache/spark/pull/26161#pullrequestreview-307590382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19c137d1-fe34-46d5-8dfd-79cc9a00db73",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you are suggesting this PR as a bug fix, you need to add `SPARK-27900` prefix to the newly added test case names.",
        "createdAt" : "2019-10-24T21:21:21Z",
        "updatedAt" : "2019-10-24T21:21:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c811ae19-5d9c-450e-b5c4-4a1e64dd453a",
        "parentId" : "19c137d1-fe34-46d5-8dfd-79cc9a00db73",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "Ok I can do that.",
        "createdAt" : "2019-10-27T19:30:59Z",
        "updatedAt" : "2019-10-27T19:30:59Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "550508339d401750d9f1e74f2bcfcd9c83ed4427",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +106,110 @@  }\n\n  test(\"Run SparkPi without the default exit on OOM error flag\", k8sTestTag) {\n    sparkAppConf\n      .set(\"spark.driver.extraJavaOptions\", \"-Dspark.test.foo=spark.test.bar\")"
  },
  {
    "id" : "62332e22-b0fc-4c93-8a43-184f2d3ae7e9",
    "prId" : 26161,
    "prUrl" : "https://github.com/apache/spark/pull/26161#pullrequestreview-307590391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b37385f4-fdb3-456a-a5e2-c7c8ec2235e1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Let's not add this new line.",
        "createdAt" : "2019-10-24T21:21:50Z",
        "updatedAt" : "2019-10-24T21:21:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b6306b72-4b35-4fe8-81ad-64fa6013835a",
        "parentId" : "b37385f4-fdb3-456a-a5e2-c7c8ec2235e1",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "Ok.",
        "createdAt" : "2019-10-27T19:31:07Z",
        "updatedAt" : "2019-10-27T19:31:08Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "550508339d401750d9f1e74f2bcfcd9c83ed4427",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +138,142 @@  val REMOTE_PAGE_RANK_FILE_NAME = \"pagerank_data.txt\"\n}\n"
  },
  {
    "id" : "d8aa2547-bae0-49f8-b5a6-2a7329f17a9e",
    "prId" : 26161,
    "prUrl" : "https://github.com/apache/spark/pull/26161#pullrequestreview-307590396",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa70c281-80c2-4af2-bb73-ed2c78edbb0f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you try `SPARK_PRINT_LAUNCH_COMMAND` instead of new `DRIVER_VERBOSE`?",
        "createdAt" : "2019-10-24T21:23:58Z",
        "updatedAt" : "2019-10-24T21:24:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "46db0d63-d5b3-40a5-ad94-eece030ed783",
        "parentId" : "fa70c281-80c2-4af2-bb73-ed2c78edbb0f",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "Sure.",
        "createdAt" : "2019-10-27T19:31:11Z",
        "updatedAt" : "2019-10-27T19:31:11Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "550508339d401750d9f1e74f2bcfcd9c83ed4427",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +109,113 @@    sparkAppConf\n      .set(\"spark.driver.extraJavaOptions\", \"-Dspark.test.foo=spark.test.bar\")\n      .set(\"spark.kubernetes.driverEnv.DRIVER_VERBOSE\", \"true\")\n      .set(KILL_ON_OOM_ERROR.key, \"false\")\n    val output = Seq(\"Pi is roughly 3\","
  },
  {
    "id" : "73898e21-d57d-44e5-ad77-3f002ee27f64",
    "prId" : 25229,
    "prUrl" : "https://github.com/apache/spark/pull/25229#pullrequestreview-271096938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ce7e878-bb35-4d77-8ff3-6855236a25cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding these test cases.",
        "createdAt" : "2019-08-06T03:13:23Z",
        "updatedAt" : "2019-08-06T03:13:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cfcefc59a4a2c9c99b737ac4cb559885c8d9489",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +119,123 @@        \"-Dspark.test.foo=spark.test.bar)\")\n    runSparkPiAndVerifyCompletion(expectedLogOnCompletion = output)\n  }\n\n  test(\"Run SparkRemoteFileTest using a remote data file\", k8sTestTag) {"
  }
]