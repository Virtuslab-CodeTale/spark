[
  {
    "id" : "a57b2de2-9208-419a-a08c-2d987affc37f",
    "prId" : 30204,
    "prUrl" : "https://github.com/apache/spark/pull/30204#pullrequestreview-523969289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9623456e-fc5f-47fa-b02d-6c1b03ac86e7",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This will default to 0 if unspecified right ? (in case of custom recipes for docker images)\r\n",
        "createdAt" : "2020-11-03T09:48:24Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "c83f2e43-36d0-4dd8-bcd9-354a9ad8f402",
        "parentId" : "9623456e-fc5f-47fa-b02d-6c1b03ac86e7",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "if the --resourceProfileId parameter isn't specified then it defaults to 0, which is default profile, so things won't work properly for stage level scheduling, it will just look like another default executor.  Is that what you meant?\r\n\r\n",
        "createdAt" : "2020-11-05T00:16:07Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "20a24c08-98e6-4523-8325-ef0b0090c889",
        "parentId" : "9623456e-fc5f-47fa-b02d-6c1b03ac86e7",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Yes, thanks for clarifying !",
        "createdAt" : "2020-11-05T06:59:34Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f5321ffe8637ae32434ac2d5eaff476fc3e0d19",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +86,90 @@      --app-id $SPARK_APPLICATION_ID\n      --hostname $SPARK_EXECUTOR_POD_IP\n      --resourceProfileId $SPARK_RESOURCE_PROFILE_ID\n    )\n    ;;"
  }
]