[
  {
    "id" : "144d81cf-0da6-4b77-a7e3-92bb2e1c163f",
    "prId" : 30282,
    "prUrl" : "https://github.com/apache/spark/pull/30282#pullrequestreview-525876156",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2e13ae1-12eb-4cb4-a4fa-0237c1a6bdab",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "You can provide zip files in pyFiles or via `spark.submit.pyFiles` configuration.",
        "createdAt" : "2020-11-08T11:39:47Z",
        "updatedAt" : "2020-11-08T11:39:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5a1a31cf-298b-432c-b109-aced3fe7e991",
        "parentId" : "c2e13ae1-12eb-4cb4-a4fa-0237c1a6bdab",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "I tried to use --py-files, but it has path and resources like below:\r\n```\r\nPYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\r\n\r\nresources:\r\n    __spark_conf__ -> resource { scheme: \"hdfs\" host: \"MTPrime-CO4-fed\" port: -1 file: \"/user/zhonzh/.sparkStaging/application_1604622164128_7216/__spark_conf__.zip\" } size: 536359 timestamp: 1604858318432 type: ARCHIVE visibility: PRIVATE\r\n    pyspark.zip -> resource { scheme: \"hdfs\" host: \"MTPrime-CO4-fed\" port: -1 file: \"/user/zhonzh/.sparkStaging/application_1604622164128_7216/pyspark.zip\" } size: 595809 timestamp: 1604858311600 type: FILE visibility: PUBLIC\r\n    py4j-0.10.9-src.zip -> resource { scheme: \"hdfs\" host: \"MTPrime-CO4-fed\" port: -1 file: \"/user/zhonzh/.sparkStaging/application_1604622164128_7216/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1604858316398 type: FILE visibility: PUBLIC\r\n    __spark_libs__ -> resource { scheme: \"hdfs\" host: \"MTPrime-CO4-fed\" port: -1 file: \"/user/zhonzh/.sparkStaging/application_1604622164128_7216/spark-3.0.1-mt-jars.zip\" } size: 197891674 timestamp: 1604858291631 type: ARCHIVE visibility: PUBLIC\r\n    py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"MTPrime-CO4-fed\" port: -1 file: \"/user/zhonzh/.sparkStaging/application_1604622164128_7216/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1604858314170 type: FILE visibility: PUBLIC\r\n```\r\n\r\nI used `--py-files \"hdfs://MTPrime-CO4-0/user/zhonzh/pyspark.zip,hdfs://MTPrime-CO4-0/user/zhonzh/py4j-0.10.9-src.zip\"`. This is from spark3 while local python lib is spark 2.4.\r\n\r\nWe have 2 issues here:\r\n\r\n1. Local pyspark.zip is added first, it take precedence. This cause passed by pyFiles not working.\r\n2. If I use same name as pyspark.zip, the upload will be skipped as both have same name.\r\n\r\nWhat's your suggestions to handle this?\r\n",
        "createdAt" : "2020-11-08T18:20:06Z",
        "updatedAt" : "2020-11-08T18:20:07Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "2462abe3-6656-469d-906a-efd838925018",
        "parentId" : "c2e13ae1-12eb-4cb4-a4fa-0237c1a6bdab",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't think Spark is supposed to work with other py4j and pyspark.zip. They are built-in for each release.",
        "createdAt" : "2020-11-09T01:38:17Z",
        "updatedAt" : "2020-11-09T01:38:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "545dab707de0138d216f812ef4eaa9925de4c573",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +132,136 @@\n  private[spark] val SPARK_PYSPARK_ARCHIVE = ConfigBuilder(\"spark.yarn.pyspark.archives\")\n    .doc(\"Location of pyspark.zip and py4j.zip.\")\n    .version(\"3.0.1\")\n    .stringConf"
  },
  {
    "id" : "7618db5c-e789-494f-a08b-fb0b67ae3518",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-504081594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47a89111-12ab-422e-a0e9-01278a82fd89",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "I wonder if `spark.yarn.exclude.excludeOnFailure.enabled` or something might be better to bring it more in line with `spark.yarn.exclude.nodes`? I don't really see why \"launch\" appears in this config name...",
        "createdAt" : "2020-10-02T23:09:30Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "dbdeddce-fdc4-472e-995d-9ef9bf000d5f",
        "parentId" : "47a89111-12ab-422e-a0e9-01278a82fd89",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "because its excluding due to executor launch failures. I'd prefer to leave this. ",
        "createdAt" : "2020-10-07T14:15:41Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c4642e1c-7929-4644-94ed-008af9f073a2",
        "parentId" : "47a89111-12ab-422e-a0e9-01278a82fd89",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Sure, makes sense.",
        "createdAt" : "2020-10-07T16:53:40Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +382,386 @@  /* YARN allocator-level excludeOnFailure related config entries. */\n  private[spark] val YARN_EXECUTOR_LAUNCH_EXCLUDE_ON_FAILURE_ENABLED =\n    ConfigBuilder(\"spark.yarn.executor.launch.excludeOnFailure.enabled\")\n      .version(\"3.1.0\")\n      .withAlternative(\"spark.yarn.blacklist.executor.launch.blacklisting.enabled\")"
  },
  {
    "id" : "b9abeead-263f-40d7-9afe-3b9b65bc6322",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-518379773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "475b81fa-9605-4d58-a73a-eee85ccd3874",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we add it to deprecated config too?",
        "createdAt" : "2020-10-28T07:43:59Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +384,388 @@    ConfigBuilder(\"spark.yarn.executor.launch.excludeOnFailure.enabled\")\n      .version(\"3.1.0\")\n      .withAlternative(\"spark.yarn.blacklist.executor.launch.blacklisting.enabled\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "7c3af8d5-40dc-49b6-8a94-e797646df483",
    "prId" : 28788,
    "prUrl" : "https://github.com/apache/spark/pull/28788#pullrequestreview-428509999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "563282bc-3b84-4dd3-a4d9-fe1a8ca1b095",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we have `true` since it's the previous behavior of `spark.yarn.populateHadoopClasspath`?",
        "createdAt" : "2020-06-10T23:07:12Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5e04d144-2595-4420-bbec-f0c16a8b63c0",
        "parentId" : "563282bc-3b84-4dd3-a4d9-fe1a8ca1b095",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This variable naming is a little confusing because\r\n`IS_HADOOP_PROVIDED` is used for `spark.yarn.populateHadoopClasspath`.",
        "createdAt" : "2020-06-10T23:10:32Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4a3dbe45-226c-4e58-88f2-ac8158483e4d",
        "parentId" : "563282bc-3b84-4dd3-a4d9-fe1a8ca1b095",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If `spark.yarn.isHadoopProvided` doesn't exist, `IS_HADOOP_PROVIDED` becomes `false`. So, Hadoop is not provided. As a result, Spark needs to use the built-in Hadoop. But, this line seems to make `spark.yarn.populateHadoopClasspath=false`. IIRC, we need to populate hadoop class pathes if we are using built-in Hadoop, aren't we?",
        "createdAt" : "2020-06-10T23:13:09Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "02ee9b82-4485-4062-ae63-c1d9669ec00e",
        "parentId" : "563282bc-3b84-4dd3-a4d9-fe1a8ca1b095",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "When hadoop is provided, that means our Spark distribution doesn't have built-in hadoop; thus, we need to populate the hadoop classpath from Yarn. I can add comment there so people can understand it easier.",
        "createdAt" : "2020-06-10T23:35:49Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "716a355c-38ba-4d8d-af3f-8fd77566b3a9",
        "parentId" : "563282bc-3b84-4dd3-a4d9-fe1a8ca1b095",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~But, what you mean is inconsistent with the PR title, `Only populate Hadoop classpath for no-hadoop build`. This PR is trying to set `IS_HADOOP_PROVIDED (false)` to `spark.yarn.populateHadoopClasspath` always for Hadoop distribution too, isn't it?~\r\n\r\nSorry, now I got the correct idea. Thanks, @dbtsai .",
        "createdAt" : "2020-06-11T00:12:37Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2241528374cb64638204b21fd20a8c15cf931da",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +411,415 @@        log.warn(s\"Can not load the default value of `$propertyKey` from \" +\n          s\"`$configPath` with error, ${e.toString}. Using `false` as a default value.\")\n        false\n    }\n  }"
  },
  {
    "id" : "2f97ddf2-910a-4c94-8831-3a13520ca23f",
    "prId" : 28788,
    "prUrl" : "https://github.com/apache/spark/pull/28788#pullrequestreview-534170798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's better to update the migration guide at `core-migration-guide.md`.",
        "createdAt" : "2020-06-12T02:38:59Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1f3ce881-b0b2-4367-b06c-931bf7bc1b63",
        "parentId" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I also have thought about adding to migration guide. No yarn related migration guide, so maybe can put in core.",
        "createdAt" : "2020-06-12T03:33:01Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b8e75db0-810b-4dd0-9a4c-132785de4ed2",
        "parentId" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "I'll add it to migration guide as well.",
        "createdAt" : "2020-06-12T04:42:00Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "728d8d9b-a382-4652-8549-5eadc7841ba6",
        "parentId" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "I'll add it as a followup PR.",
        "createdAt" : "2020-06-18T06:04:09Z",
        "updatedAt" : "2020-06-18T06:04:10Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "f7cd4dee-2dab-413b-b04a-9fc6e5f28a45",
        "parentId" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dbtsai, seems like it has not been added yet. Could we add it soon before we forget :-)?",
        "createdAt" : "2020-06-29T05:44:23Z",
        "updatedAt" : "2020-06-29T05:44:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3673fdfc-c515-4cc5-8b7d-cd6909c2c6bd",
        "parentId" : "38a88454-26e4-4bae-a6d7-ee0adcac0db7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Gentle ping.",
        "createdAt" : "2020-11-19T08:17:31Z",
        "updatedAt" : "2020-11-19T08:17:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2241528374cb64638204b21fd20a8c15cf931da",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +81,85 @@    .version(\"2.4.6\")\n    .booleanConf\n    .createWithDefault(isHadoopProvided())\n\n  private[spark] val GATEWAY_ROOT_PATH = ConfigBuilder(\"spark.yarn.config.gatewayPath\")"
  },
  {
    "id" : "c76ccc01-746f-4296-9309-2fdff1c5ca24",
    "prId" : 28376,
    "prUrl" : "https://github.com/apache/spark/pull/28376#pullrequestreview-401984464",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14838679-f25c-46d3-9c07-f1ab504a6075",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we might add a comment saying requires bundling Hadoop with spark or user to provide Hadoop install separately.\r\nAlso need to add this to configuration.md doc",
        "createdAt" : "2020-04-28T13:53:52Z",
        "updatedAt" : "2020-04-29T20:55:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "10f33250-1c90-4edb-a0a1-52f6226f04ea",
        "parentId" : "14838679-f25c-46d3-9c07-f1ab504a6075",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for the comment and the doc.",
        "createdAt" : "2020-04-28T16:00:03Z",
        "updatedAt" : "2020-04-29T20:55:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f2504ae96dc2b1c6c70eb4dcd3b3b06d7642a6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +72,76 @@\n  private[spark] val POPULATE_HADOOP_CLASSPATH = ConfigBuilder(\"spark.yarn.populateHadoopClasspath\")\n    .doc(\"Whether to populate Hadoop classpath from `yarn.application.classpath` and \" +\n      \"`mapreduce.application.classpath` Note that if this is set to `false`, it requires \" +\n      \"a `with-Hadoop` Spark distribution that bundles Hadoop runtime or user has to provide \" +"
  }
]