[
  {
    "id" : "eb3e1d20-6890-4079-b2cc-a2e0b142d6e0",
    "prId" : 30375,
    "prUrl" : "https://github.com/apache/spark/pull/30375#pullrequestreview-530573689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c762ecc1-6eed-484f-b030-743c7eeb28b5",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Hadoop 2.7 only support Int type:\r\nhttps://github.com/apache/hadoop/blob/release-2.7.4-RC0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java#L56",
        "createdAt" : "2020-11-14T11:59:54Z",
        "updatedAt" : "2020-11-14T11:59:54Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60025042b86ecf27ef8d0698ce4dd256731d54ac",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +318,322 @@      }\n      val resource =\n        Resource.newInstance(resourcesWithDefaults.totalMemMiB.toInt, resourcesWithDefaults.cores)\n      ResourceRequestHelper.setResourceRequests(customResources, resource)\n      logDebug(s\"Created resource capability: $resource\")"
  },
  {
    "id" : "38ebe43a-e8b5-44cb-a5a8-c2eca7b9d46c",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-430087815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e060d34f-51b3-4f55-91ba-25a9d9d66ebe",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "This comment is superfluous.\r\n\r\nthank -> then",
        "createdAt" : "2020-06-12T23:55:16Z",
        "updatedAt" : "2020-06-13T02:14:58Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +451,455 @@    }\n\n    // If the flags is enabled than GRACEFUL_DECOMMISSION_ENABLE\n    // than handling the Node loss scenario using the decommission tracker.\n    if (sparkConf.get(GRACEFUL_DECOMMISSION_ENABLE)) {"
  },
  {
    "id" : "3b73e921-eeb2-402c-bd3b-5e4a9296c55b",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-430087815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6de3827-749b-432a-9eab-8df5b1003adb",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "How about a better variable name than 'x'.",
        "createdAt" : "2020-06-12T23:57:01Z",
        "updatedAt" : "2020-06-13T02:14:58Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +488,492 @@    if (getUpdatedNodes != null) {\n      val updatedNodes = getUpdatedNodes.asScala\n      for (x <- updatedNodes) {\n        if (x.getNodeState.toString.equals(NodeState.DECOMMISSIONING.toString)) {\n          // In hadoop 2.7 there is no support getDecommissioningTimeout whereas"
  },
  {
    "id" : "e8cdd455-5ae6-4b54-8264-93b985656831",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-430087815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e0a755c-b9d0-4e38-8289-8970ca9f2386",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "typo",
        "createdAt" : "2020-06-12T23:57:59Z",
        "updatedAt" : "2020-06-13T02:14:58Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +497,501 @@          var nodeTerminationTime = clock.getTimeMillis() + nodeLossInterval * 1000\n          try {\n              val decommiossioningTimeout = x.getClass.getMethod(\n                \"getDecommissioningTimeout\").invoke(x).asInstanceOf[Integer]\n              if (decommiossioningTimeout != null) {"
  },
  {
    "id" : "59558538-b70b-4066-9040-7894d2668d2a",
    "prId" : 27583,
    "prUrl" : "https://github.com/apache/spark/pull/27583#pullrequestreview-363573549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e47886d-19f1-40b7-aa03-e51adeeb67c9",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I am wondering if we want to make the locking semantics more formal in this class.\r\nEarlier, it was volatiles and concurrent hashmap (or sets back by concurrent hashmap) to eliminate need for locking - but lot of state changes were in context of 'this' being synchronized.\r\n\r\nDo we want to make sure all changes are guarded by a lock now ? Either use 'this' everywhere or some explicit private lock object and mark it via \"\\@GuardedBy\"\r\nIt is becoming slightly difficult too reason about the MT-safety of this class.\r\nDo you have any thoughts Tom ?",
        "createdAt" : "2020-02-17T11:37:38Z",
        "updatedAt" : "2020-02-28T03:38:28Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "171bd8d7-4a84-4c6c-8538-cd491bbc73ea",
        "parentId" : "3e47886d-19f1-40b7-aa03-e51adeeb67c9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sure let me take a look and see if I can simplify it.",
        "createdAt" : "2020-02-24T17:37:30Z",
        "updatedAt" : "2020-02-28T03:38:28Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd3509c044d3eca6895815a0e1347b97c1019f29",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +177,181 @@  // Number of cores per executor for the default profile\n  protected val defaultExecutorCores = sparkConf.get(EXECUTOR_CORES)\n\n  private val executorResourceRequests =\n    getYarnResourcesAndAmounts(sparkConf, config.YARN_EXECUTOR_RESOURCE_TYPES_PREFIX) ++"
  },
  {
    "id" : "ddf1630e-081c-4240-8282-1d9d829b7e90",
    "prId" : 27583,
    "prUrl" : "https://github.com/apache/spark/pull/27583#pullrequestreview-366532240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b658ee1-d191-466b-adbd-0651ebc91cd4",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Can there be a race such that rp.id is present in the map ?\r\nAnd if it does, should we be overwriting it here ?",
        "createdAt" : "2020-02-28T09:02:51Z",
        "updatedAt" : "2020-02-28T09:02:51Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "a11831e2-366a-4678-b638-fd0420d2bb51",
        "parentId" : "2b658ee1-d191-466b-adbd-0651ebc91cd4",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "no not at the moment anyway, this function is synchronized and no where else adds it so only one can run at a time. I put in putIfAbsent but it doesn't  really matter. ResourceProfile ids are unique and ResourceProfiles are immutable. Even if this code ran in multiple threads at the same time the result should be exactly the same so we would put the same thing in twice and it wouldn't matter which one got inserted first.\r\nStrictly speaking that doesn't need to be a concurrent hashmap due to locking of the calling functions but to be more strict on it and ot help with future changes I made it one.\r\nIf you think its more clear one way or another let me know and I can modify.",
        "createdAt" : "2020-02-28T14:38:23Z",
        "updatedAt" : "2020-02-28T14:38:23Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "291c1e4d-5c06-49bb-b58d-a9e29146389e",
        "parentId" : "2b658ee1-d191-466b-adbd-0651ebc91cd4",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "We changed rpIdToYarnResource to ConcurrentHashMap in commit \r\ne89a8b5 above from mutable.HashMap ... wanted to make sure this was only for concurrent reads and not writes which might insert keys here in parallel.",
        "createdAt" : "2020-02-28T16:39:13Z",
        "updatedAt" : "2020-02-28T16:39:14Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd3509c044d3eca6895815a0e1347b97c1019f29",
    "line" : 318,
    "diffHunk" : "@@ -1,1 +341,345 @@        ResourceRequestHelper.setResourceRequests(customResources.toMap, resource)\n        logDebug(s\"Created resource capability: $resource\")\n        rpIdToYarnResource.putIfAbsent(rp.id, resource)\n        rpIdToResourceProfile(rp.id) = rp\n      }"
  },
  {
    "id" : "624466e1-25de-445b-aaf5-4228c5d744a7",
    "prId" : 25309,
    "prUrl" : "https://github.com/apache/spark/pull/25309#pullrequestreview-282392385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95377b7d-699f-4bb9-9349-743f0290d88a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "According line 258 to 260 in `docs/configuration.md`, `memoryOverhead` includes `pysparkWorkerMemory`, but looks difference here.",
        "createdAt" : "2019-08-28T02:35:56Z",
        "updatedAt" : "2019-09-04T03:35:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "9d90c983-5e3a-46a8-ae91-b0166cf5fc92",
        "parentId" : "95377b7d-699f-4bb9-9349-743f0290d88a",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Described in the document is `Additional memory includes PySpark executor memory (when <code>spark.executor.pyspark.memory</code> is not configured)`, I think we need a new jira to discuss how to solve this problem.",
        "createdAt" : "2019-08-28T08:00:42Z",
        "updatedAt" : "2019-09-04T03:35:59Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "8016c821-c540-4f83-a3ad-597ddfafeb55",
        "parentId" : "95377b7d-699f-4bb9-9349-743f0290d88a",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Yes, this makes me confused.",
        "createdAt" : "2019-08-28T09:57:36Z",
        "updatedAt" : "2019-09-04T03:35:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "c1e8ec15-0fd7-4ada-88d2-9ae7f35e1b91",
        "parentId" : "95377b7d-699f-4bb9-9349-743f0290d88a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Isn't this what `spark.executor.memoryOverhead` already adds via `memoryOverhead`?\r\nI guess I'm wondering what `MEMORY_OFFHEAP_SIZE` does that's supposed to be different.",
        "createdAt" : "2019-09-01T15:19:43Z",
        "updatedAt" : "2019-09-04T03:35:59Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c8061527-ea71-44e6-b50f-a21c5c3b3d37",
        "parentId" : "95377b7d-699f-4bb9-9349-743f0290d88a",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@srowen `memoryOverhead` include `MEMORY_OFFHEAP_SIZE `  before this pr, and `memoryOverhead` and `MEMORY_OFFHEAP_SIZE ` had to be modified at the same time to ensure request enough resources from yarn if I want to increase `MEMORY_OFFHEAP_SIZE `, this is not user friendly, and this has always been confusing, why we need to modify two memory-related parameters simultaneously for one purpose? This pr let them be independent.",
        "createdAt" : "2019-09-02T03:49:42Z",
        "updatedAt" : "2019-09-04T03:35:59Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb2948811e42eba2b99a1d118a5eb4559cae57bb",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +152,156 @@  private[yarn] val resource: Resource = {\n    val resource = Resource.newInstance(\n      executorMemory + executorOffHeapMemory + memoryOverhead + pysparkWorkerMemory, executorCores)\n    ResourceRequestHelper.setResourceRequests(executorResourceRequests, resource)\n    logDebug(s\"Created resource capability: $resource\")"
  }
]