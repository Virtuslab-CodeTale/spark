[
  {
    "id" : "913ffe8f-6847-4f01-9869-a737549ed9bd",
    "prId" : 31591,
    "prUrl" : "https://github.com/apache/spark/pull/31591#pullrequestreview-636017642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33289098-ea5b-4e7f-84c0-7a3176a8e8fa",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : " This block of code (and the IllegalArgumentException below) -- convert to URI, check scheme, throw if bad scheme, assert on file existence -- is duplicated between `SparkSubmit` and `Client`. Can we put a method like `getIvySettingsFile` into `SparkSubmit` which is leveraged in both places?\n \n Not too big of a duplication so if it can't be done cleanly no worries from my end.",
        "createdAt" : "2021-04-14T15:22:32Z",
        "updatedAt" : "2021-04-14T19:33:26Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "b5629ae5-3c14-4522-acea-879ebdabccd7",
        "parentId" : "33289098-ea5b-4e7f-84c0-7a3176a8e8fa",
        "authorId" : "a9818936-8272-4717-aeaf-31025433b84d",
        "body" : "The assertions of file existence are slightly different in both places. In YarnClient we would want to make this assertion only for the `file` scheme before we process it, however in `SparkSubmit` we would make to make the assertion for all schemes after the appropriate download steps. E.g. for `local` scheme too we would want to validate that the final file that `loadIvySettings` will process does exist.\r\nOnce we add handling for `local` and `hdfs` schemes, the functions in both classes will differ even more. So I think it is okay to have duplication here.",
        "createdAt" : "2021-04-14T19:41:58Z",
        "updatedAt" : "2021-04-14T21:35:10Z",
        "lastEditedBy" : "a9818936-8272-4717-aeaf-31025433b84d",
        "tags" : [
        ]
      },
      {
        "id" : "79e6bdf5-9636-42b3-9452-ef03b38fdbea",
        "parentId" : "33289098-ea5b-4e7f-84c0-7a3176a8e8fa",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "SGTM thanks!",
        "createdAt" : "2021-04-14T19:44:20Z",
        "updatedAt" : "2021-04-14T19:44:21Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd3ddb2bd60fd34e605479eb07c2d7021f3c44e0",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +531,535 @@            require(ivySettingsFile.isFile(), s\"Ivy settings file $ivySettingsFile is not a\" +\n              \"normal file\")\n            // Generate a file name that can be used for the ivySettings file, that does not\n            // conflict with any user file.\n            val localizedFileName = Some(ivySettingsFile.getName() + \"-\" +"
  },
  {
    "id" : "eb9696f8-a5d5-4f6e-8210-4f9225598e53",
    "prId" : 30581,
    "prUrl" : "https://github.com/apache/spark/pull/30581#pullrequestreview-543951766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e674bf1-d62d-4304-b790-01b523ae6c09",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is also unrelated changes but I just piggybacked here as it looks pretty clear to me.",
        "createdAt" : "2020-12-04T05:35:06Z",
        "updatedAt" : "2020-12-04T05:52:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f048008d061bbed27e8df6831b3b5386088669f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1630,1634 @@    conf.remove(JARS)\n    conf.remove(FILES)\n    conf.remove(ARCHIVES)\n\n    new Client(new ClientArguments(args), conf, null).run()"
  },
  {
    "id" : "a954517f-32e2-4443-b07d-ed647d835146",
    "prId" : 30096,
    "prUrl" : "https://github.com/apache/spark/pull/30096#pullrequestreview-523729706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddae59e5-0eeb-4c0e-a739-4838ef5a54ac",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I would prefer to see this configurable as this could add quite a bit of load to the RM UI.  Also do we really need to print this every time we do the report details when the state changes?  It seems like you only need this when the driver starts, which would help with the load on RM.",
        "createdAt" : "2020-11-04T19:59:55Z",
        "updatedAt" : "2020-11-05T16:21:00Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "2407b568-d149-44b4-a024-b717a0bf2379",
        "parentId" : "ddae59e5-0eeb-4c0e-a739-4838ef5a54ac",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Sure, added a configuration for this.\r\n\r\nYou can't necessarily just do it a single time, since with different application attempts you can get a different driver link. Given that there is already logic in place to only print the report when the status changes, this seems like a pretty low cost. We already poll the RM API to get an `ApplicationReport` once per second (by default). By comparison, this new logic will cause the `appattempts` API to get hit just a few times over the entire lifetime of the application (`ACCEPTED`, `RUNNING`, `FINISHED`). So the relative cost is very low vs. existing logic (up to hundreds or thousands of calls to get application report vs. 3-ish calls to get driver log links).\r\n\r\nThis assumes that you don't have DEBUG logging enabled. One thing we can consider is only updating the driver logs link if `lastState != state`, regardless of the log level.",
        "createdAt" : "2020-11-04T20:58:03Z",
        "updatedAt" : "2020-11-05T16:21:00Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "42212a8c566776d92d9910e86aff2343055e6680",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1089,1093 @@          logDebug(formatReportDetails(report, getDriverLogsLink(report.getApplicationId)))\n        } else if (lastState != state) {\n          logInfo(formatReportDetails(report, getDriverLogsLink(report.getApplicationId)))\n        }\n      }"
  },
  {
    "id" : "5dd0ccaa-b8e1-410f-9535-c0988c345a99",
    "prId" : 28688,
    "prUrl" : "https://github.com/apache/spark/pull/28688#pullrequestreview-421706613",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ee1ddb0-f66f-4361-b52c-f34f16258db2",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "is it still useful to print the number of node managers?",
        "createdAt" : "2020-06-01T04:52:07Z",
        "updatedAt" : "2020-06-01T09:21:27Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "618dfd2a-bd28-48d1-88b3-0fb2229fe846",
        "parentId" : "6ee1ddb0-f66f-4361-b52c-f34f16258db2",
        "authorId" : "86dfe414-6e61-475a-8e63-0ae110ceb946",
        "body" : "if it can call `yarnClient.getYarnClusterMetrics.getNumNodeManagers` without exception, i tend to keep the original log info.",
        "createdAt" : "2020-06-01T09:23:06Z",
        "updatedAt" : "2020-06-01T09:23:07Z",
        "lastEditedBy" : "86dfe414-6e61-475a-8e63-0ae110ceb946",
        "tags" : [
        ]
      }
    ],
    "commit" : "28a59a80f9ca63aed095782209155a807308db24",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +176,180 @@          s\" ${hadoopConf.get(YarnConfiguration.RM_ADDRESS,\n            YarnConfiguration.DEFAULT_RM_ADDRESS)}\" +\n          s\" with %d NodeManagers.\".format(yarnClient.getYarnClusterMetrics.getNumNodeManagers))\n      } catch {\n        case NonFatal(e) =>"
  },
  {
    "id" : "26d815a2-0b1c-48d1-91f3-1e0e2b97ff67",
    "prId" : 28688,
    "prUrl" : "https://github.com/apache/spark/pull/28688#pullrequestreview-423127827",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cd7b1e9-42c6-4840-a955-99792a9df579",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "nit: Remove the \"s\" because we don't include any expression in this line.",
        "createdAt" : "2020-06-02T23:41:28Z",
        "updatedAt" : "2020-06-02T23:41:29Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "28a59a80f9ca63aed095782209155a807308db24",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +173,177 @@\n      try {\n        logInfo(s\"Requesting a new application from cluster\" +\n          s\" ${hadoopConf.get(YarnConfiguration.RM_ADDRESS,\n            YarnConfiguration.DEFAULT_RM_ADDRESS)}\" +"
  },
  {
    "id" : "d3a069c7-078d-4cc5-a5a3-4a2fcaa0904b",
    "prId" : 28688,
    "prUrl" : "https://github.com/apache/spark/pull/28688#pullrequestreview-423128471",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c655edf1-12a9-45b2-a34f-db95a38545f9",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I would make a variable outside to represent the RM_ADDRESS value.",
        "createdAt" : "2020-06-02T23:43:25Z",
        "updatedAt" : "2020-06-02T23:43:25Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "28a59a80f9ca63aed095782209155a807308db24",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +180,184 @@        case NonFatal(e) =>\n          logWarning(s\"Failed to request YARN cluster information from cluster \" +\n            s\"${hadoopConf.get(YarnConfiguration.RM_ADDRESS,\n              YarnConfiguration.DEFAULT_RM_ADDRESS)}\" + \" with excepation: $e\")\n      }"
  },
  {
    "id" : "139c3644-656a-491b-921a-c79e64df44fc",
    "prId" : 28688,
    "prUrl" : "https://github.com/apache/spark/pull/28688#pullrequestreview-423128637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01887545-c258-47bc-967d-6b6a4ee29c95",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "nit: `excepation` -> `exception`",
        "createdAt" : "2020-06-02T23:43:51Z",
        "updatedAt" : "2020-06-02T23:43:52Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "28a59a80f9ca63aed095782209155a807308db24",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +181,185 @@          logWarning(s\"Failed to request YARN cluster information from cluster \" +\n            s\"${hadoopConf.get(YarnConfiguration.RM_ADDRESS,\n              YarnConfiguration.DEFAULT_RM_ADDRESS)}\" + \" with excepation: $e\")\n      }\n"
  },
  {
    "id" : "11c0870c-884e-4ab0-b718-dd4495b07b96",
    "prId" : 27598,
    "prUrl" : "https://github.com/apache/spark/pull/27598#pullrequestreview-427675667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "459f79de-d549-4fbf-b954-9bbd46e57228",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Does it work when Spark is not installed in other nodes? IIRC, we can run the application in the cluster where Spark is not installed because the jars are shipped together in Yarn cluster.\r\n\r\nLikewise, PySpark was able to run. From my very cursory look, it's going to break this case because it will not distribute the local pyspark archive anymore. Can you confirm this @shanyu and @tgravescs?",
        "createdAt" : "2020-06-09T01:33:51Z",
        "updatedAt" : "2020-06-09T01:33:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2e0d30d7-1aed-4b33-ad9e-a0faf946a4a7",
        "parentId" : "459f79de-d549-4fbf-b954-9bbd46e57228",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this is the case someone explicitly put local: on the url so its expected to be on every machine.   YARN distributes everything that is file: or downloads it if its hdfs:",
        "createdAt" : "2020-06-09T13:53:24Z",
        "updatedAt" : "2020-06-09T13:53:25Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "889bcb93-53bb-4591-b8d9-9610f0d70ac6",
        "parentId" : "459f79de-d549-4fbf-b954-9bbd46e57228",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay, thanks for clarification. LGTM",
        "createdAt" : "2020-06-10T02:51:39Z",
        "updatedAt" : "2020-06-10T02:51:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "20a7a9c82510d1f26953b3884a4824c5dfa65e47",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +636,640 @@    }\n\n    pySparkArchives.foreach { f =>\n      val uri = Utils.resolveURI(f)\n      if (uri.getScheme != Utils.LOCAL_SCHEME) {"
  }
]