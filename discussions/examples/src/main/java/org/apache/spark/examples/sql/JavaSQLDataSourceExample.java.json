[
  {
    "id" : "c991907a-92b7-4f34-833c-f37061851746",
    "prId" : 27415,
    "prUrl" : "https://github.com/apache/spark/pull/27415#pullrequestreview-351584679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2153b700-1d5b-429b-ae5e-0fdc3ceb850d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we show the result like the previous example? ",
        "createdAt" : "2020-01-31T08:50:42Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "de617b15-db04-48c3-af58-c6882da44b67",
        "parentId" : "2153b700-1d5b-429b-ae5e-0fdc3ceb850d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this is from an existing example. I think we should rewrite it to use the same file set. How about using `*.parquet`?",
        "createdAt" : "2020-01-31T08:52:39Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "821d986d-27dc-4140-bbf9-380ef2181aae",
        "parentId" : "2153b700-1d5b-429b-ae5e-0fdc3ceb850d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "and see if we can remove `partitioned_users.orc`",
        "createdAt" : "2020-01-31T08:53:08Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5384d03e-0015-44e1-9823-b4fd409872f6",
        "parentId" : "2153b700-1d5b-429b-ae5e-0fdc3ceb850d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "After checking the partitioned_users.orc, it's a demo for hive partitioned table, add the description for `partitioned_users.orc` in the doc. Done in 1de38be.",
        "createdAt" : "2020-01-31T15:32:24Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "b06a88416ecbe04513c74131d338f2e507c3d8c8",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +128,132 @@    Dataset<Row> partitionedUsersDF = spark.read().format(\"orc\")\n            .option(\"pathGlobFilter\", \"*.orc\")\n            .load(\"examples/src/main/resources/partitioned_users.orc\");\n    partitionedUsersDF.show();\n    // +------+----------------+--------------+"
  },
  {
    "id" : "11f7b4be-c5fd-4ea3-b69c-a7952de57bff",
    "prId" : 27415,
    "prUrl" : "https://github.com/apache/spark/pull/27415#pullrequestreview-351846214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5abe7e70-5a18-42b8-81b1-b02ec1b48f04",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Why do we need this",
        "createdAt" : "2020-01-31T09:16:09Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "fb29f12b-132b-4427-9b19-0924487488a2",
        "parentId" : "5abe7e70-5a18-42b8-81b1-b02ec1b48f04",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "turn off the feature to not affect the following examples that test other features.",
        "createdAt" : "2020-01-31T12:14:08Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "214e471c-a183-450b-b531-e51db3195242",
        "parentId" : "5abe7e70-5a18-42b8-81b1-b02ec1b48f04",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think it is confusing here. The conf `spark.sql.files.ignoreMissingFiles` is disabled by default.",
        "createdAt" : "2020-01-31T19:05:44Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "8f04bef3-2393-444c-9a6f-8b506ca211a8",
        "parentId" : "5abe7e70-5a18-42b8-81b1-b02ec1b48f04",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "I guess setting the config explicitly here is for corresponding with the doc: https://github.com/apache/spark/pull/27415/files#diff-dab20ee598064a54fecef8842b972944R72",
        "createdAt" : "2020-02-01T00:28:41Z",
        "updatedAt" : "2020-02-01T00:30:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "b06a88416ecbe04513c74131d338f2e507c3d8c8",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +124,128 @@    // +-------------+\n    // $example off:ignore_corrupt_files$\n    spark.sql(\"set spark.sql.files.ignoreMissingFiles=false\");\n    // $example on:load_with_path_glob_filter$\n    Dataset<Row> partitionedUsersDF = spark.read().format(\"orc\")"
  },
  {
    "id" : "71de300d-f0f3-41c9-897f-1a12f54556a6",
    "prId" : 27302,
    "prUrl" : "https://github.com/apache/spark/pull/27302#pullrequestreview-352780060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9636712-0474-4362-9981-12dd5acf07a5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we reset the config before running the path glob filter example? Otherwise it's hard to prove that if it's the ignore-corrupt-files or the path-glob-filter takes effect.\r\n\r\ncc @gengliangwang ",
        "createdAt" : "2020-02-04T08:01:02Z",
        "updatedAt" : "2020-02-05T08:01:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "df9f556cf0eeb284c869624bfe311ab6826b78fc",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +136,140 @@    // +-------------+\n    // $example off:recursive_file_lookup$\n    spark.sql(\"set spark.sql.files.ignoreCorruptFiles=false\");\n    // $example on:load_with_path_glob_filter$\n    Dataset<Row> testGlobFilterDF = spark.read().format(\"parquet\")"
  }
]