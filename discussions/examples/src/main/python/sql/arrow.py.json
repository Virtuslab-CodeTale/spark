[
  {
    "id" : "b1dccdc7-94aa-4fc9-9123-1586ca0da339",
    "prId" : 29591,
    "prUrl" : "https://github.com/apache/spark/pull/29591#pullrequestreview-486048431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51306188-0d63-415c-a968-f493217beebf",
        "parentId" : null,
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Numpy has hinting as well, why ignore it? I got this working on https://github.com/zero323/pyspark-stubs/pull/464",
        "createdAt" : "2020-09-09T05:34:17Z",
        "updatedAt" : "2020-09-23T13:52:42Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "359ce241-b84f-4398-81c6-8267e8cef03c",
        "parentId" : "51306188-0d63-415c-a968-f493217beebf",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Because, AFAIK, released versions are not typed ‒ doesn't seem there were any released versions (nothing for 1.19.1 ‒https://github.com/numpy/numpy/tree/v1.19.1/numpy) since https://github.com/numpy/numpy/commit/11b95d15f10c2bc652ed19d5e27efa0384396cb8.",
        "createdAt" : "2020-09-10T15:20:10Z",
        "updatedAt" : "2020-09-23T13:52:42Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "fab00f1107beb301d093618d5c797799df768253",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +33,37 @@\ndef dataframe_with_arrow_example(spark):\n    import numpy as np  # type: ignore[import]\n    import pandas as pd  # type: ignore[import]\n"
  },
  {
    "id" : "93fc9a97-247c-495a-aebf-19cf5114d20b",
    "prId" : 29548,
    "prUrl" : "https://github.com/apache/spark/pull/29548#pullrequestreview-477226248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f301003-acbd-4dc2-9e5d-64865a8d4c84",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`pyspark.sql.pandas.functions`?",
        "createdAt" : "2020-08-28T00:18:23Z",
        "updatedAt" : "2020-08-28T04:07:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5fdf51a0-3b04-4a0b-960b-c55facd9a56e",
        "parentId" : "8f301003-acbd-4dc2-9e5d-64865a8d4c84",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Actually `pyspark.sql.pandas` is kind of an internal module. It is exposed through `pyspark.sql.functions`.",
        "createdAt" : "2020-08-28T04:03:28Z",
        "updatedAt" : "2020-08-28T04:07:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56af1022a43e59f24b2d91e49c8a8a97f7f457cc",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +54,58 @@    import pandas as pd\n\n    from pyspark.sql.functions import pandas_udf\n\n    @pandas_udf(\"col1 string, col2 long\")"
  }
]