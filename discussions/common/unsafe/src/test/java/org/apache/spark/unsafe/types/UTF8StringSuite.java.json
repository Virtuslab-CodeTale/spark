[
  {
    "id" : "980788e5-8a68-48ff-967d-92576d72de69",
    "prId" : 31164,
    "prUrl" : "https://github.com/apache/spark/pull/31164#pullrequestreview-570094290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85f5dbd8-9dab-437a-bfdc-4f1c25a35916",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Why do these need to be strings now? all seem to be 1 char.",
        "createdAt" : "2021-01-16T15:43:53Z",
        "updatedAt" : "2021-01-16T15:43:53Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d4ef7688-4f70-4f72-845e-267508629c5f",
        "parentId" : "85f5dbd8-9dab-437a-bfdc-4f1c25a35916",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "The type parameter of `translate` is changed from `Map<Character, Character>` to `Map<String, String>` to support >=`U+10000` unicode characters so this change is necessary. Those characters can't be represented as `Character`.",
        "createdAt" : "2021-01-17T21:06:05Z",
        "updatedAt" : "2021-01-17T21:10:19Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "0bc8e241c8f245204c5edf6654ebb315f149aeeb",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +466,470 @@      fromString(\"1a2s3ae\"),\n      fromString(\"translate\").translate(ImmutableMap.of(\n        \"r\", \"1\",\n        \"n\", \"2\",\n        \"l\", \"3\","
  },
  {
    "id" : "87c3714d-15d7-41e9-b471-cb1575f08ceb",
    "prId" : 26477,
    "prUrl" : "https://github.com/apache/spark/pull/26477#pullrequestreview-336687878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, the previous behaivour was like PostgreSQL before but you propose to match it to Hive, Mysql and Oracle?",
        "createdAt" : "2019-12-27T03:42:54Z",
        "updatedAt" : "2019-12-27T03:42:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8afa9aee-372b-4a25-b6af-9f31042da982",
        "parentId" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If that's the case, I am less sure why we should necessarily change. The lpad and rpad implementations seem different per DBMS implementation. Spark's case at least has one reference and the current behaviour makes sense as well.",
        "createdAt" : "2019-12-27T03:46:44Z",
        "updatedAt" : "2019-12-27T03:46:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c6fa42c0-c3b5-435c-adf3-b63ff69f3e97",
        "parentId" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "authorId" : "22ab4837-e86a-4aa2-8ddb-dbc09e778c43",
        "body" : "It is reasonable to change this because as per description of this function \r\nIn case of empty pad string, the return value should be null.\r\n",
        "createdAt" : "2019-12-27T05:22:55Z",
        "updatedAt" : "2019-12-27T05:22:56Z",
        "lastEditedBy" : "22ab4837-e86a-4aa2-8ddb-dbc09e778c43",
        "tags" : [
        ]
      },
      {
        "id" : "45e7c88b-d7d0-4b30-b037-e15827dc37d8",
        "parentId" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "authorId" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "body" : "@HyukjinKwon, I feel reasonable because in other DBMS like hive, Mysql they handle this issue after they find this problem.",
        "createdAt" : "2019-12-27T05:44:20Z",
        "updatedAt" : "2019-12-27T05:44:20Z",
        "lastEditedBy" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "tags" : [
        ]
      },
      {
        "id" : "4d293314-be33-4354-aa2a-36d44ae2cdff",
        "parentId" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This function behaviour varies per DBMS implementations and we have a reference as you described, PostgreSQL. Can you guys elaborate why it looks reasonable to you guys? I don't see a strong reason to change the current behaviour.",
        "createdAt" : "2019-12-27T06:41:46Z",
        "updatedAt" : "2019-12-27T06:41:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cc36bf74-3047-4ec4-818b-9dcb838cfe3b",
        "parentId" : "e6e01126-8aad-454a-8501-e463d37fae14",
        "authorId" : "22ab4837-e86a-4aa2-8ddb-dbc09e778c43",
        "body" : "I think NULL is the better choice, that way we don't return unpadded data.\r\nNULL makes sense with invalid input.\r\nPlease refer the JIRA how Hive handled\r\nhttps://issues.apache.org/jira/browse/HIVE-15792\r\nSo as per me we should follow this as per Hive.",
        "createdAt" : "2019-12-27T07:05:05Z",
        "updatedAt" : "2019-12-27T07:05:06Z",
        "lastEditedBy" : "22ab4837-e86a-4aa2-8ddb-dbc09e778c43",
        "tags" : [
        ]
      }
    ],
    "commit" : "51247b3e4b28751150d8f3104d5e5f1ec1a5be35",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +369,373 @@    assertEquals(EMPTY_UTF8, fromString(\"数据砖头\").rpad(-10, fromString(\"孙行者\")));\n    assertEquals(EMPTY_UTF8, fromString(\"数据砖头\").rpad(-10, EMPTY_UTF8));\n    assertEquals(fromString(null), fromString(\"数据砖头\").rpad(5, EMPTY_UTF8));\n    assertEquals(fromString(null), fromString(\"数据砖头\").rpad(3, EMPTY_UTF8));\n    assertEquals(fromString(null), EMPTY_UTF8.rpad(3, EMPTY_UTF8));"
  },
  {
    "id" : "8b15c93c-d51b-48a3-9ceb-7fe436f171e7",
    "prId" : 24707,
    "prUrl" : "https://github.com/apache/spark/pull/24707#pullrequestreview-251993150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "077eb3a7-615d-40e9-86a8-8fdfaa83cc5b",
        "parentId" : null,
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "I realized that Apache Commons StringUtils differs from Java `.replace()` in the special case where the search sring is empty: In Java, this would prepend the replacement string to each character, e.g. `\"ABC.replace(\"\", 1\")` is \"1A1B1C\".\r\n\r\nI also spotted a case where `UTF8String` diverges from Java strings: in Java, we have\r\n\r\n```\r\n@ \"\".replace(\"\", \"!\")\r\nres2: String = \"!\"\r\n```\r\n\r\nbut the existing Spark `UTF8String.replace()` always returns an empty string when passed an empty input string. I'm going to preserve that existing behavior.",
        "createdAt" : "2019-06-19T21:47:20Z",
        "updatedAt" : "2019-06-19T21:47:21Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "a88fd8a7-a6b4-4396-ad41-0bc3be7281e7",
        "parentId" : "077eb3a7-615d-40e9-86a8-8fdfaa83cc5b",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "**Update**: I confused myself andI had this backwards:\r\n\r\n I ran a couple of tests in `spark-shell` with the SQL replace expression:\r\n\r\n```\r\n%sql select replace('ABC', '', '1')\r\nABC\r\n```\r\n\r\nbecause the old Spark code had a `if (EMPTY_UTF8.equals(search)) { return this }` branch (so Spark's behavior matches StringUtils in this case).\r\n\r\nIf you don't permit empty search strings then also it's safe to bail out early if the input string is empty (since an empty input string can only become non-empty via an empty search string, something we already disallowed).\r\n\r\nAs a result, I think that the logic here is correct. Sorry for the false-alarm!",
        "createdAt" : "2019-06-19T21:55:14Z",
        "updatedAt" : "2019-06-19T21:55:41Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "971d2441-1d73-4b63-bddc-a0a83abe53b3",
        "parentId" : "077eb3a7-615d-40e9-86a8-8fdfaa83cc5b",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "In fact, this behavior is also tested at https://github.com/apache/spark/blob/fe5145ede2be50185d77ad3843e95be060d4a73e/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala#L415 and I didn't change those tests in this PR, so we can be extra confident that the new short-circuit check is correct.",
        "createdAt" : "2019-06-19T21:57:55Z",
        "updatedAt" : "2019-06-19T21:57:56Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      }
    ],
    "commit" : "6188dcdd321967eb78535d7cf55b73f20047f449",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +414,418 @@    assertEquals(\n      fromString(\"replace\"),\n      fromString(\"replace\").replace(fromString(\"\"), fromString(\"123\")));\n    // tests for multiple replacements\n    assertEquals("
  }
]