[
  {
    "id" : "bcb65992-0066-433e-99c8-912419a3ccec",
    "prId" : 29425,
    "prUrl" : "https://github.com/apache/spark/pull/29425#pullrequestreview-474975667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b0b3770-0984-4f04-a474-1a4be5306250",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "I left a comment in https://github.com/apache/spark/pull/28412#pullrequestreview-468098043 . I think we should add the unit test cases for verifying the code work as our design. We might need to update the implementation to provide the private APIs for testing.",
        "createdAt" : "2020-08-16T21:57:53Z",
        "updatedAt" : "2020-09-01T22:35:42Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "d41788c0-d591-4f7e-818a-16c108ee3e74",
        "parentId" : "5b0b3770-0984-4f04-a474-1a4be5306250",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Sure I will add unit tests.",
        "createdAt" : "2020-08-17T15:28:16Z",
        "updatedAt" : "2020-09-01T22:35:42Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "6abec7e9-2827-4d5e-bfb3-cbc2830682c2",
        "parentId" : "5b0b3770-0984-4f04-a474-1a4be5306250",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Hi @gatorsmile , unit tests for HybridStore are added in https://github.com/apache/spark/pull/29509. And the unit test for **writeAll()** method of levelDB is added in current pr.",
        "createdAt" : "2020-08-25T22:23:08Z",
        "updatedAt" : "2020-09-01T22:35:42Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "5a5f6c9906bfdc5c8244310c921709e2cd6559a9",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +176,180 @@      // Partition the large value list to a set of smaller batches, to reduce the memory\n      // pressure caused by serialization and give fairness to other writing threads.\n      for (List<?> batchList : Iterables.partition(entry.getValue(), 128)) {\n        final Iterator<?> valueIter = batchList.iterator();\n        final Iterator<byte[]> serializedValueIter;"
  },
  {
    "id" : "9093bb21-ddaa-409b-bb84-43f85f9a5a6d",
    "prId" : 29425,
    "prUrl" : "https://github.com/apache/spark/pull/29425#pullrequestreview-480175269",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca1220ed-21e0-40e3-8e22-4dfafa8f6c32",
        "parentId" : null,
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "@HeartSaVioR  Here I use **naturalIndex.updateCount()** to put the count information of all indexes to the batch. When implementing this I found we can lift the method **updateCount()** and **long getCount(byte[] key)** from LevelDBTypeInfo.Index to LevelDBTypeInfo, as these methods are not accessing any member of LevelDBTypeInfo.Index. Doing that would allow us to call **ti.updateCount()** to update count for all indexes, which would make more sense. However, it's totally optional.",
        "createdAt" : "2020-09-01T23:06:10Z",
        "updatedAt" : "2020-09-01T23:07:11Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "5a5f6c9906bfdc5c8244310c921709e2cd6559a9",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +201,205 @@            }\n            for (Map.Entry<ByteBuffer, Long> countEntry : counts.entrySet()) {\n              naturalIndex.updateCount(batch, countEntry.getKey().array(), countEntry.getValue());\n            }\n            db().write(batch);"
  }
]