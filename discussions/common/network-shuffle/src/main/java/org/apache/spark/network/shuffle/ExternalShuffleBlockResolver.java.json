[
  {
    "id" : "d808970a-92cc-4dae-935f-2e9f8986c068",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-715186611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c7991b5-6ac0-4c5c-a3b9-cce322159b83",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Btw, while looking at this, an unrelated potential issue - we use `intern` in `ExecutorDiskUtils`.\r\nProbably should move to using guava interner (`Utils.weakIntern` does this) ... thoughts @Ngone51 ?",
        "createdAt" : "2021-07-22T03:50:08Z",
        "updatedAt" : "2021-07-22T04:35:47Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "dc43fd71-3696-472c-8c94-8b92922528d3",
        "parentId" : "3c7991b5-6ac0-4c5c-a3b9-cce322159b83",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Unfortunately, `Utils` can't be referenced in `network-shuffle` module.",
        "createdAt" : "2021-07-26T08:31:34Z",
        "updatedAt" : "2021-07-26T08:31:34Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cbdb1e78-9bde-4a8d-b690-2beeeafd630f",
        "parentId" : "3c7991b5-6ac0-4c5c-a3b9-cce322159b83",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Yeah, I meant something similar ... we dont need to do this for this PR btw; just thinking out.",
        "createdAt" : "2021-07-26T19:06:34Z",
        "updatedAt" : "2021-07-26T19:40:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +394,398 @@      executor.localDirs,\n      executor.subDirsPerLocalDir,\n      fileName);\n    ManagedBuffer data = getBlockData(appId, execId, shuffleId, mapId, reduceId);\n    return ShuffleChecksumHelper.diagnoseCorruption("
  },
  {
    "id" : "cec57fa0-3795-49a9-963c-6e54714efe2a",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-715186611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea90653a-3394-4150-8293-e88855fe0d77",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "After we pass the `algorithm` to the server, finding the checksum file is no longer an issue. cc @mridulm @otterc ",
        "createdAt" : "2021-07-26T15:54:37Z",
        "updatedAt" : "2021-07-26T15:54:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a08c761a-d7ed-4d60-a6ed-c1d1c08f3139",
        "parentId" : "ea90653a-3394-4150-8293-e88855fe0d77",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "That is a cleaner solution, thanks !\r\nWill also mean we can spread the checksum files without needing to couple it with the shuffle file in the dir layout.",
        "createdAt" : "2021-07-26T19:07:07Z",
        "updatedAt" : "2021-07-26T19:41:13Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "1cf0f231-cbff-4aa8-9835-7b51ed7a2b56",
        "parentId" : "ea90653a-3394-4150-8293-e88855fe0d77",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Add a comment that this should be in sync with `IndexShuffleBlockResolver.getChecksumFile` ?",
        "createdAt" : "2021-07-26T19:14:02Z",
        "updatedAt" : "2021-07-26T19:41:45Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +390,394 @@    ExecutorShuffleInfo executor = executors.get(new AppExecId(appId, execId));\n    // This should be in sync with IndexShuffleBlockResolver.getChecksumFile\n    String fileName = \"shuffle_\" + shuffleId + \"_\" + mapId + \"_0.checksum.\" + algorithm;\n    File checksumFile = ExecutorDiskUtils.getFile(\n      executor.localDirs,"
  },
  {
    "id" : "5bbbc71f-d5a0-46e2-b096-f9f7d78daf93",
    "prId" : 32811,
    "prUrl" : "https://github.com/apache/spark/pull/32811#pullrequestreview-687791202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f0d3332-9774-419a-88c0-8c929291c7ee",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we need this function signature change?",
        "createdAt" : "2021-06-17T15:17:57Z",
        "updatedAt" : "2021-06-17T15:17:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b59285d-e51e-4cbe-a89b-c1e77175c110",
        "parentId" : "4f0d3332-9774-419a-88c0-8c929291c7ee",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "@mridulm suggested to make this change here:\r\nhttps://github.com/apache/spark/pull/32140#discussion_r645290580\r\n\r\nThis change is not necessary for push-based shuffle but it was a small change so I was fine with it. ",
        "createdAt" : "2021-06-17T16:02:11Z",
        "updatedAt" : "2021-06-17T16:02:11Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "9dddadd4-d013-414e-a12e-20718bd1540c",
        "parentId" : "4f0d3332-9774-419a-88c0-8c929291c7ee",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This is not a public api, is it @dongjoon-hyun ?\r\nWe could keep the signature and internally use a `Set` if it is being externally depended on.",
        "createdAt" : "2021-06-17T17:14:40Z",
        "updatedAt" : "2021-06-17T17:14:40Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "34d7c129-2bf4-435c-88e6-0d592f4b049d",
        "parentId" : "4f0d3332-9774-419a-88c0-8c929291c7ee",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "+CC @dongjoon-hyun any thoughts on the above ? Thx",
        "createdAt" : "2021-06-19T03:30:28Z",
        "updatedAt" : "2021-06-19T03:30:29Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7a612dab57a47de0d671b8c28a2f271a18fb7bb",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +362,366 @@  }\n\n  public Map<String, String[]> getLocalDirs(String appId, Set<String> execIds) {\n    return execIds.stream()\n      .map(exec -> {"
  },
  {
    "id" : "ea50bfb6-4f15-47e5-b2a5-854ba66d0144",
    "prId" : 31517,
    "prUrl" : "https://github.com/apache/spark/pull/31517#pullrequestreview-702685607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "becc5582-6a4d-4df4-8d2b-665295ad7054",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Can you tell me why we needed to change the exception? Is this just what Caffeine throws instead? Do we have test coverage for this?",
        "createdAt" : "2021-07-08T18:22:34Z",
        "updatedAt" : "2021-07-08T18:29:12Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "5022fafc-a513-4557-b3e6-a9a082c5660b",
        "parentId" : "becc5582-6a4d-4df4-8d2b-665295ad7054",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "> Can you tell me why we needed to change the exception? Is this just what Caffeine throws instead? \r\n\r\n`com.github.benmanes.caffeine.cache.LoadingCache#get` method throw `CompletionException` if a checked exception was thrown while loading the value.\r\n\r\n`com.google.common.cache.LoadingCache#get` method throw `ExecutionException` if a checked exception was thrown while loading the value.\r\n\r\nSo the exception type here has changed.",
        "createdAt" : "2021-07-09T03:01:29Z",
        "updatedAt" : "2021-07-09T03:01:30Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "cc5495ba-529f-4d56-b783-151d4e7f1f56",
        "parentId" : "becc5582-6a4d-4df4-8d2b-665295ad7054",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "> Do we have test coverage for this?\r\n\r\nThe `ExecutionException (now is CompletionException)` is re-throw as `RuntimeException`, I need to further check whether it is covered by existing test case",
        "createdAt" : "2021-07-09T03:13:34Z",
        "updatedAt" : "2021-07-09T03:13:43Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "940ffb87-ec28-433d-96c8-466052e8c1a3",
        "parentId" : "becc5582-6a4d-4df4-8d2b-665295ad7054",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@holdenk `testFetchWrongExecutor` and `testFetchNonexistent` in `ExternalBlockHandlerSuite` already cover this.",
        "createdAt" : "2021-07-09T05:08:39Z",
        "updatedAt" : "2021-07-09T05:08:39Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "81f863ff67d0236f050f4a24e9470c2b1bb7aaff",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +304,308 @@        shuffleIndexRecord.getOffset(),\n        shuffleIndexRecord.getLength());\n    } catch (CompletionException e) {\n      throw new RuntimeException(\"Failed to open file: \" + indexFile, e);\n    }"
  },
  {
    "id" : "81c53518-9911-49b0-b5bc-ef3d062d4941",
    "prId" : 28525,
    "prUrl" : "https://github.com/apache/spark/pull/28525#pullrequestreview-415280717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f061487-6d64-45fa-bb79-9caeafbe1d3c",
        "parentId" : null,
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "IIUC, if we calculate digest on the fly for continuous shuffle read, I think we could guarantee the end-to-end consistency.",
        "createdAt" : "2020-05-20T12:28:07Z",
        "updatedAt" : "2020-05-21T12:22:16Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c59fcd627f5132bf8ebe493a8f1282139b31f1cf",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +331,335 @@          shuffleIndexRecord.getOffset(),\n          shuffleIndexRecord.getLength(),\n          shuffleIndexRecord.getDigest().orElse(DigestUtils.getDigest(\n            dataFile, shuffleIndexRecord.getOffset(), shuffleIndexRecord.getLength())));\n"
  },
  {
    "id" : "39fe7340-e77a-4e47-9627-e227e8c04f59",
    "prId" : 25620,
    "prUrl" : "https://github.com/apache/spark/pull/25620#pullrequestreview-365446367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd5a8be0-dcb5-4a53-919a-537bd1509748",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "@xuanyuanking why change this from int to long? Is it possible that a mapId can be greater than 2^31?",
        "createdAt" : "2020-02-26T22:30:51Z",
        "updatedAt" : "2020-02-26T22:30:51Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "aa9d9e1a-912a-45b9-8546-fb4dbf6c771b",
        "parentId" : "bd5a8be0-dcb5-4a53-919a-537bd1509748",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "previous the map id is the index of the mapper, and can get conflicts when we re-run the task. Now the map id is the task id, which is unique. task id needs to be long.",
        "createdAt" : "2020-02-27T05:41:18Z",
        "updatedAt" : "2020-02-27T05:41:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c0f106d2-f09d-4e30-80db-0d87b3cc92cd",
        "parentId" : "bd5a8be0-dcb5-4a53-919a-537bd1509748",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, after this patch, we set mapId by using the `taskAttemptId` of map task, which is a unique Id within the same SparkContext. You can see the comment https://github.com/apache/spark/pull/25620#discussion_r319396089",
        "createdAt" : "2020-02-27T06:49:50Z",
        "updatedAt" : "2020-02-27T06:49:50Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c9f9c2da1215a836fef7925e95a40c7c6dd87e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +173,177 @@      String execId,\n      int shuffleId,\n      long mapId,\n      int reduceId) {\n    ExecutorShuffleInfo executor = executors.get(new AppExecId(appId, execId));"
  }
]