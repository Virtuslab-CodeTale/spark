[
  {
    "id" : "b5cc187f-ac8e-4b79-96ec-3ee65a189a3e",
    "prId" : 33613,
    "prUrl" : "https://github.com/apache/spark/pull/33613#pullrequestreview-722883247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e248bbd-7fa6-4410-8379-d558a36a3350",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Nice catch !",
        "createdAt" : "2021-08-05T02:55:14Z",
        "updatedAt" : "2021-08-05T03:28:59Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "52b709356b367a8616acffec9f7f3a0eec834fce",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +219,223 @@            logger.trace(\"Successfully got merged block meta for shuffleId {} shuffleMergeId {}\"\n              + \" reduceId {}\", shuffleId, shuffleMergeId, reduceId);\n            listener.onSuccess(shuffleId, shuffleMergeId, reduceId,\n              new MergedBlockMeta(numChunks, buffer));\n          }"
  },
  {
    "id" : "c1030037-1523-4992-85db-712465470ab2",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-712119482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5598f28e-8d77-4e0f-aaa8-afbbce9709fe",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Note to self: all the changes in this file are related to renaming of `conf`",
        "createdAt" : "2021-07-21T19:49:44Z",
        "updatedAt" : "2021-07-21T23:21:34Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +63,67 @@      boolean authEnabled,\n      long registrationTimeoutMs) {\n    this.transportConf = conf;\n    this.secretKeyHolder = secretKeyHolder;\n    this.authEnabled = authEnabled;"
  },
  {
    "id" : "02ec842c-4b37-48cb-9fdc-28dc5c04fa47",
    "prId" : 30163,
    "prUrl" : "https://github.com/apache/spark/pull/30163#pullrequestreview-529833901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f9afacb-0d5e-43a4-bc84-7028baca88b4",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "The part we want to get community inputs is whether to put this API inside `ExternalBlockStoreClient` or a separate client.\r\nThis RPC will be used by the Spark driver (DAGScheduler) to finalize the shuffle merge when it decides to do so and to retrieve the corresponding `MergeStatuses` from individual shuffle services for a given shuffle.\r\nRight now, we have to initialize an `ExternalBLockStoreClient` on the DAGScheduler side with some dummy parameters like the following:\r\n`\r\n  private lazy val externalShuffleClient: Option[ExternalShuffleClient] =\r\n\r\n    if (pushBasedShuffleEnabled) {\r\n      val transConf = SparkTransportConf.fromSparkConf(sc.conf, \"shuffle\", 1)\r\n      val shuffleClient = new ExternalShuffleClient(transConf, env.securityManager,\r\n        env.securityManager.isAuthenticationEnabled(),\r\n        sc.conf.get(config.SHUFFLE_REGISTRATION_TIMEOUT))\r\n      shuffleClient.init(sc.conf.getAppId)\r\n      Some(shuffleClient)\r\n    } else {\r\n      None\r\n    }\r\n`\r\n\r\nCC @Ngone51 @jiangxb1987 @attilapiros @tgravescs ",
        "createdAt" : "2020-10-27T22:55:53Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "f4dce350-f7b2-4789-9d8a-ca8209aef611",
        "parentId" : "4f9afacb-0d5e-43a4-bc84-7028baca88b4",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : " you modified the BlockStoreclient interface to have pushBlocks so why not have this there as well? If everything else is going through the BlockStoreClient to push blocks it seems to make sense to put this there as well.  \r\n\r\nCan't you get the external shuffle client from the block manger on the driver?  I think that manager gets the external shuffle client even though it doesn't really use it. I thought it was all initialized from spark context.\r\n          sc.env.blockManager.blockStoreClient\r\n\r\nnote It think this is hard to review on its own, I had to look at the WIP to get more context, not sure if that means ideally this should be part of another PR, open to others thoughts on that.",
        "createdAt" : "2020-10-30T20:06:58Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6af7e564-e236-4762-8867-ed46a4a9f6fd",
        "parentId" : "4f9afacb-0d5e-43a4-bc84-7028baca88b4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "+1, we can get the client through `sc.env.blockManager.blockStoreClient` and add the method to `BlockStoreClient`.",
        "createdAt" : "2020-11-03T12:18:01Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "40a847c7-cc4e-41fb-9b24-053549465ac1",
        "parentId" : "4f9afacb-0d5e-43a4-bc84-7028baca88b4",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Good suggestion. I will try it out and update the PR accordingly.",
        "createdAt" : "2020-11-05T07:57:42Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "cd5421be-4343-42d8-93e1-ca33e5261340",
        "parentId" : "4f9afacb-0d5e-43a4-bc84-7028baca88b4",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Tried it out and it worked with our WIP branch. Updated the PR. Please review. Thanks.",
        "createdAt" : "2020-11-13T07:46:23Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b3bbe9291e12a7decca46874dfe007285c9d562",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +160,164 @@\n  @Override\n  public void finalizeShuffleMerge(\n      String host,\n      int port,"
  },
  {
    "id" : "2bfe3300-9d65-4796-ae0c-4cf0f077ee47",
    "prId" : 30163,
    "prUrl" : "https://github.com/apache/spark/pull/30163#pullrequestreview-534796816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18fe3d5a-e32a-4f0b-8f15-d80ffefc4e9d",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I am uncertain here: is not this one kind of shuffle merge fail too? \r\nSo if you do not call `onShuffleMergeFailure` here then this error is not propagated. \r\nWould not this cause an endless waiting for some kind of asynchronous result at the caller?",
        "createdAt" : "2020-11-15T10:48:48Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "e99770ab-d99c-40d7-a151-a6d5892d6aef",
        "parentId" : "18fe3d5a-e32a-4f0b-8f15-d80ffefc4e9d",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Yes, this is a good catch.\r\n`onShuffleMergeFailure` should be called here as well.\r\nIt won't cause an endless waiting in the current implementation though, as the caller (a separate thread in DAGScheduler) would bound the wait for getting merge results from all related shuffle service locations.",
        "createdAt" : "2020-11-16T02:23:49Z",
        "updatedAt" : "2020-11-19T19:51:19Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "fc17449f-dc49-461b-bae4-ef8897205af7",
        "parentId" : "18fe3d5a-e32a-4f0b-8f15-d80ffefc4e9d",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Added the onShuffleMergeFailure.",
        "createdAt" : "2020-11-19T19:51:39Z",
        "updatedAt" : "2020-11-19T19:51:39Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b3bbe9291e12a7decca46874dfe007285c9d562",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +182,186 @@      });\n    } catch (Exception e) {\n      logger.error(\"Exception while sending finalizeShuffleMerge request to {}:{}\",\n        host, port, e);\n      listener.onShuffleMergeFailure(e);"
  },
  {
    "id" : "93485076-17f9-45ca-a03e-40e566454ddd",
    "prId" : 29855,
    "prUrl" : "https://github.com/apache/spark/pull/29855#pullrequestreview-506290854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d244711-2ccb-4789-a767-48dd6bfe2a5e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Is `buffersWithId` necessary since `blockIds` and `buffers` sharing the same index? Inside `OneForOneBlockPusher` we can also identify them by the index.",
        "createdAt" : "2020-10-11T13:44:15Z",
        "updatedAt" : "2020-10-13T15:54:30Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "13035f01-f117-42a6-9986-0713ea4260ab",
        "parentId" : "3d244711-2ccb-4789-a767-48dd6bfe2a5e",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "This is necessary, due to `RetryingBlockFetcher` could supply a subrange of blockIds during retry.\r\n`OneForOneBlockPusher` is created by `RetryingBlockFetcher.BlockFetchStarter` during the invocation its `createAndStart`.\r\n`RetryingBlockFetcher` could supply an array of blockIds which is a subset of the original blockIds, based on how many blocks have already been successfully pushed.\r\nSince that API does not have an array of buffers, and we want to reuse `RetryingBlockFetcher` between both block fetch and push, creating this hashmap is thus necessary to keep track of the mapping between blockId and the buffer throughout multiple retries of this batch.",
        "createdAt" : "2020-10-11T15:27:12Z",
        "updatedAt" : "2020-10-13T15:54:30Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "61975b96-bb06-45c8-ae36-16e5ffe4d9a9",
        "parentId" : "3d244711-2ccb-4789-a767-48dd6bfe2a5e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2020-10-12T04:53:18Z",
        "updatedAt" : "2020-10-13T15:54:30Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c95f18d2bdf9ac373b0d5319b686a1d66c1e72b",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +134,138 @@    Map<String, ManagedBuffer> buffersWithId = new HashMap<>();\n    for (int i = 0; i < blockIds.length; i++) {\n      buffersWithId.put(blockIds[i], buffers[i]);\n    }\n    logger.debug(\"Push {} shuffle blocks to {}:{}\", blockIds.length, host, port);"
  },
  {
    "id" : "c371b36f-2186-463b-b5db-6778ce47af2f",
    "prId" : 29855,
    "prUrl" : "https://github.com/apache/spark/pull/29855#pullrequestreview-506389238",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "895c8b93-9814-42c9-bac3-f6b1be3d627e",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "What's the benefit of handle this case separately here, instead of just create a `RetryingBlockFetcher` anyway?",
        "createdAt" : "2020-10-12T04:46:29Z",
        "updatedAt" : "2020-10-13T15:54:30Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "a0d1a783-0756-4014-937f-c1958cc0cb8d",
        "parentId" : "895c8b93-9814-42c9-bac3-f6b1be3d627e",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "This follows the way it is done in the `fetchBlocks` API.\r\nShould we remove it?",
        "createdAt" : "2020-10-12T08:12:57Z",
        "updatedAt" : "2020-10-13T15:54:30Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c95f18d2bdf9ac373b0d5319b686a1d66c1e72b",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +149,153 @@          conf, blockPushStarter, blockIds, listener, PUSH_ERROR_HANDLER).start();\n      } else {\n        blockPushStarter.createAndStart(blockIds, listener);\n      }\n    } catch (Exception e) {"
  },
  {
    "id" : "ac84f831-875b-4df9-b837-d8735488d565",
    "prId" : 27419,
    "prUrl" : "https://github.com/apache/spark/pull/27419#pullrequestreview-357908105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7400e36a-0659-4f3d-b1d3-04896e305ce9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's also possible that `Host` is unreachable or connectable when network is unstable, right?",
        "createdAt" : "2020-02-13T01:47:53Z",
        "updatedAt" : "2020-02-13T01:47:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c4995e985baca97838523954395dc68b38b0c3c",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +116,120 @@                throw e;\n              } catch (IOException e) {\n                if (!isHostReachable(host, 3000) || !isHostPortConnectable(host, port)) {\n                  // throw ExternalShuffleServiceLostException exception then we won't retry to connect\n                  // un-connected External Shuffle Service."
  }
]