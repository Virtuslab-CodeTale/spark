[
  {
    "id" : "d6241c2c-292a-4345-8445-9d215b601b60",
    "prId" : 33605,
    "prUrl" : "https://github.com/apache/spark/pull/33605#pullrequestreview-720751144",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa24b765-e95a-44b8-ad25-3b95e3574049",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Validate that cleanup was executed ?",
        "createdAt" : "2021-08-02T05:47:26Z",
        "updatedAt" : "2021-08-02T05:47:26Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "6387bfbe-fa91-4410-89d7-a8277d5eed32",
        "parentId" : "fa24b765-e95a-44b8-ad25-3b95e3574049",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Isn't `getMergedBlockMeta` for the older shuffle merge id is the validation? Checking for the exception from `getMergedBlockMeta`. Am I missing something?",
        "createdAt" : "2021-08-03T03:28:00Z",
        "updatedAt" : "2021-08-03T03:28:00Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "6fa7e5b6-071f-432c-8f32-82eb4c428c0f",
        "parentId" : "fa24b765-e95a-44b8-ad25-3b95e3574049",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "The thing is `getMergedBlockMeta` throws exception checking the metadata itself. Let me  change the test to validate by checking for the files itself.",
        "createdAt" : "2021-08-03T03:29:41Z",
        "updatedAt" : "2021-08-03T03:29:41Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "0412f8e9-ce9d-46e2-a5bc-1004af3685e2",
        "parentId" : "fa24b765-e95a-44b8-ad25-3b95e3574049",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Explicitly checking for `!file.exists`",
        "createdAt" : "2021-08-03T03:38:40Z",
        "updatedAt" : "2021-08-03T03:38:41Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "1019d7a6d83cfd3e1def51c590499985277d59a7",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1247,1251 @@    // but no blocks pushed for that shuffleMergeId\n    pushResolver.finalizeShuffleMerge(new FinalizeShuffleMerge(testApp, NO_ATTEMPT_ID, 0, 5));\n    closed.acquire();\n    assertFalse(\"MergedBlock meta file for shuffle 0 and shuffleMergeId 4 should be cleaned\"\n      + \" up\", appShuffleInfo.getMergedShuffleMetaFile(0, 4, 0).exists());"
  },
  {
    "id" : "ce327968-9478-481e-a993-4976a8415f66",
    "prId" : 30433,
    "prUrl" : "https://github.com/apache/spark/pull/30433#pullrequestreview-547882592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "What's the expected behavior for those deferred blocks after exceeding the IO exception threshold? Seem like we don't any strategy for those blocks yet.",
        "createdAt" : "2020-12-08T07:04:16Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "26b68e48-a1bf-46e7-bde5-2ac264bff14c",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "The deferred blocks are just dereferenced. This is same as what happens when the server responds with the `TooLate` response.\r\n```\r\n    private void incrementIOExceptionsAndAbortIfNecessary() {\r\n      // Update the count of IOExceptions\r\n      partitionInfo.incrementIOExceptions();\r\n      if (partitionInfo.shouldAbort(mergeManager.ioExceptionsThresholdDuringMerge)) {\r\n        deferredBufs = null;\r\n        throw new RuntimeException(String.format(\"%s when merging %s\",\r\n          ErrorHandler.BlockPushErrorHandler.IOEXCEPTIONS_EXCEEDED_THRESHOLD_PREFIX,\r\n          streamId));\r\n      }\r\n    }\r\n```",
        "createdAt" : "2020-12-08T18:04:01Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "ad636d9a-173a-463b-87b3-d03b7f0800a2",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's `deferredBufs` rather than deferred blocks, right? I actually mean those blocks which are waiting for the partition lock at the same time.",
        "createdAt" : "2020-12-09T02:02:57Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "67da3b6c-b659-44e4-8c87-c5a17025eecc",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Yes, I meant `deferredBufs`.",
        "createdAt" : "2020-12-09T02:10:52Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "3580c3ab-3173-48bf-b91c-8435a823595d",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> I actually mean those blocks which are waiting for the partition lock at the same time.\r\n\r\nI see. That's a good point. The server would attempt to merge them. If they don't see any IOExceptions, which is unlikely, they will be merged. Do you think we should check before writing them?",
        "createdAt" : "2020-12-09T02:15:10Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "a92cbaff-8442-4d1f-bf21-852773acb25f",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Do you think we should check before writing them?\r\n\r\nI think so. And stop writing when the partition already hits IO exceptions threshold.",
        "createdAt" : "2020-12-09T04:11:27Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "db046ad2-391b-4d81-9ac9-0f55b4f2ee34",
        "parentId" : "690e49cf-b2ae-42e9-8a6f-ed5336880f3d",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "I have updated the PR to fail any pending blocks immediately (before attempting to write) if the shuffle partition has hit the IOException threshold. PTAL\r\n\r\n> And stop writing when the partition already hits IO exceptions threshold.\r\n\r\nThis should already be happening. Not sure what you mean. ",
        "createdAt" : "2020-12-09T06:47:02Z",
        "updatedAt" : "2020-12-09T18:13:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "f48d52f68e52b63742e7cce5dbb4fbcc89845402",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +580,584 @@        t.getMessage());\n      throw t;\n    }\n  }\n"
  },
  {
    "id" : "ad97372b-91cd-4b39-bd1f-46b1db65d502",
    "prId" : 30062,
    "prUrl" : "https://github.com/apache/spark/pull/30062#pullrequestreview-515169221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ae6a6d2-2792-4ca2-abe0-53edd2341a07",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Maybe, uses `spy()` on `stream2`. So we can verify `writeAnyDeferredBlocks` is really invoked later?",
        "createdAt" : "2020-10-22T08:36:29Z",
        "updatedAt" : "2020-11-08T07:14:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "90767ece-4f12-4a0e-b3c3-26a37145eee0",
        "parentId" : "3ae6a6d2-2792-4ca2-abe0-53edd2341a07",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "When we verify the data in the merged shuffle file, it does imply that `writeAnyDeferredBlocks` is invoked. The validation also checks the bytes for each block. If `writeAnyDeferredBlocks` wasn't invoked, the validation will fail.",
        "createdAt" : "2020-10-22T22:01:16Z",
        "updatedAt" : "2020-11-08T07:14:19Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "35e0332e-c5a7-490d-8775-587681fab7c7",
        "parentId" : "3ae6a6d2-2792-4ca2-abe0-53edd2341a07",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Also, will not be able to use `spy()` to verify if `writeAnyDeferredBlocks` is invoked. `writeAnyDeferredBlocks` is a method of the anonymous implementation  of `StreamCallback` in `RemoteBlockPushResolver.  `StreamCallback` api doesn't expose it.",
        "createdAt" : "2020-10-22T22:41:47Z",
        "updatedAt" : "2020-11-08T07:14:19Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb1881cc02e9606471d0f29345267bf2052f6880",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +150,154 @@      pushResolver.receiveBlockDataAsStream(new PushBlockStream(TEST_APP, 0, 1, 0, 0));\n    // This should be deferred\n    stream2.onData(stream2.getID(), ByteBuffer.wrap(new byte[3]));\n    // stream 1 now completes\n    stream1.onData(stream1.getID(), ByteBuffer.wrap(new byte[2]));"
  },
  {
    "id" : "863b687d-e28b-4270-a45e-fd1c2c19981b",
    "prId" : 30062,
    "prUrl" : "https://github.com/apache/spark/pull/30062#pullrequestreview-522891243",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec26a161-5ed8-4908-a019-c6dce2aa57d6",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "You could add the corresponding`ByteBuffer` too. \r\nAnd remove  `    Preconditions.checkArgument(blocks.length == buffers.length); `\r\nIs not it?",
        "createdAt" : "2020-11-03T13:21:50Z",
        "updatedAt" : "2020-11-08T07:14:20Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "33f91e43-9e3c-4fff-b46e-76170bb2da52",
        "parentId" : "ec26a161-5ed8-4908-a019-c6dce2aa57d6",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Done",
        "createdAt" : "2020-11-03T21:35:34Z",
        "updatedAt" : "2020-11-08T07:14:20Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb1881cc02e9606471d0f29345267bf2052f6880",
    "line" : 487,
    "diffHunk" : "@@ -1,1 +485,489 @@    private final int shuffleId;\n    private final int mapIndex;\n    private final int reduceId;\n    private final ByteBuffer buffer;\n    PushBlock(int shuffleId, int mapIndex, int reduceId, ByteBuffer buffer) {"
  }
]